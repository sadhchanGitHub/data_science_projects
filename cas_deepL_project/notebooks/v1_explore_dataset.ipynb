{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c7bdac-ce3a-40ae-8d35-5155ac6f6cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734858923.384925    1456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734858923.442019    1456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Is GPU available?\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95e192-91af-41e9-b0f8-004e968980bf",
   "metadata": {},
   "source": [
    "### define directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa99578-276a-4708-95e8-c1d96c31febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "dataset_path = \"../data/selected_categories/EuroSAT\"\n",
    "test_data_path = \"../data/test_data\"\n",
    "preprocessed_dir = \"../data/preprocessed_data_imSize513\"\n",
    "categories = [\"Forest\", \"Residential\", \"Highway\", \"AnnualCrop\", \"HerbaceousVegetation\", \"Industrial\"]\n",
    "image_size = 513  # Resize to 513*513 -- better to use in pre-tarined DeepLabV3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2991b-106c-463b-ad66-7297dcb9fc3d",
   "metadata": {},
   "source": [
    "### preprocess in batches so as to not run into issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037158c-1fab-4105-a529-590132bbf62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: Forest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "\n",
    "# Preprocessing\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    output_path = os.path.join(preprocessed_dir, f\"{category}.npy\")\n",
    "    \n",
    "    print(f\"Processing category: {category}\")\n",
    "    images = []\n",
    "    for file in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, file)\n",
    "        try:\n",
    "            img = cv2.imread(file_path)\n",
    "            img = cv2.resize(img, (image_size, image_size))\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    # Save preprocessed images to disk\n",
    "    np.save(output_path, np.array(images))\n",
    "    print(f\"Saved preprocessed images for {category} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4cce66b-d2d8-40e9-ab73-608acef7502d",
   "metadata": {},
   "source": [
    "What Happens During Preprocessing?\n",
    "\n",
    "    Reads All Images in the Category Folder:\n",
    "        The script loops through all files in the AnnualCrop folder (../data/selected_categories/EuroSAT/AnnualCrop).\n",
    "        Assuming there are 3,000 images, it processes each one.\n",
    "\n",
    "    Resizes and Normalizes Each Image:\n",
    "        Each image is resized to 128x128.\n",
    "        Pixel values are scaled to the range [0, 1].\n",
    "\n",
    "    Saves All Images to a Single .npy File:\n",
    "        Once all 3,000 images are processed, they are stored as a 4D NumPy array in AnnualCrop.npy.\n",
    "\n",
    "Whatâ€™s Inside AnnualCrop.npy?\n",
    "\n",
    "AnnualCrop.npy contains:\n",
    "\n",
    "    Shape: (3000, 128, 128, 3):\n",
    "        3,000 images.\n",
    "        Each image is resized to 128x128 pixels with 3 color channels (RGB).\n",
    "    Data Type: float32 (normalized to [0, 1]).\n",
    "\n",
    "            Why Store as .npy?\n",
    "\n",
    "    Efficiency:\n",
    "        Storing all images in one .npy file is faster to load than reading individual image files from disk.\n",
    "    Compact:\n",
    "        .npy files compress the data efficiently without loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f969b87-9ea4-4413-8282-7a719959f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/preprocessed_data/AnnualCrop.npy\")\n",
    "print(data.shape)  # Output: (N, 128, 128, 3)\n",
    "print(data[0])  # First image as a NumPy array with pixel values in [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6fe96-308c-4c55-b736-43895f2f0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not as pixel but it seeing it as image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(data[0])  # Show the first image\n",
    "plt.title(\"AnnualCrop Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e2c82-7718-4bc1-a425-e2cf23ccd8f8",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data\n",
    "\n",
    "### Combine all .npy files into a single dataset and create train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d89f7-4093-49b2-8b9f-0b7f308a66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Batch loading function\n",
    "def batch_split_and_save(preprocessed_dir, categories, test_size=0.2, val_size=0.2):\n",
    "    for category in categories:\n",
    "        file_path = os.path.join(preprocessed_dir, f\"{category}.npy\")\n",
    "        print(f\"Processing {category}\")\n",
    "        \n",
    "        # Load the category data\n",
    "        images = np.load(file_path)\n",
    "        labels = np.full(len(images), categories.index(category))\n",
    "        \n",
    "        # Split into Train+Validation and Test\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            images, labels, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Split Train+Validation into Training and Validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=val_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Save splits for this category\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_train.npy\"), X_train)\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_val.npy\"), X_val)\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_test.npy\"), X_test)\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_train_labels.npy\"), y_train)\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_val_labels.npy\"), y_val)\n",
    "        np.save(os.path.join(preprocessed_dir, f\"{category}_test_labels.npy\"), y_test)\n",
    "        print(f\"Saved splits for {category}\")\n",
    "\n",
    "print(\"preprocessed_dir\", preprocessed_dir)\n",
    "batch_split_and_save(preprocessed_dir, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abcffc-a3dc-4e7c-b60c-c02c2fa611f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the size of the train, val and test for one Category\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(np.load(os.path.join(preprocessed_dir, \"AnnualCrop_train.npy\")).shape)\n",
    "print(np.load(os.path.join(preprocessed_dir, \"AnnualCrop_val.npy\")).shape)\n",
    "print(np.load(os.path.join(preprocessed_dir, \"AnnualCrop_test.npy\")).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681e046-0ea2-4aab-9bb2-766577248b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display sample of one image per category\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Function to visualize one random image from each category\n",
    "def display_one_per_category(preprocessed_dir, categories):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, category in enumerate(categories):\n",
    "        # Load data for the category\n",
    "        category_images = np.load(os.path.join(preprocessed_dir, f\"{category}_train.npy\"))\n",
    "        \n",
    "        # Randomly pick an image\n",
    "        random_idx = random.randint(0, len(category_images) - 1)\n",
    "        image = category_images[random_idx]\n",
    "        \n",
    "        # Display the image\n",
    "        plt.subplot(1, len(categories), i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(category)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize one random image from each category\n",
    "display_one_per_category(preprocessed_dir, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01403146-7faa-40cf-9935-16ed25f95e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize total image count\n",
    "total_images = 0\n",
    "\n",
    "# Ensure the folder exists\n",
    "if os.path.exists(preprocessed_dir):\n",
    "    for file in os.listdir(preprocessed_dir):\n",
    "        if file.endswith(\".npy\"):  # Only process .npy files\n",
    "            file_path = os.path.join(preprocessed_dir, file)\n",
    "            # Load the numpy file and add the number of images\n",
    "            try:\n",
    "                data = np.load(file_path)  # Corrected to load the file path\n",
    "                total_images += data.shape[0]  # Add the number of images\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "    print(f\"Total number of images in '{preprocessed_dir}': {total_images}\")\n",
    "else:\n",
    "    print(f\"Directory '{preprocessed_dir}' does not exist. Please check the path.\")\n",
    "\n",
    "# Memory usage calculation\n",
    "image_size = (128, 128, 3)  # Replace with your image dimensions\n",
    "dtype = np.float32  # Replace with your data type\n",
    "\n",
    "# Memory required in bytes\n",
    "memory_bytes = total_images * np.prod(image_size) * np.dtype(dtype).itemsize\n",
    "memory_gb = memory_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Estimated memory usage: {memory_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973a022-0142-42cc-9338-cadcf2893f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory details\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Total memory in GB\n",
    "total_memory_gb = memory.total / (1024 ** 3)\n",
    "\n",
    "print(f\"Total System Memory: {total_memory_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dc1a5-ca5d-4885-a545-a351801ae58b",
   "metadata": {},
   "source": [
    "# Reshape labels for binary segmentation\n",
    "y_train = np.expand_dims(y_train, axis=-1)  # Add the last channel dimension\n",
    "y_val = np.expand_dims(y_val, axis=-1)\n",
    "y_test = np.expand_dims(y_test, axis=-1)\n",
    "\n",
    "# Expand each label to match the spatial dimensions of the image\n",
    "y_train = np.broadcast_to(y_train, (y_train.shape[0], 128, 128, 1))\n",
    "y_val = np.broadcast_to(y_val, (y_val.shape[0], 128, 128, 1))\n",
    "y_test = np.broadcast_to(y_test, (y_test.shape[0], 128, 128, 1))\n",
    "\n",
    "print(\"New y_train shape:\", y_train.shape)  # Should be (10880, 128, 128, 1)\n",
    "print(\"New y_val shape:\", y_val.shape)  # Should be (2720, 128, 128, 1)\n",
    "print(\"New y_test shape:\", y_test.shape)  # Should be (3400, 128, 128, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a38ff8-d877-464f-8c90-dc694512a9a0",
   "metadata": {},
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (batch_size, 128, 128, 3)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (batch_size, 128, 128, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f2159-7479-4144-b5ef-9ef0127944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Balancing Classes\n",
    "### Why This Matters:\n",
    "\n",
    "### If your dataset has significantly more images for some categories (e.g., Forest) than others (e.g., Highway), the model may become biased toward the dominant classes. Balancing ensures fair contributions from all categories.\n",
    "### Code to Check Class Balance\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Check training set balance\n",
    "class_counts = {}\n",
    "for category in categories:\n",
    "    labels = np.load(os.path.join(output_dir, f\"{category}_train_labels.npy\"))\n",
    "    class_counts[category] = len(labels)\n",
    "\n",
    "# Print class distribution\n",
    "for category, count in class_counts.items():\n",
    "    print(f\"{category}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "877ed8f4-7bd7-47e5-b605-3092a29ce89f",
   "metadata": {},
   "source": [
    "The above results suggets, classes are almost balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30640b1d-d615-4f64-a1f6-689585710789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1920, 224, 224, 3), Labels shape: (1920,)\n",
      "First 5 labels: [3 3 3 3 3]\n",
      "Shape of labels: (1920,)\n",
      "Min pixel value: 0.10196078568696976\n",
      "Max pixel value: 1.0\n",
      "Min pixel value for Labels: 3\n",
      "Max pixel value for Labels: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "training_dir = \"../data/training_data_224\"\n",
    "images = np.load(f'{training_dir}/AnnualCrop_train.npy')\n",
    "labels = np.load(f'{training_dir}/AnnualCrop_train_labels.npy')\n",
    "\n",
    "# Check sample shapes and alignment\n",
    "print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "print(f\"First 5 labels: {labels[:5]}\")\n",
    "\n",
    "# Debug: Check the shape of one-hot encoded labels\n",
    "print(\"Shape of labels:\", labels.shape)\n",
    "\n",
    "# Check the range of pixel values in the dataset\n",
    "print(f\"Min pixel value: {images.min()}\")\n",
    "print(f\"Max pixel value: {images.max()}\")\n",
    "\n",
    "# Check the range of pixel values in the dataset\n",
    "print(f\"Min pixel value for Labels: {labels.min()}\")\n",
    "print(f\"Max pixel value for Labels: {labels.max()}\")\n",
    "\n",
    "\n",
    "#cv2.imshow('Sample Image', images[0])\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d66bfb-c780-4fe6-8283-53958df74c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepL_venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
