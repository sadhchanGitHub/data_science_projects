{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 33750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 22.185319900512695,
      "learning_rate": 4.926518518518518e-05,
      "loss": 2.0843,
      "step": 500
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 14.324646949768066,
      "learning_rate": 4.852444444444444e-05,
      "loss": 1.8824,
      "step": 1000
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 11.69240951538086,
      "learning_rate": 4.7783703703703706e-05,
      "loss": 1.8029,
      "step": 1500
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 11.071653366088867,
      "learning_rate": 4.7042962962962965e-05,
      "loss": 1.7759,
      "step": 2000
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 10.418721199035645,
      "learning_rate": 4.630370370370371e-05,
      "loss": 1.7542,
      "step": 2500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 9.386791229248047,
      "learning_rate": 4.5562962962962965e-05,
      "loss": 1.7301,
      "step": 3000
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 8.378546714782715,
      "learning_rate": 4.4822222222222224e-05,
      "loss": 1.7016,
      "step": 3500
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 8.472025871276855,
      "learning_rate": 4.408148148148148e-05,
      "loss": 1.6638,
      "step": 4000
    },
    {
      "epoch": 0.4,
      "grad_norm": 8.17807674407959,
      "learning_rate": 4.334074074074074e-05,
      "loss": 1.6544,
      "step": 4500
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 10.168551445007324,
      "learning_rate": 4.26e-05,
      "loss": 1.6272,
      "step": 5000
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 10.430829048156738,
      "learning_rate": 4.185925925925926e-05,
      "loss": 1.6364,
      "step": 5500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 7.708828449249268,
      "learning_rate": 4.111851851851852e-05,
      "loss": 1.6286,
      "step": 6000
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 6.814898490905762,
      "learning_rate": 4.037777777777778e-05,
      "loss": 1.6117,
      "step": 6500
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 11.544855117797852,
      "learning_rate": 3.9638518518518516e-05,
      "loss": 1.5962,
      "step": 7000
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.943668842315674,
      "learning_rate": 3.8897777777777774e-05,
      "loss": 1.6089,
      "step": 7500
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 12.458205223083496,
      "learning_rate": 3.815703703703704e-05,
      "loss": 1.5845,
      "step": 8000
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 8.125927925109863,
      "learning_rate": 3.74162962962963e-05,
      "loss": 1.5887,
      "step": 8500
    },
    {
      "epoch": 0.8,
      "grad_norm": 8.558561325073242,
      "learning_rate": 3.667703703703704e-05,
      "loss": 1.5795,
      "step": 9000
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 8.247987747192383,
      "learning_rate": 3.59362962962963e-05,
      "loss": 1.5585,
      "step": 9500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 8.908944129943848,
      "learning_rate": 3.519555555555556e-05,
      "loss": 1.559,
      "step": 10000
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 12.393251419067383,
      "learning_rate": 3.4454814814814815e-05,
      "loss": 1.5548,
      "step": 10500
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 6.642855644226074,
      "learning_rate": 3.371407407407407e-05,
      "loss": 1.5325,
      "step": 11000
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 11.559700965881348,
      "learning_rate": 3.2974814814814816e-05,
      "loss": 1.5186,
      "step": 11500
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 8.218401908874512,
      "learning_rate": 3.223407407407408e-05,
      "loss": 1.5112,
      "step": 12000
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 8.74992847442627,
      "learning_rate": 3.149333333333334e-05,
      "loss": 1.4994,
      "step": 12500
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 7.742073059082031,
      "learning_rate": 3.07525925925926e-05,
      "loss": 1.4948,
      "step": 13000
    },
    {
      "epoch": 1.2,
      "grad_norm": 8.690921783447266,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 1.483,
      "step": 13500
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 9.457795143127441,
      "learning_rate": 2.9272592592592595e-05,
      "loss": 1.4765,
      "step": 14000
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 8.01525592803955,
      "learning_rate": 2.8531851851851853e-05,
      "loss": 1.4762,
      "step": 14500
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 8.013822555541992,
      "learning_rate": 2.779111111111111e-05,
      "loss": 1.4883,
      "step": 15000
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 8.51690673828125,
      "learning_rate": 2.705037037037037e-05,
      "loss": 1.4766,
      "step": 15500
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 10.140066146850586,
      "learning_rate": 2.6311111111111115e-05,
      "loss": 1.4624,
      "step": 16000
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 7.549137115478516,
      "learning_rate": 2.5570370370370374e-05,
      "loss": 1.4924,
      "step": 16500
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 6.956478118896484,
      "learning_rate": 2.4829629629629632e-05,
      "loss": 1.4777,
      "step": 17000
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 9.183493614196777,
      "learning_rate": 2.408888888888889e-05,
      "loss": 1.468,
      "step": 17500
    },
    {
      "epoch": 1.6,
      "grad_norm": Infinity,
      "learning_rate": 2.334962962962963e-05,
      "loss": 1.4583,
      "step": 18000
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 9.846075057983398,
      "learning_rate": 2.260888888888889e-05,
      "loss": 1.4619,
      "step": 18500
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 7.5384368896484375,
      "learning_rate": 2.186814814814815e-05,
      "loss": 1.4763,
      "step": 19000
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 9.080018997192383,
      "learning_rate": 2.112888888888889e-05,
      "loss": 1.4609,
      "step": 19500
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 11.327052116394043,
      "learning_rate": 2.0388148148148146e-05,
      "loss": 1.4609,
      "step": 20000
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 7.296541690826416,
      "learning_rate": 1.9647407407407408e-05,
      "loss": 1.4596,
      "step": 20500
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 9.469728469848633,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 1.4626,
      "step": 21000
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 8.128583908081055,
      "learning_rate": 1.8165925925925928e-05,
      "loss": 1.4433,
      "step": 21500
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 8.093542098999023,
      "learning_rate": 1.7425185185185186e-05,
      "loss": 1.4681,
      "step": 22000
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.869649887084961,
      "learning_rate": 1.6684444444444445e-05,
      "loss": 1.4517,
      "step": 22500
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 10.255434036254883,
      "learning_rate": 1.5943703703703706e-05,
      "loss": 1.4037,
      "step": 23000
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 10.087589263916016,
      "learning_rate": 1.5204444444444445e-05,
      "loss": 1.4195,
      "step": 23500
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 10.717789649963379,
      "learning_rate": 1.4463703703703704e-05,
      "loss": 1.4011,
      "step": 24000
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 8.290473937988281,
      "learning_rate": 1.3722962962962962e-05,
      "loss": 1.4209,
      "step": 24500
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 8.659655570983887,
      "learning_rate": 1.2982222222222224e-05,
      "loss": 1.3997,
      "step": 25000
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 7.464287281036377,
      "learning_rate": 1.2241481481481482e-05,
      "loss": 1.4202,
      "step": 25500
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 10.73477554321289,
      "learning_rate": 1.1502222222222223e-05,
      "loss": 1.4058,
      "step": 26000
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 9.128948211669922,
      "learning_rate": 1.0761481481481483e-05,
      "loss": 1.401,
      "step": 26500
    },
    {
      "epoch": 2.4,
      "grad_norm": 9.344803810119629,
      "learning_rate": 1.0020740740740741e-05,
      "loss": 1.4053,
      "step": 27000
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 8.68384075164795,
      "learning_rate": 9.28e-06,
      "loss": 1.4026,
      "step": 27500
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 8.761444091796875,
      "learning_rate": 8.53925925925926e-06,
      "loss": 1.4086,
      "step": 28000
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 8.938729286193848,
      "learning_rate": 7.8e-06,
      "loss": 1.4003,
      "step": 28500
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 10.211456298828125,
      "learning_rate": 7.05925925925926e-06,
      "loss": 1.3989,
      "step": 29000
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 8.243492126464844,
      "learning_rate": 6.318518518518519e-06,
      "loss": 1.4052,
      "step": 29500
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 11.806243896484375,
      "learning_rate": 5.577777777777778e-06,
      "loss": 1.4131,
      "step": 30000
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 9.271256446838379,
      "learning_rate": 4.838518518518519e-06,
      "loss": 1.3981,
      "step": 30500
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 10.168973922729492,
      "learning_rate": 4.097777777777778e-06,
      "loss": 1.416,
      "step": 31000
    },
    {
      "epoch": 2.8,
      "grad_norm": 9.358492851257324,
      "learning_rate": 3.357037037037037e-06,
      "loss": 1.4053,
      "step": 31500
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 8.237030029296875,
      "learning_rate": 2.6162962962962967e-06,
      "loss": 1.3999,
      "step": 32000
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 8.614872932434082,
      "learning_rate": 1.8770370370370371e-06,
      "loss": 1.3945,
      "step": 32500
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 8.053933143615723,
      "learning_rate": 1.1362962962962965e-06,
      "loss": 1.3956,
      "step": 33000
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 8.207995414733887,
      "learning_rate": 3.955555555555556e-07,
      "loss": 1.3833,
      "step": 33500
    }
  ],
  "logging_steps": 500,
  "max_steps": 33750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.5482000698496e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
