{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 112500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 17.388330459594727,
      "learning_rate": 9.99137777777778e-06,
      "loss": 2.5239,
      "step": 100
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 15.297152519226074,
      "learning_rate": 9.982577777777779e-06,
      "loss": 2.2017,
      "step": 200
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 16.997190475463867,
      "learning_rate": 9.97368888888889e-06,
      "loss": 2.1843,
      "step": 300
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 18.57970428466797,
      "learning_rate": 9.964800000000002e-06,
      "loss": 2.1175,
      "step": 400
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 18.253337860107422,
      "learning_rate": 9.955911111111111e-06,
      "loss": 2.0691,
      "step": 500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 18.772064208984375,
      "learning_rate": 9.947022222222223e-06,
      "loss": 2.0603,
      "step": 600
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 18.24104881286621,
      "learning_rate": 9.938133333333334e-06,
      "loss": 2.0136,
      "step": 700
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 18.533178329467773,
      "learning_rate": 9.929244444444445e-06,
      "loss": 2.0571,
      "step": 800
    },
    {
      "epoch": 0.04,
      "grad_norm": 17.04436492919922,
      "learning_rate": 9.920355555555557e-06,
      "loss": 1.9986,
      "step": 900
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 18.302810668945312,
      "learning_rate": 9.911466666666666e-06,
      "loss": 1.9825,
      "step": 1000
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 17.93073081970215,
      "learning_rate": 9.90257777777778e-06,
      "loss": 1.9476,
      "step": 1100
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 19.28477668762207,
      "learning_rate": 9.89368888888889e-06,
      "loss": 1.9469,
      "step": 1200
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 17.502145767211914,
      "learning_rate": 9.8848e-06,
      "loss": 1.9748,
      "step": 1300
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 16.757835388183594,
      "learning_rate": 9.875911111111112e-06,
      "loss": 1.9157,
      "step": 1400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 17.788881301879883,
      "learning_rate": 9.867022222222223e-06,
      "loss": 1.9759,
      "step": 1500
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 15.665871620178223,
      "learning_rate": 9.858133333333335e-06,
      "loss": 1.9128,
      "step": 1600
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 14.819282531738281,
      "learning_rate": 9.849244444444446e-06,
      "loss": 1.9164,
      "step": 1700
    },
    {
      "epoch": 0.08,
      "grad_norm": 18.77085304260254,
      "learning_rate": 9.840355555555556e-06,
      "loss": 1.9463,
      "step": 1800
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 16.496150970458984,
      "learning_rate": 9.831466666666667e-06,
      "loss": 1.9569,
      "step": 1900
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 25.103160858154297,
      "learning_rate": 9.822577777777779e-06,
      "loss": 1.9127,
      "step": 2000
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 16.085351943969727,
      "learning_rate": 9.81368888888889e-06,
      "loss": 1.9225,
      "step": 2100
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 16.417327880859375,
      "learning_rate": 9.804800000000001e-06,
      "loss": 1.913,
      "step": 2200
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 16.416656494140625,
      "learning_rate": 9.795911111111111e-06,
      "loss": 1.8561,
      "step": 2300
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 13.902851104736328,
      "learning_rate": 9.787022222222222e-06,
      "loss": 1.8757,
      "step": 2400
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 18.490571975708008,
      "learning_rate": 9.778133333333334e-06,
      "loss": 1.8831,
      "step": 2500
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 15.05556583404541,
      "learning_rate": 9.769244444444445e-06,
      "loss": 1.8659,
      "step": 2600
    },
    {
      "epoch": 0.12,
      "grad_norm": 13.764554023742676,
      "learning_rate": 9.760355555555557e-06,
      "loss": 1.8669,
      "step": 2700
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 14.832228660583496,
      "learning_rate": 9.751466666666668e-06,
      "loss": 1.8532,
      "step": 2800
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 17.416955947875977,
      "learning_rate": 9.74257777777778e-06,
      "loss": 1.8403,
      "step": 2900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 13.371906280517578,
      "learning_rate": 9.733688888888889e-06,
      "loss": 1.8437,
      "step": 3000
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 18.324811935424805,
      "learning_rate": 9.7248e-06,
      "loss": 1.8593,
      "step": 3100
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 17.474910736083984,
      "learning_rate": 9.715911111111112e-06,
      "loss": 1.8662,
      "step": 3200
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 18.10265350341797,
      "learning_rate": 9.707022222222223e-06,
      "loss": 1.8786,
      "step": 3300
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 16.60758399963379,
      "learning_rate": 9.698133333333335e-06,
      "loss": 1.8268,
      "step": 3400
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 17.44681167602539,
      "learning_rate": 9.689244444444446e-06,
      "loss": 1.7954,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 17.312105178833008,
      "learning_rate": 9.680355555555556e-06,
      "loss": 1.8502,
      "step": 3600
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 12.048954963684082,
      "learning_rate": 9.671466666666667e-06,
      "loss": 1.8138,
      "step": 3700
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 13.084237098693848,
      "learning_rate": 9.662577777777778e-06,
      "loss": 1.8076,
      "step": 3800
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 16.204092025756836,
      "learning_rate": 9.65368888888889e-06,
      "loss": 1.8413,
      "step": 3900
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 13.554156303405762,
      "learning_rate": 9.644800000000001e-06,
      "loss": 1.7997,
      "step": 4000
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 14.89533805847168,
      "learning_rate": 9.635911111111111e-06,
      "loss": 1.8335,
      "step": 4100
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 11.877056121826172,
      "learning_rate": 9.627111111111112e-06,
      "loss": 1.8226,
      "step": 4200
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 12.016780853271484,
      "learning_rate": 9.618222222222223e-06,
      "loss": 1.8352,
      "step": 4300
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 15.573159217834473,
      "learning_rate": 9.609333333333333e-06,
      "loss": 1.7861,
      "step": 4400
    },
    {
      "epoch": 0.2,
      "grad_norm": 10.65854549407959,
      "learning_rate": 9.600444444444446e-06,
      "loss": 1.8184,
      "step": 4500
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 13.277341842651367,
      "learning_rate": 9.591555555555556e-06,
      "loss": 1.8023,
      "step": 4600
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 15.705934524536133,
      "learning_rate": 9.582666666666667e-06,
      "loss": 1.8113,
      "step": 4700
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 16.005813598632812,
      "learning_rate": 9.573777777777779e-06,
      "loss": 1.8082,
      "step": 4800
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 11.886817932128906,
      "learning_rate": 9.56488888888889e-06,
      "loss": 1.8059,
      "step": 4900
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 14.310324668884277,
      "learning_rate": 9.556000000000001e-06,
      "loss": 1.8324,
      "step": 5000
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 15.168248176574707,
      "learning_rate": 9.547111111111111e-06,
      "loss": 1.7786,
      "step": 5100
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 13.86662769317627,
      "learning_rate": 9.538222222222222e-06,
      "loss": 1.7824,
      "step": 5200
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 14.12816333770752,
      "learning_rate": 9.529333333333334e-06,
      "loss": 1.7969,
      "step": 5300
    },
    {
      "epoch": 0.24,
      "grad_norm": 13.691091537475586,
      "learning_rate": 9.520444444444445e-06,
      "loss": 1.8066,
      "step": 5400
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 11.227616310119629,
      "learning_rate": 9.511555555555557e-06,
      "loss": 1.7616,
      "step": 5500
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 9.762173652648926,
      "learning_rate": 9.502666666666668e-06,
      "loss": 1.8105,
      "step": 5600
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 16.057554244995117,
      "learning_rate": 9.493777777777778e-06,
      "loss": 1.7768,
      "step": 5700
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 12.824868202209473,
      "learning_rate": 9.48488888888889e-06,
      "loss": 1.7561,
      "step": 5800
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 12.752138137817383,
      "learning_rate": 9.476e-06,
      "loss": 1.783,
      "step": 5900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 14.644753456115723,
      "learning_rate": 9.467111111111112e-06,
      "loss": 1.7715,
      "step": 6000
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 12.042257308959961,
      "learning_rate": 9.458222222222223e-06,
      "loss": 1.7784,
      "step": 6100
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 16.984880447387695,
      "learning_rate": 9.449422222222223e-06,
      "loss": 1.7772,
      "step": 6200
    },
    {
      "epoch": 0.28,
      "grad_norm": 10.879103660583496,
      "learning_rate": 9.440533333333334e-06,
      "loss": 1.8327,
      "step": 6300
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 11.500114440917969,
      "learning_rate": 9.431644444444445e-06,
      "loss": 1.7628,
      "step": 6400
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 13.261302947998047,
      "learning_rate": 9.422755555555557e-06,
      "loss": 1.7208,
      "step": 6500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 10.421128273010254,
      "learning_rate": 9.413866666666668e-06,
      "loss": 1.7596,
      "step": 6600
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 9.510852813720703,
      "learning_rate": 9.404977777777778e-06,
      "loss": 1.7502,
      "step": 6700
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 10.070283889770508,
      "learning_rate": 9.39608888888889e-06,
      "loss": 1.7449,
      "step": 6800
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 11.000155448913574,
      "learning_rate": 9.3872e-06,
      "loss": 1.7262,
      "step": 6900
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 12.140169143676758,
      "learning_rate": 9.378311111111112e-06,
      "loss": 1.7516,
      "step": 7000
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 11.949451446533203,
      "learning_rate": 9.369422222222223e-06,
      "loss": 1.7347,
      "step": 7100
    },
    {
      "epoch": 0.32,
      "grad_norm": 9.843961715698242,
      "learning_rate": 9.360533333333333e-06,
      "loss": 1.7336,
      "step": 7200
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 12.260293006896973,
      "learning_rate": 9.351644444444446e-06,
      "loss": 1.7079,
      "step": 7300
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 10.42822265625,
      "learning_rate": 9.342755555555556e-06,
      "loss": 1.711,
      "step": 7400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 9.431721687316895,
      "learning_rate": 9.333866666666667e-06,
      "loss": 1.7562,
      "step": 7500
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 9.12606430053711,
      "learning_rate": 9.324977777777779e-06,
      "loss": 1.7265,
      "step": 7600
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 10.723414421081543,
      "learning_rate": 9.31608888888889e-06,
      "loss": 1.7779,
      "step": 7700
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 9.323163986206055,
      "learning_rate": 9.307200000000001e-06,
      "loss": 1.7318,
      "step": 7800
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 10.922049522399902,
      "learning_rate": 9.298311111111113e-06,
      "loss": 1.7343,
      "step": 7900
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 13.73877239227295,
      "learning_rate": 9.289422222222222e-06,
      "loss": 1.7317,
      "step": 8000
    },
    {
      "epoch": 0.36,
      "grad_norm": 17.39781951904297,
      "learning_rate": 9.280533333333334e-06,
      "loss": 1.6896,
      "step": 8100
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 12.814717292785645,
      "learning_rate": 9.271733333333333e-06,
      "loss": 1.7476,
      "step": 8200
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 10.385541915893555,
      "learning_rate": 9.262844444444446e-06,
      "loss": 1.733,
      "step": 8300
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 10.067367553710938,
      "learning_rate": 9.253955555555556e-06,
      "loss": 1.7421,
      "step": 8400
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 10.894796371459961,
      "learning_rate": 9.245066666666667e-06,
      "loss": 1.7096,
      "step": 8500
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 12.3491792678833,
      "learning_rate": 9.236177777777779e-06,
      "loss": 1.7398,
      "step": 8600
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 10.280139923095703,
      "learning_rate": 9.22728888888889e-06,
      "loss": 1.7039,
      "step": 8700
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 10.620744705200195,
      "learning_rate": 9.218400000000001e-06,
      "loss": 1.7074,
      "step": 8800
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 10.807515144348145,
      "learning_rate": 9.209511111111113e-06,
      "loss": 1.6719,
      "step": 8900
    },
    {
      "epoch": 0.4,
      "grad_norm": 10.173757553100586,
      "learning_rate": 9.200622222222222e-06,
      "loss": 1.7314,
      "step": 9000
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 10.942221641540527,
      "learning_rate": 9.191733333333334e-06,
      "loss": 1.7055,
      "step": 9100
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 11.87822437286377,
      "learning_rate": 9.182844444444445e-06,
      "loss": 1.6639,
      "step": 9200
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 13.548543930053711,
      "learning_rate": 9.173955555555557e-06,
      "loss": 1.703,
      "step": 9300
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 10.284761428833008,
      "learning_rate": 9.165066666666668e-06,
      "loss": 1.7348,
      "step": 9400
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 10.872702598571777,
      "learning_rate": 9.156177777777778e-06,
      "loss": 1.6905,
      "step": 9500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 9.855941772460938,
      "learning_rate": 9.14728888888889e-06,
      "loss": 1.7279,
      "step": 9600
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 11.923169136047363,
      "learning_rate": 9.1384e-06,
      "loss": 1.7317,
      "step": 9700
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 9.642537117004395,
      "learning_rate": 9.129511111111112e-06,
      "loss": 1.6809,
      "step": 9800
    },
    {
      "epoch": 0.44,
      "grad_norm": 9.469614028930664,
      "learning_rate": 9.120622222222223e-06,
      "loss": 1.699,
      "step": 9900
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 10.01758861541748,
      "learning_rate": 9.111733333333333e-06,
      "loss": 1.7359,
      "step": 10000
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 9.720852851867676,
      "learning_rate": 9.102844444444446e-06,
      "loss": 1.706,
      "step": 10100
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 9.986241340637207,
      "learning_rate": 9.093955555555556e-06,
      "loss": 1.7063,
      "step": 10200
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 10.34422492980957,
      "learning_rate": 9.085155555555557e-06,
      "loss": 1.7272,
      "step": 10300
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 12.663330078125,
      "learning_rate": 9.076266666666668e-06,
      "loss": 1.7181,
      "step": 10400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 10.301664352416992,
      "learning_rate": 9.067377777777778e-06,
      "loss": 1.6938,
      "step": 10500
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 11.301789283752441,
      "learning_rate": 9.05848888888889e-06,
      "loss": 1.6953,
      "step": 10600
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 9.808507919311523,
      "learning_rate": 9.0496e-06,
      "loss": 1.702,
      "step": 10700
    },
    {
      "epoch": 0.48,
      "grad_norm": 10.915820121765137,
      "learning_rate": 9.040711111111112e-06,
      "loss": 1.7045,
      "step": 10800
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 10.194230079650879,
      "learning_rate": 9.031822222222223e-06,
      "loss": 1.7329,
      "step": 10900
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 9.148147583007812,
      "learning_rate": 9.022933333333333e-06,
      "loss": 1.6767,
      "step": 11000
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 9.405537605285645,
      "learning_rate": 9.014044444444446e-06,
      "loss": 1.7107,
      "step": 11100
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 10.658920288085938,
      "learning_rate": 9.005155555555556e-06,
      "loss": 1.6887,
      "step": 11200
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 11.727605819702148,
      "learning_rate": 8.996266666666667e-06,
      "loss": 1.7156,
      "step": 11300
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 9.040508270263672,
      "learning_rate": 8.987377777777779e-06,
      "loss": 1.6817,
      "step": 11400
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 10.149596214294434,
      "learning_rate": 8.97848888888889e-06,
      "loss": 1.7147,
      "step": 11500
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 9.129467010498047,
      "learning_rate": 8.969600000000001e-06,
      "loss": 1.7036,
      "step": 11600
    },
    {
      "epoch": 0.52,
      "grad_norm": 8.030245780944824,
      "learning_rate": 8.960711111111113e-06,
      "loss": 1.6912,
      "step": 11700
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 8.971735954284668,
      "learning_rate": 8.951822222222222e-06,
      "loss": 1.6763,
      "step": 11800
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 10.432539939880371,
      "learning_rate": 8.942933333333334e-06,
      "loss": 1.7113,
      "step": 11900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 7.971395015716553,
      "learning_rate": 8.934044444444445e-06,
      "loss": 1.7049,
      "step": 12000
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 10.118154525756836,
      "learning_rate": 8.925155555555557e-06,
      "loss": 1.6999,
      "step": 12100
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 10.822587966918945,
      "learning_rate": 8.916266666666668e-06,
      "loss": 1.6674,
      "step": 12200
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 7.188018321990967,
      "learning_rate": 8.907466666666667e-06,
      "loss": 1.7044,
      "step": 12300
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 10.695361137390137,
      "learning_rate": 8.898577777777779e-06,
      "loss": 1.6603,
      "step": 12400
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 8.900588035583496,
      "learning_rate": 8.88968888888889e-06,
      "loss": 1.7018,
      "step": 12500
    },
    {
      "epoch": 0.56,
      "grad_norm": 12.057779312133789,
      "learning_rate": 8.8808e-06,
      "loss": 1.6879,
      "step": 12600
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 10.189815521240234,
      "learning_rate": 8.871911111111113e-06,
      "loss": 1.7067,
      "step": 12700
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 10.131999969482422,
      "learning_rate": 8.863022222222223e-06,
      "loss": 1.6716,
      "step": 12800
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 9.462944030761719,
      "learning_rate": 8.854133333333334e-06,
      "loss": 1.6619,
      "step": 12900
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 8.634381294250488,
      "learning_rate": 8.845244444444445e-06,
      "loss": 1.6877,
      "step": 13000
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 7.763330936431885,
      "learning_rate": 8.836355555555555e-06,
      "loss": 1.68,
      "step": 13100
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 9.522828102111816,
      "learning_rate": 8.827466666666668e-06,
      "loss": 1.6403,
      "step": 13200
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 8.237302780151367,
      "learning_rate": 8.818577777777778e-06,
      "loss": 1.6779,
      "step": 13300
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 7.0724945068359375,
      "learning_rate": 8.809688888888889e-06,
      "loss": 1.7058,
      "step": 13400
    },
    {
      "epoch": 0.6,
      "grad_norm": 9.826648712158203,
      "learning_rate": 8.8008e-06,
      "loss": 1.6762,
      "step": 13500
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 9.752035140991211,
      "learning_rate": 8.791911111111112e-06,
      "loss": 1.6663,
      "step": 13600
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 8.073930740356445,
      "learning_rate": 8.783022222222223e-06,
      "loss": 1.6946,
      "step": 13700
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 9.249505043029785,
      "learning_rate": 8.774133333333335e-06,
      "loss": 1.6309,
      "step": 13800
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 9.818511009216309,
      "learning_rate": 8.765244444444444e-06,
      "loss": 1.6672,
      "step": 13900
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 9.497021675109863,
      "learning_rate": 8.756355555555556e-06,
      "loss": 1.6845,
      "step": 14000
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 7.652246952056885,
      "learning_rate": 8.747466666666667e-06,
      "loss": 1.6371,
      "step": 14100
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 9.560464859008789,
      "learning_rate": 8.738577777777779e-06,
      "loss": 1.6246,
      "step": 14200
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 7.357038497924805,
      "learning_rate": 8.72968888888889e-06,
      "loss": 1.6771,
      "step": 14300
    },
    {
      "epoch": 0.64,
      "grad_norm": 9.394508361816406,
      "learning_rate": 8.7208e-06,
      "loss": 1.6371,
      "step": 14400
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 7.964039325714111,
      "learning_rate": 8.711911111111113e-06,
      "loss": 1.6223,
      "step": 14500
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 10.275206565856934,
      "learning_rate": 8.703111111111112e-06,
      "loss": 1.672,
      "step": 14600
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 8.031740188598633,
      "learning_rate": 8.694222222222223e-06,
      "loss": 1.62,
      "step": 14700
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 8.533313751220703,
      "learning_rate": 8.685333333333335e-06,
      "loss": 1.6496,
      "step": 14800
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 9.507645606994629,
      "learning_rate": 8.676444444444444e-06,
      "loss": 1.6449,
      "step": 14900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.849364757537842,
      "learning_rate": 8.667555555555556e-06,
      "loss": 1.6443,
      "step": 15000
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 7.613852024078369,
      "learning_rate": 8.658666666666667e-06,
      "loss": 1.6485,
      "step": 15100
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 7.4144062995910645,
      "learning_rate": 8.649777777777779e-06,
      "loss": 1.6324,
      "step": 15200
    },
    {
      "epoch": 0.68,
      "grad_norm": 10.9335298538208,
      "learning_rate": 8.64088888888889e-06,
      "loss": 1.6408,
      "step": 15300
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 8.390998840332031,
      "learning_rate": 8.632e-06,
      "loss": 1.6778,
      "step": 15400
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 9.599309921264648,
      "learning_rate": 8.623111111111113e-06,
      "loss": 1.6611,
      "step": 15500
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 9.75041389465332,
      "learning_rate": 8.614222222222222e-06,
      "loss": 1.6659,
      "step": 15600
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 8.453857421875,
      "learning_rate": 8.605333333333334e-06,
      "loss": 1.6082,
      "step": 15700
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 7.816817283630371,
      "learning_rate": 8.596444444444445e-06,
      "loss": 1.6471,
      "step": 15800
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 10.411517143249512,
      "learning_rate": 8.587555555555557e-06,
      "loss": 1.6287,
      "step": 15900
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 9.99903678894043,
      "learning_rate": 8.578666666666668e-06,
      "loss": 1.6555,
      "step": 16000
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 7.137473106384277,
      "learning_rate": 8.569777777777778e-06,
      "loss": 1.6513,
      "step": 16100
    },
    {
      "epoch": 0.72,
      "grad_norm": 9.68210506439209,
      "learning_rate": 8.560888888888889e-06,
      "loss": 1.6662,
      "step": 16200
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 9.617753028869629,
      "learning_rate": 8.552e-06,
      "loss": 1.6359,
      "step": 16300
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 6.820323467254639,
      "learning_rate": 8.543111111111112e-06,
      "loss": 1.632,
      "step": 16400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 10.73631477355957,
      "learning_rate": 8.534222222222223e-06,
      "loss": 1.6578,
      "step": 16500
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 7.06068754196167,
      "learning_rate": 8.525333333333335e-06,
      "loss": 1.6367,
      "step": 16600
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 7.139745235443115,
      "learning_rate": 8.516444444444444e-06,
      "loss": 1.6374,
      "step": 16700
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 10.950468063354492,
      "learning_rate": 8.507555555555557e-06,
      "loss": 1.6219,
      "step": 16800
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 7.788507461547852,
      "learning_rate": 8.498666666666667e-06,
      "loss": 1.5976,
      "step": 16900
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 8.566888809204102,
      "learning_rate": 8.489777777777778e-06,
      "loss": 1.6539,
      "step": 17000
    },
    {
      "epoch": 0.76,
      "grad_norm": 6.657455921173096,
      "learning_rate": 8.48088888888889e-06,
      "loss": 1.6026,
      "step": 17100
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 6.726129531860352,
      "learning_rate": 8.47208888888889e-06,
      "loss": 1.6371,
      "step": 17200
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 9.021007537841797,
      "learning_rate": 8.4632e-06,
      "loss": 1.6345,
      "step": 17300
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 8.984150886535645,
      "learning_rate": 8.454311111111112e-06,
      "loss": 1.6305,
      "step": 17400
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 7.172377586364746,
      "learning_rate": 8.445422222222223e-06,
      "loss": 1.6333,
      "step": 17500
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 7.673545837402344,
      "learning_rate": 8.436533333333335e-06,
      "loss": 1.6449,
      "step": 17600
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 6.202629566192627,
      "learning_rate": 8.427644444444444e-06,
      "loss": 1.6271,
      "step": 17700
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 7.6658477783203125,
      "learning_rate": 8.418755555555557e-06,
      "loss": 1.6368,
      "step": 17800
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 6.899865627288818,
      "learning_rate": 8.409866666666667e-06,
      "loss": 1.6164,
      "step": 17900
    },
    {
      "epoch": 0.8,
      "grad_norm": 8.96827220916748,
      "learning_rate": 8.400977777777779e-06,
      "loss": 1.6345,
      "step": 18000
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 6.600774765014648,
      "learning_rate": 8.39208888888889e-06,
      "loss": 1.6232,
      "step": 18100
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 6.6473917961120605,
      "learning_rate": 8.3832e-06,
      "loss": 1.6197,
      "step": 18200
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 7.549502849578857,
      "learning_rate": 8.374311111111113e-06,
      "loss": 1.5936,
      "step": 18300
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 8.4423828125,
      "learning_rate": 8.365422222222222e-06,
      "loss": 1.6078,
      "step": 18400
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 6.629007816314697,
      "learning_rate": 8.356533333333334e-06,
      "loss": 1.6628,
      "step": 18500
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 10.268714904785156,
      "learning_rate": 8.347644444444445e-06,
      "loss": 1.6375,
      "step": 18600
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 8.607678413391113,
      "learning_rate": 8.338755555555557e-06,
      "loss": 1.6094,
      "step": 18700
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 7.6094136238098145,
      "learning_rate": 8.329866666666668e-06,
      "loss": 1.6411,
      "step": 18800
    },
    {
      "epoch": 0.84,
      "grad_norm": 9.94395923614502,
      "learning_rate": 8.32097777777778e-06,
      "loss": 1.6523,
      "step": 18900
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 7.436761856079102,
      "learning_rate": 8.312088888888889e-06,
      "loss": 1.6344,
      "step": 19000
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 6.486690998077393,
      "learning_rate": 8.3032e-06,
      "loss": 1.6335,
      "step": 19100
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 8.458440780639648,
      "learning_rate": 8.294311111111112e-06,
      "loss": 1.6413,
      "step": 19200
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 7.614358901977539,
      "learning_rate": 8.285422222222223e-06,
      "loss": 1.595,
      "step": 19300
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 7.695815086364746,
      "learning_rate": 8.276622222222223e-06,
      "loss": 1.6075,
      "step": 19400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 5.950987815856934,
      "learning_rate": 8.267733333333334e-06,
      "loss": 1.6594,
      "step": 19500
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 6.752473831176758,
      "learning_rate": 8.258844444444445e-06,
      "loss": 1.584,
      "step": 19600
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 8.3783597946167,
      "learning_rate": 8.249955555555557e-06,
      "loss": 1.6583,
      "step": 19700
    },
    {
      "epoch": 0.88,
      "grad_norm": 9.332966804504395,
      "learning_rate": 8.241066666666668e-06,
      "loss": 1.5932,
      "step": 19800
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 8.672566413879395,
      "learning_rate": 8.23217777777778e-06,
      "loss": 1.6253,
      "step": 19900
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 7.815343379974365,
      "learning_rate": 8.223288888888889e-06,
      "loss": 1.6206,
      "step": 20000
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 8.190935134887695,
      "learning_rate": 8.2144e-06,
      "loss": 1.6511,
      "step": 20100
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 8.077916145324707,
      "learning_rate": 8.205511111111112e-06,
      "loss": 1.595,
      "step": 20200
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 8.664311408996582,
      "learning_rate": 8.196622222222223e-06,
      "loss": 1.6082,
      "step": 20300
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 5.9335832595825195,
      "learning_rate": 8.187733333333335e-06,
      "loss": 1.5958,
      "step": 20400
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 7.462915897369385,
      "learning_rate": 8.178844444444444e-06,
      "loss": 1.6367,
      "step": 20500
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 7.8472723960876465,
      "learning_rate": 8.169955555555556e-06,
      "loss": 1.625,
      "step": 20600
    },
    {
      "epoch": 0.92,
      "grad_norm": 7.444392681121826,
      "learning_rate": 8.161066666666667e-06,
      "loss": 1.598,
      "step": 20700
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 7.220822811126709,
      "learning_rate": 8.152177777777778e-06,
      "loss": 1.6024,
      "step": 20800
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 9.68114185333252,
      "learning_rate": 8.14328888888889e-06,
      "loss": 1.6026,
      "step": 20900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 6.617390155792236,
      "learning_rate": 8.134400000000001e-06,
      "loss": 1.6026,
      "step": 21000
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 7.811330795288086,
      "learning_rate": 8.125511111111111e-06,
      "loss": 1.5902,
      "step": 21100
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 6.562040328979492,
      "learning_rate": 8.116622222222222e-06,
      "loss": 1.5812,
      "step": 21200
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 8.382641792297363,
      "learning_rate": 8.107733333333334e-06,
      "loss": 1.6306,
      "step": 21300
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 7.373388290405273,
      "learning_rate": 8.098844444444445e-06,
      "loss": 1.6184,
      "step": 21400
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 7.544788360595703,
      "learning_rate": 8.089955555555556e-06,
      "loss": 1.6409,
      "step": 21500
    },
    {
      "epoch": 0.96,
      "grad_norm": 7.320038318634033,
      "learning_rate": 8.081066666666668e-06,
      "loss": 1.6395,
      "step": 21600
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 8.033020973205566,
      "learning_rate": 8.07217777777778e-06,
      "loss": 1.592,
      "step": 21700
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 6.521832466125488,
      "learning_rate": 8.063288888888889e-06,
      "loss": 1.5992,
      "step": 21800
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 7.836426258087158,
      "learning_rate": 8.0544e-06,
      "loss": 1.6386,
      "step": 21900
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 9.387101173400879,
      "learning_rate": 8.045511111111112e-06,
      "loss": 1.6123,
      "step": 22000
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 8.025131225585938,
      "learning_rate": 8.036622222222223e-06,
      "loss": 1.5849,
      "step": 22100
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 7.736831188201904,
      "learning_rate": 8.027733333333334e-06,
      "loss": 1.6017,
      "step": 22200
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 7.7090840339660645,
      "learning_rate": 8.018844444444444e-06,
      "loss": 1.6117,
      "step": 22300
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 8.107121467590332,
      "learning_rate": 8.009955555555556e-06,
      "loss": 1.6061,
      "step": 22400
    },
    {
      "epoch": 1.0,
      "grad_norm": 6.659148216247559,
      "learning_rate": 8.001066666666667e-06,
      "loss": 1.6409,
      "step": 22500
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 7.932631492614746,
      "learning_rate": 7.992177777777778e-06,
      "loss": 1.5982,
      "step": 22600
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 7.146259784698486,
      "learning_rate": 7.98328888888889e-06,
      "loss": 1.5532,
      "step": 22700
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 5.4986491203308105,
      "learning_rate": 7.974400000000001e-06,
      "loss": 1.5884,
      "step": 22800
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 5.687443733215332,
      "learning_rate": 7.96551111111111e-06,
      "loss": 1.5923,
      "step": 22900
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 6.1999101638793945,
      "learning_rate": 7.956622222222224e-06,
      "loss": 1.5722,
      "step": 23000
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 6.785337924957275,
      "learning_rate": 7.947733333333334e-06,
      "loss": 1.6086,
      "step": 23100
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 6.401342391967773,
      "learning_rate": 7.938844444444445e-06,
      "loss": 1.6115,
      "step": 23200
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 6.894389629364014,
      "learning_rate": 7.929955555555556e-06,
      "loss": 1.5817,
      "step": 23300
    },
    {
      "epoch": 1.04,
      "grad_norm": 8.28018569946289,
      "learning_rate": 7.921155555555556e-06,
      "loss": 1.5786,
      "step": 23400
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 6.259400844573975,
      "learning_rate": 7.912266666666667e-06,
      "loss": 1.595,
      "step": 23500
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 8.374893188476562,
      "learning_rate": 7.903377777777778e-06,
      "loss": 1.5762,
      "step": 23600
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 8.769479751586914,
      "learning_rate": 7.89448888888889e-06,
      "loss": 1.5882,
      "step": 23700
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 7.0068888664245605,
      "learning_rate": 7.885600000000001e-06,
      "loss": 1.6177,
      "step": 23800
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 6.643163681030273,
      "learning_rate": 7.876711111111111e-06,
      "loss": 1.6018,
      "step": 23900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 8.110276222229004,
      "learning_rate": 7.867822222222224e-06,
      "loss": 1.6197,
      "step": 24000
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 6.689886093139648,
      "learning_rate": 7.858933333333334e-06,
      "loss": 1.575,
      "step": 24100
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 7.614892959594727,
      "learning_rate": 7.850133333333335e-06,
      "loss": 1.5652,
      "step": 24200
    },
    {
      "epoch": 1.08,
      "grad_norm": 6.388031959533691,
      "learning_rate": 7.841244444444444e-06,
      "loss": 1.5771,
      "step": 24300
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 9.326695442199707,
      "learning_rate": 7.832355555555556e-06,
      "loss": 1.5989,
      "step": 24400
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 6.254749298095703,
      "learning_rate": 7.823466666666667e-06,
      "loss": 1.5906,
      "step": 24500
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 7.425785064697266,
      "learning_rate": 7.814577777777779e-06,
      "loss": 1.5477,
      "step": 24600
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 6.93689489364624,
      "learning_rate": 7.80568888888889e-06,
      "loss": 1.6216,
      "step": 24700
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 7.174461364746094,
      "learning_rate": 7.796800000000001e-06,
      "loss": 1.597,
      "step": 24800
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 6.8939313888549805,
      "learning_rate": 7.787911111111111e-06,
      "loss": 1.6026,
      "step": 24900
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 6.861616611480713,
      "learning_rate": 7.779022222222224e-06,
      "loss": 1.564,
      "step": 25000
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 7.641388893127441,
      "learning_rate": 7.770133333333334e-06,
      "loss": 1.5875,
      "step": 25100
    },
    {
      "epoch": 1.12,
      "grad_norm": 7.865501880645752,
      "learning_rate": 7.761244444444445e-06,
      "loss": 1.5833,
      "step": 25200
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 7.109558582305908,
      "learning_rate": 7.752355555555557e-06,
      "loss": 1.5957,
      "step": 25300
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 7.107778549194336,
      "learning_rate": 7.743466666666666e-06,
      "loss": 1.5951,
      "step": 25400
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 6.820830821990967,
      "learning_rate": 7.73457777777778e-06,
      "loss": 1.5898,
      "step": 25500
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 6.92511510848999,
      "learning_rate": 7.725688888888889e-06,
      "loss": 1.5905,
      "step": 25600
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 8.571768760681152,
      "learning_rate": 7.7168e-06,
      "loss": 1.624,
      "step": 25700
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 6.309203147888184,
      "learning_rate": 7.707911111111112e-06,
      "loss": 1.6336,
      "step": 25800
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 7.88905668258667,
      "learning_rate": 7.699022222222223e-06,
      "loss": 1.605,
      "step": 25900
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 8.97355842590332,
      "learning_rate": 7.690133333333335e-06,
      "loss": 1.5872,
      "step": 26000
    },
    {
      "epoch": 1.16,
      "grad_norm": 7.422603130340576,
      "learning_rate": 7.681244444444446e-06,
      "loss": 1.5659,
      "step": 26100
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 6.681830406188965,
      "learning_rate": 7.672355555555556e-06,
      "loss": 1.6106,
      "step": 26200
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 7.6613287925720215,
      "learning_rate": 7.663555555555557e-06,
      "loss": 1.5434,
      "step": 26300
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 7.962862014770508,
      "learning_rate": 7.654666666666666e-06,
      "loss": 1.5475,
      "step": 26400
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 6.370100975036621,
      "learning_rate": 7.64577777777778e-06,
      "loss": 1.5734,
      "step": 26500
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 8.556066513061523,
      "learning_rate": 7.636888888888889e-06,
      "loss": 1.5713,
      "step": 26600
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 5.866230010986328,
      "learning_rate": 7.628000000000001e-06,
      "loss": 1.5863,
      "step": 26700
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 7.438526630401611,
      "learning_rate": 7.619111111111112e-06,
      "loss": 1.5814,
      "step": 26800
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 7.702383041381836,
      "learning_rate": 7.610222222222223e-06,
      "loss": 1.578,
      "step": 26900
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.642817497253418,
      "learning_rate": 7.601333333333334e-06,
      "loss": 1.5713,
      "step": 27000
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 5.6157307624816895,
      "learning_rate": 7.592444444444446e-06,
      "loss": 1.5731,
      "step": 27100
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 7.1586012840271,
      "learning_rate": 7.5835555555555566e-06,
      "loss": 1.6198,
      "step": 27200
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 6.425991535186768,
      "learning_rate": 7.574666666666667e-06,
      "loss": 1.5852,
      "step": 27300
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 6.6092848777771,
      "learning_rate": 7.5657777777777785e-06,
      "loss": 1.6167,
      "step": 27400
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 6.2680511474609375,
      "learning_rate": 7.556888888888889e-06,
      "loss": 1.5545,
      "step": 27500
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 8.78457260131836,
      "learning_rate": 7.548000000000001e-06,
      "loss": 1.5651,
      "step": 27600
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 6.5936479568481445,
      "learning_rate": 7.539111111111112e-06,
      "loss": 1.5818,
      "step": 27700
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 7.897549629211426,
      "learning_rate": 7.530222222222223e-06,
      "loss": 1.6,
      "step": 27800
    },
    {
      "epoch": 1.24,
      "grad_norm": 8.465332984924316,
      "learning_rate": 7.521333333333334e-06,
      "loss": 1.5867,
      "step": 27900
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 5.552760601043701,
      "learning_rate": 7.512444444444446e-06,
      "loss": 1.5562,
      "step": 28000
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 6.7730937004089355,
      "learning_rate": 7.5035555555555565e-06,
      "loss": 1.5688,
      "step": 28100
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 7.173166751861572,
      "learning_rate": 7.494666666666667e-06,
      "loss": 1.5848,
      "step": 28200
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 7.592113494873047,
      "learning_rate": 7.485777777777778e-06,
      "loss": 1.55,
      "step": 28300
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 6.1944355964660645,
      "learning_rate": 7.476888888888889e-06,
      "loss": 1.5817,
      "step": 28400
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 7.807333469390869,
      "learning_rate": 7.468000000000001e-06,
      "loss": 1.5578,
      "step": 28500
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 6.674444675445557,
      "learning_rate": 7.459111111111112e-06,
      "loss": 1.5804,
      "step": 28600
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 5.766730785369873,
      "learning_rate": 7.450222222222223e-06,
      "loss": 1.6111,
      "step": 28700
    },
    {
      "epoch": 1.28,
      "grad_norm": 6.227319240570068,
      "learning_rate": 7.441333333333334e-06,
      "loss": 1.552,
      "step": 28800
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 6.967386722564697,
      "learning_rate": 7.432444444444446e-06,
      "loss": 1.5889,
      "step": 28900
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 7.051300048828125,
      "learning_rate": 7.423555555555556e-06,
      "loss": 1.5652,
      "step": 29000
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 6.620711803436279,
      "learning_rate": 7.414755555555556e-06,
      "loss": 1.5643,
      "step": 29100
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 6.74252986907959,
      "learning_rate": 7.405866666666667e-06,
      "loss": 1.5509,
      "step": 29200
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 5.365945339202881,
      "learning_rate": 7.3969777777777785e-06,
      "loss": 1.5553,
      "step": 29300
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 6.520283222198486,
      "learning_rate": 7.388088888888889e-06,
      "loss": 1.5914,
      "step": 29400
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 6.955718994140625,
      "learning_rate": 7.3792000000000004e-06,
      "loss": 1.6039,
      "step": 29500
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 8.127215385437012,
      "learning_rate": 7.370311111111112e-06,
      "loss": 1.5875,
      "step": 29600
    },
    {
      "epoch": 1.32,
      "grad_norm": 5.897741794586182,
      "learning_rate": 7.361422222222223e-06,
      "loss": 1.5699,
      "step": 29700
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 6.550039768218994,
      "learning_rate": 7.352533333333334e-06,
      "loss": 1.5842,
      "step": 29800
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 6.0433526039123535,
      "learning_rate": 7.343644444444445e-06,
      "loss": 1.5728,
      "step": 29900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 6.584390640258789,
      "learning_rate": 7.334755555555556e-06,
      "loss": 1.5829,
      "step": 30000
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 5.653693199157715,
      "learning_rate": 7.325866666666668e-06,
      "loss": 1.6131,
      "step": 30100
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 6.434367656707764,
      "learning_rate": 7.3169777777777784e-06,
      "loss": 1.5999,
      "step": 30200
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 6.658377170562744,
      "learning_rate": 7.308088888888889e-06,
      "loss": 1.5595,
      "step": 30300
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 9.357048034667969,
      "learning_rate": 7.2992e-06,
      "loss": 1.5649,
      "step": 30400
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 5.666801452636719,
      "learning_rate": 7.290311111111112e-06,
      "loss": 1.5559,
      "step": 30500
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 6.70497989654541,
      "learning_rate": 7.281422222222223e-06,
      "loss": 1.564,
      "step": 30600
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 6.274736404418945,
      "learning_rate": 7.272533333333334e-06,
      "loss": 1.5569,
      "step": 30700
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 9.971829414367676,
      "learning_rate": 7.263644444444445e-06,
      "loss": 1.5487,
      "step": 30800
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 6.322424411773682,
      "learning_rate": 7.254755555555556e-06,
      "loss": 1.5666,
      "step": 30900
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 6.814370155334473,
      "learning_rate": 7.245866666666668e-06,
      "loss": 1.5603,
      "step": 31000
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 6.601344585418701,
      "learning_rate": 7.236977777777778e-06,
      "loss": 1.6229,
      "step": 31100
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 6.827740669250488,
      "learning_rate": 7.228088888888889e-06,
      "loss": 1.5835,
      "step": 31200
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 7.558791160583496,
      "learning_rate": 7.2192e-06,
      "loss": 1.578,
      "step": 31300
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 5.217653751373291,
      "learning_rate": 7.210311111111112e-06,
      "loss": 1.5725,
      "step": 31400
    },
    {
      "epoch": 1.4,
      "grad_norm": 6.986949920654297,
      "learning_rate": 7.201422222222223e-06,
      "loss": 1.54,
      "step": 31500
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 5.91647481918335,
      "learning_rate": 7.1925333333333336e-06,
      "loss": 1.5485,
      "step": 31600
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 5.266974449157715,
      "learning_rate": 7.183644444444445e-06,
      "loss": 1.5616,
      "step": 31700
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 7.154858589172363,
      "learning_rate": 7.1747555555555555e-06,
      "loss": 1.6096,
      "step": 31800
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 6.170751094818115,
      "learning_rate": 7.165866666666668e-06,
      "loss": 1.5662,
      "step": 31900
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 7.136101245880127,
      "learning_rate": 7.156977777777778e-06,
      "loss": 1.5553,
      "step": 32000
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 5.539132595062256,
      "learning_rate": 7.14808888888889e-06,
      "loss": 1.5281,
      "step": 32100
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 6.943672180175781,
      "learning_rate": 7.1392e-06,
      "loss": 1.5778,
      "step": 32200
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 6.080425262451172,
      "learning_rate": 7.1303111111111116e-06,
      "loss": 1.5545,
      "step": 32300
    },
    {
      "epoch": 1.44,
      "grad_norm": 5.968753814697266,
      "learning_rate": 7.121422222222223e-06,
      "loss": 1.5425,
      "step": 32400
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 6.38734769821167,
      "learning_rate": 7.1125333333333335e-06,
      "loss": 1.55,
      "step": 32500
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 4.33803129196167,
      "learning_rate": 7.103644444444445e-06,
      "loss": 1.5813,
      "step": 32600
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 8.313152313232422,
      "learning_rate": 7.094755555555555e-06,
      "loss": 1.5106,
      "step": 32700
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 6.967814922332764,
      "learning_rate": 7.085955555555556e-06,
      "loss": 1.5777,
      "step": 32800
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 6.239413261413574,
      "learning_rate": 7.077066666666668e-06,
      "loss": 1.5416,
      "step": 32900
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 6.497992992401123,
      "learning_rate": 7.068177777777778e-06,
      "loss": 1.56,
      "step": 33000
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 6.4183149337768555,
      "learning_rate": 7.05928888888889e-06,
      "loss": 1.5323,
      "step": 33100
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 5.3181257247924805,
      "learning_rate": 7.0504e-06,
      "loss": 1.5456,
      "step": 33200
    },
    {
      "epoch": 1.48,
      "grad_norm": 5.739299297332764,
      "learning_rate": 7.041511111111111e-06,
      "loss": 1.5484,
      "step": 33300
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 6.727297306060791,
      "learning_rate": 7.032622222222223e-06,
      "loss": 1.5562,
      "step": 33400
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 6.157293796539307,
      "learning_rate": 7.023733333333334e-06,
      "loss": 1.5787,
      "step": 33500
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 5.88239049911499,
      "learning_rate": 7.014844444444445e-06,
      "loss": 1.6031,
      "step": 33600
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 6.434669494628906,
      "learning_rate": 7.0059555555555555e-06,
      "loss": 1.5574,
      "step": 33700
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 5.680057048797607,
      "learning_rate": 6.997066666666668e-06,
      "loss": 1.5588,
      "step": 33800
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 7.05838680267334,
      "learning_rate": 6.988177777777778e-06,
      "loss": 1.559,
      "step": 33900
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 7.737556457519531,
      "learning_rate": 6.97928888888889e-06,
      "loss": 1.539,
      "step": 34000
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 8.207597732543945,
      "learning_rate": 6.9704e-06,
      "loss": 1.5851,
      "step": 34100
    },
    {
      "epoch": 1.52,
      "grad_norm": 6.0109171867370605,
      "learning_rate": 6.9615111111111124e-06,
      "loss": 1.561,
      "step": 34200
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 7.500855922698975,
      "learning_rate": 6.952622222222223e-06,
      "loss": 1.585,
      "step": 34300
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 7.488315105438232,
      "learning_rate": 6.9437333333333335e-06,
      "loss": 1.5403,
      "step": 34400
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 5.369443416595459,
      "learning_rate": 6.934844444444445e-06,
      "loss": 1.5501,
      "step": 34500
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 5.619010925292969,
      "learning_rate": 6.9259555555555554e-06,
      "loss": 1.5545,
      "step": 34600
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 7.500516414642334,
      "learning_rate": 6.917066666666668e-06,
      "loss": 1.5773,
      "step": 34700
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 7.055943965911865,
      "learning_rate": 6.908177777777778e-06,
      "loss": 1.5823,
      "step": 34800
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 6.805295944213867,
      "learning_rate": 6.89928888888889e-06,
      "loss": 1.525,
      "step": 34900
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 7.62284517288208,
      "learning_rate": 6.8904e-06,
      "loss": 1.5738,
      "step": 35000
    },
    {
      "epoch": 1.56,
      "grad_norm": 8.081843376159668,
      "learning_rate": 6.881511111111112e-06,
      "loss": 1.5565,
      "step": 35100
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 7.289237976074219,
      "learning_rate": 6.872622222222223e-06,
      "loss": 1.5247,
      "step": 35200
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 6.2406325340271,
      "learning_rate": 6.8637333333333334e-06,
      "loss": 1.5626,
      "step": 35300
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 6.458880424499512,
      "learning_rate": 6.854844444444445e-06,
      "loss": 1.5523,
      "step": 35400
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 6.466349124908447,
      "learning_rate": 6.845955555555555e-06,
      "loss": 1.5772,
      "step": 35500
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 6.788664817810059,
      "learning_rate": 6.837066666666668e-06,
      "loss": 1.5376,
      "step": 35600
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 6.737133979797363,
      "learning_rate": 6.828177777777778e-06,
      "loss": 1.539,
      "step": 35700
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 5.336559295654297,
      "learning_rate": 6.8192888888888895e-06,
      "loss": 1.5327,
      "step": 35800
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 5.321359634399414,
      "learning_rate": 6.8104e-06,
      "loss": 1.5263,
      "step": 35900
    },
    {
      "epoch": 1.6,
      "grad_norm": 6.8283610343933105,
      "learning_rate": 6.801511111111112e-06,
      "loss": 1.5506,
      "step": 36000
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 7.578571796417236,
      "learning_rate": 6.792622222222223e-06,
      "loss": 1.5363,
      "step": 36100
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 5.720787048339844,
      "learning_rate": 6.783733333333334e-06,
      "loss": 1.5503,
      "step": 36200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 6.102880954742432,
      "learning_rate": 6.774844444444445e-06,
      "loss": 1.5626,
      "step": 36300
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 5.9833903312683105,
      "learning_rate": 6.765955555555555e-06,
      "loss": 1.5579,
      "step": 36400
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 7.473219394683838,
      "learning_rate": 6.7570666666666675e-06,
      "loss": 1.5603,
      "step": 36500
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 6.983191013336182,
      "learning_rate": 6.748177777777778e-06,
      "loss": 1.5786,
      "step": 36600
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 5.613826751708984,
      "learning_rate": 6.7392888888888894e-06,
      "loss": 1.5514,
      "step": 36700
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 6.485313415527344,
      "learning_rate": 6.73048888888889e-06,
      "loss": 1.5458,
      "step": 36800
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 5.577081203460693,
      "learning_rate": 6.7216e-06,
      "loss": 1.5597,
      "step": 36900
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 5.367799282073975,
      "learning_rate": 6.712711111111112e-06,
      "loss": 1.5527,
      "step": 37000
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 7.0143961906433105,
      "learning_rate": 6.703822222222223e-06,
      "loss": 1.5533,
      "step": 37100
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 6.12742805480957,
      "learning_rate": 6.694933333333334e-06,
      "loss": 1.5471,
      "step": 37200
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 5.090803623199463,
      "learning_rate": 6.686044444444445e-06,
      "loss": 1.5363,
      "step": 37300
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 6.5833210945129395,
      "learning_rate": 6.677155555555555e-06,
      "loss": 1.5372,
      "step": 37400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 7.243375778198242,
      "learning_rate": 6.668266666666668e-06,
      "loss": 1.5782,
      "step": 37500
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 6.436271667480469,
      "learning_rate": 6.659377777777778e-06,
      "loss": 1.5683,
      "step": 37600
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 7.731805324554443,
      "learning_rate": 6.6504888888888895e-06,
      "loss": 1.5602,
      "step": 37700
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 6.2165069580078125,
      "learning_rate": 6.6416e-06,
      "loss": 1.567,
      "step": 37800
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 6.394110202789307,
      "learning_rate": 6.632711111111112e-06,
      "loss": 1.5725,
      "step": 37900
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 7.059322834014893,
      "learning_rate": 6.623822222222223e-06,
      "loss": 1.541,
      "step": 38000
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 6.320363521575928,
      "learning_rate": 6.614933333333334e-06,
      "loss": 1.5466,
      "step": 38100
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 6.114841461181641,
      "learning_rate": 6.606044444444445e-06,
      "loss": 1.5489,
      "step": 38200
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 4.770445346832275,
      "learning_rate": 6.597155555555557e-06,
      "loss": 1.5424,
      "step": 38300
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 6.0331573486328125,
      "learning_rate": 6.5882666666666675e-06,
      "loss": 1.53,
      "step": 38400
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 9.305363655090332,
      "learning_rate": 6.579377777777778e-06,
      "loss": 1.5647,
      "step": 38500
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 6.168186664581299,
      "learning_rate": 6.5704888888888895e-06,
      "loss": 1.5403,
      "step": 38600
    },
    {
      "epoch": 1.72,
      "grad_norm": 6.841040134429932,
      "learning_rate": 6.5616e-06,
      "loss": 1.5314,
      "step": 38700
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 7.697948932647705,
      "learning_rate": 6.5528e-06,
      "loss": 1.534,
      "step": 38800
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 6.8662109375,
      "learning_rate": 6.543911111111112e-06,
      "loss": 1.5652,
      "step": 38900
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 5.625552177429199,
      "learning_rate": 6.535022222222223e-06,
      "loss": 1.5057,
      "step": 39000
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 6.5353684425354,
      "learning_rate": 6.526222222222222e-06,
      "loss": 1.5351,
      "step": 39100
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 6.047280788421631,
      "learning_rate": 6.5173333333333345e-06,
      "loss": 1.5665,
      "step": 39200
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 6.906128883361816,
      "learning_rate": 6.508444444444445e-06,
      "loss": 1.5289,
      "step": 39300
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 6.388877868652344,
      "learning_rate": 6.499555555555556e-06,
      "loss": 1.5863,
      "step": 39400
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 5.474090099334717,
      "learning_rate": 6.490666666666667e-06,
      "loss": 1.5643,
      "step": 39500
    },
    {
      "epoch": 1.76,
      "grad_norm": 7.389440536499023,
      "learning_rate": 6.481777777777778e-06,
      "loss": 1.5356,
      "step": 39600
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 5.59758186340332,
      "learning_rate": 6.47288888888889e-06,
      "loss": 1.5477,
      "step": 39700
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 6.840673923492432,
      "learning_rate": 6.464e-06,
      "loss": 1.5459,
      "step": 39800
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 5.078192710876465,
      "learning_rate": 6.455111111111112e-06,
      "loss": 1.5159,
      "step": 39900
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 5.31404447555542,
      "learning_rate": 6.446222222222222e-06,
      "loss": 1.4991,
      "step": 40000
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 5.993261814117432,
      "learning_rate": 6.4373333333333344e-06,
      "loss": 1.5362,
      "step": 40100
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 7.2932209968566895,
      "learning_rate": 6.428444444444445e-06,
      "loss": 1.5676,
      "step": 40200
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 5.164085865020752,
      "learning_rate": 6.4195555555555555e-06,
      "loss": 1.531,
      "step": 40300
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 5.127695083618164,
      "learning_rate": 6.410666666666667e-06,
      "loss": 1.5738,
      "step": 40400
    },
    {
      "epoch": 1.8,
      "grad_norm": 5.7883381843566895,
      "learning_rate": 6.401777777777778e-06,
      "loss": 1.5634,
      "step": 40500
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 5.36130428314209,
      "learning_rate": 6.39288888888889e-06,
      "loss": 1.5391,
      "step": 40600
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 5.166922569274902,
      "learning_rate": 6.384e-06,
      "loss": 1.5582,
      "step": 40700
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 6.645636558532715,
      "learning_rate": 6.375111111111112e-06,
      "loss": 1.5899,
      "step": 40800
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 9.616236686706543,
      "learning_rate": 6.366222222222222e-06,
      "loss": 1.5534,
      "step": 40900
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 5.828885555267334,
      "learning_rate": 6.357333333333334e-06,
      "loss": 1.4988,
      "step": 41000
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 6.240208148956299,
      "learning_rate": 6.348444444444445e-06,
      "loss": 1.5247,
      "step": 41100
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 6.392978668212891,
      "learning_rate": 6.339555555555556e-06,
      "loss": 1.5112,
      "step": 41200
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 6.465495586395264,
      "learning_rate": 6.330666666666667e-06,
      "loss": 1.5321,
      "step": 41300
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 5.3829545974731445,
      "learning_rate": 6.321777777777778e-06,
      "loss": 1.5183,
      "step": 41400
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 5.177002429962158,
      "learning_rate": 6.31288888888889e-06,
      "loss": 1.5791,
      "step": 41500
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 7.172863006591797,
      "learning_rate": 6.304e-06,
      "loss": 1.5241,
      "step": 41600
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 6.301916122436523,
      "learning_rate": 6.2951111111111115e-06,
      "loss": 1.5401,
      "step": 41700
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 5.780455112457275,
      "learning_rate": 6.286222222222222e-06,
      "loss": 1.5521,
      "step": 41800
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 6.262840747833252,
      "learning_rate": 6.277333333333334e-06,
      "loss": 1.5197,
      "step": 41900
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 6.059311866760254,
      "learning_rate": 6.268444444444445e-06,
      "loss": 1.5505,
      "step": 42000
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 10.029783248901367,
      "learning_rate": 6.259555555555556e-06,
      "loss": 1.5614,
      "step": 42100
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 6.964897632598877,
      "learning_rate": 6.250666666666667e-06,
      "loss": 1.5308,
      "step": 42200
    },
    {
      "epoch": 1.88,
      "grad_norm": 6.976278305053711,
      "learning_rate": 6.241777777777779e-06,
      "loss": 1.57,
      "step": 42300
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 6.4942426681518555,
      "learning_rate": 6.2328888888888895e-06,
      "loss": 1.5103,
      "step": 42400
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 4.971755504608154,
      "learning_rate": 6.224e-06,
      "loss": 1.526,
      "step": 42500
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 6.475659370422363,
      "learning_rate": 6.2151111111111114e-06,
      "loss": 1.5287,
      "step": 42600
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 5.581170558929443,
      "learning_rate": 6.206222222222222e-06,
      "loss": 1.5409,
      "step": 42700
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 6.325690269470215,
      "learning_rate": 6.197333333333334e-06,
      "loss": 1.5251,
      "step": 42800
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 6.605616092681885,
      "learning_rate": 6.188444444444445e-06,
      "loss": 1.5531,
      "step": 42900
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 5.894525527954102,
      "learning_rate": 6.179555555555556e-06,
      "loss": 1.5087,
      "step": 43000
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 4.477473735809326,
      "learning_rate": 6.170755555555556e-06,
      "loss": 1.5605,
      "step": 43100
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.9798970222473145,
      "learning_rate": 6.161866666666667e-06,
      "loss": 1.5423,
      "step": 43200
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 6.738548278808594,
      "learning_rate": 6.152977777777779e-06,
      "loss": 1.5377,
      "step": 43300
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 7.108492851257324,
      "learning_rate": 6.14408888888889e-06,
      "loss": 1.5294,
      "step": 43400
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 5.5898542404174805,
      "learning_rate": 6.1352e-06,
      "loss": 1.5418,
      "step": 43500
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 6.892210006713867,
      "learning_rate": 6.1263111111111115e-06,
      "loss": 1.5341,
      "step": 43600
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 7.572237968444824,
      "learning_rate": 6.117422222222222e-06,
      "loss": 1.5425,
      "step": 43700
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 6.740200042724609,
      "learning_rate": 6.108533333333334e-06,
      "loss": 1.5406,
      "step": 43800
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 6.465364456176758,
      "learning_rate": 6.099644444444445e-06,
      "loss": 1.5526,
      "step": 43900
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 5.755325794219971,
      "learning_rate": 6.090755555555556e-06,
      "loss": 1.5347,
      "step": 44000
    },
    {
      "epoch": 1.96,
      "grad_norm": 6.352814197540283,
      "learning_rate": 6.081866666666667e-06,
      "loss": 1.5057,
      "step": 44100
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 4.7976837158203125,
      "learning_rate": 6.072977777777779e-06,
      "loss": 1.52,
      "step": 44200
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 6.730218410491943,
      "learning_rate": 6.0640888888888895e-06,
      "loss": 1.5341,
      "step": 44300
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 4.92548131942749,
      "learning_rate": 6.0552e-06,
      "loss": 1.5395,
      "step": 44400
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 6.754398345947266,
      "learning_rate": 6.0463111111111114e-06,
      "loss": 1.5253,
      "step": 44500
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 6.0117950439453125,
      "learning_rate": 6.037422222222222e-06,
      "loss": 1.5546,
      "step": 44600
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 5.459531784057617,
      "learning_rate": 6.028533333333334e-06,
      "loss": 1.53,
      "step": 44700
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 5.08430814743042,
      "learning_rate": 6.019644444444445e-06,
      "loss": 1.5319,
      "step": 44800
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 5.2377142906188965,
      "learning_rate": 6.010755555555556e-06,
      "loss": 1.5121,
      "step": 44900
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.757847309112549,
      "learning_rate": 6.001866666666667e-06,
      "loss": 1.5352,
      "step": 45000
    },
    {
      "epoch": 2.0044444444444443,
      "grad_norm": 4.761312961578369,
      "learning_rate": 5.993066666666667e-06,
      "loss": 1.5343,
      "step": 45100
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 5.3049750328063965,
      "learning_rate": 5.984177777777779e-06,
      "loss": 1.5171,
      "step": 45200
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 5.489744186401367,
      "learning_rate": 5.97528888888889e-06,
      "loss": 1.5492,
      "step": 45300
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 5.991397857666016,
      "learning_rate": 5.9664e-06,
      "loss": 1.5266,
      "step": 45400
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 5.723753929138184,
      "learning_rate": 5.9575111111111116e-06,
      "loss": 1.5515,
      "step": 45500
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 6.430820941925049,
      "learning_rate": 5.948622222222222e-06,
      "loss": 1.5343,
      "step": 45600
    },
    {
      "epoch": 2.031111111111111,
      "grad_norm": 6.468318462371826,
      "learning_rate": 5.939733333333334e-06,
      "loss": 1.5243,
      "step": 45700
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 6.821277618408203,
      "learning_rate": 5.930844444444445e-06,
      "loss": 1.5342,
      "step": 45800
    },
    {
      "epoch": 2.04,
      "grad_norm": 4.814772605895996,
      "learning_rate": 5.921955555555556e-06,
      "loss": 1.5762,
      "step": 45900
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 5.6122660636901855,
      "learning_rate": 5.913066666666667e-06,
      "loss": 1.5057,
      "step": 46000
    },
    {
      "epoch": 2.048888888888889,
      "grad_norm": 8.780856132507324,
      "learning_rate": 5.904177777777779e-06,
      "loss": 1.5297,
      "step": 46100
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 6.174585342407227,
      "learning_rate": 5.8952888888888896e-06,
      "loss": 1.5138,
      "step": 46200
    },
    {
      "epoch": 2.057777777777778,
      "grad_norm": 5.936131954193115,
      "learning_rate": 5.886400000000001e-06,
      "loss": 1.4987,
      "step": 46300
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 7.0469279289245605,
      "learning_rate": 5.8775111111111115e-06,
      "loss": 1.5033,
      "step": 46400
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 8.529433250427246,
      "learning_rate": 5.868622222222222e-06,
      "loss": 1.5009,
      "step": 46500
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 5.265646934509277,
      "learning_rate": 5.859733333333334e-06,
      "loss": 1.5229,
      "step": 46600
    },
    {
      "epoch": 2.0755555555555554,
      "grad_norm": 7.464733123779297,
      "learning_rate": 5.850844444444445e-06,
      "loss": 1.4916,
      "step": 46700
    },
    {
      "epoch": 2.08,
      "grad_norm": 6.116762638092041,
      "learning_rate": 5.841955555555556e-06,
      "loss": 1.5157,
      "step": 46800
    },
    {
      "epoch": 2.0844444444444443,
      "grad_norm": 6.834920883178711,
      "learning_rate": 5.833066666666667e-06,
      "loss": 1.5438,
      "step": 46900
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 6.78181266784668,
      "learning_rate": 5.824177777777779e-06,
      "loss": 1.5595,
      "step": 47000
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 5.6565351486206055,
      "learning_rate": 5.815377777777778e-06,
      "loss": 1.514,
      "step": 47100
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 5.9250383377075195,
      "learning_rate": 5.80648888888889e-06,
      "loss": 1.5115,
      "step": 47200
    },
    {
      "epoch": 2.102222222222222,
      "grad_norm": 5.671351432800293,
      "learning_rate": 5.797600000000001e-06,
      "loss": 1.5066,
      "step": 47300
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 5.357291221618652,
      "learning_rate": 5.788711111111112e-06,
      "loss": 1.5416,
      "step": 47400
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 6.848896026611328,
      "learning_rate": 5.779822222222222e-06,
      "loss": 1.5185,
      "step": 47500
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 6.10288143157959,
      "learning_rate": 5.770933333333334e-06,
      "loss": 1.4997,
      "step": 47600
    },
    {
      "epoch": 2.12,
      "grad_norm": 7.043394088745117,
      "learning_rate": 5.762044444444445e-06,
      "loss": 1.5188,
      "step": 47700
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 6.95942497253418,
      "learning_rate": 5.753155555555556e-06,
      "loss": 1.4818,
      "step": 47800
    },
    {
      "epoch": 2.128888888888889,
      "grad_norm": 5.96002197265625,
      "learning_rate": 5.744266666666667e-06,
      "loss": 1.5059,
      "step": 47900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 6.679205417633057,
      "learning_rate": 5.735377777777778e-06,
      "loss": 1.5021,
      "step": 48000
    },
    {
      "epoch": 2.137777777777778,
      "grad_norm": 4.740731716156006,
      "learning_rate": 5.7264888888888896e-06,
      "loss": 1.4924,
      "step": 48100
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 6.495748996734619,
      "learning_rate": 5.717600000000001e-06,
      "loss": 1.4968,
      "step": 48200
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 5.082347393035889,
      "learning_rate": 5.7087111111111115e-06,
      "loss": 1.5304,
      "step": 48300
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 5.523674488067627,
      "learning_rate": 5.699822222222222e-06,
      "loss": 1.4815,
      "step": 48400
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 5.963528156280518,
      "learning_rate": 5.690933333333334e-06,
      "loss": 1.5133,
      "step": 48500
    },
    {
      "epoch": 2.16,
      "grad_norm": 7.243768215179443,
      "learning_rate": 5.682044444444445e-06,
      "loss": 1.5316,
      "step": 48600
    },
    {
      "epoch": 2.1644444444444444,
      "grad_norm": 5.850263595581055,
      "learning_rate": 5.673155555555556e-06,
      "loss": 1.5137,
      "step": 48700
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 5.14383602142334,
      "learning_rate": 5.664266666666667e-06,
      "loss": 1.5332,
      "step": 48800
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 7.416391849517822,
      "learning_rate": 5.655377777777778e-06,
      "loss": 1.5417,
      "step": 48900
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 5.645524501800537,
      "learning_rate": 5.6464888888888895e-06,
      "loss": 1.5467,
      "step": 49000
    },
    {
      "epoch": 2.1822222222222223,
      "grad_norm": 4.871496677398682,
      "learning_rate": 5.637688888888889e-06,
      "loss": 1.514,
      "step": 49100
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 5.456282138824463,
      "learning_rate": 5.628800000000001e-06,
      "loss": 1.5427,
      "step": 49200
    },
    {
      "epoch": 2.1911111111111112,
      "grad_norm": 4.673121452331543,
      "learning_rate": 5.619911111111112e-06,
      "loss": 1.5123,
      "step": 49300
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 5.322500705718994,
      "learning_rate": 5.611022222222222e-06,
      "loss": 1.4587,
      "step": 49400
    },
    {
      "epoch": 2.2,
      "grad_norm": 5.479060649871826,
      "learning_rate": 5.6021333333333335e-06,
      "loss": 1.528,
      "step": 49500
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 6.097711563110352,
      "learning_rate": 5.593244444444444e-06,
      "loss": 1.5057,
      "step": 49600
    },
    {
      "epoch": 2.2088888888888887,
      "grad_norm": 5.178313732147217,
      "learning_rate": 5.584355555555556e-06,
      "loss": 1.5048,
      "step": 49700
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 8.11757755279541,
      "learning_rate": 5.575466666666667e-06,
      "loss": 1.5508,
      "step": 49800
    },
    {
      "epoch": 2.2177777777777776,
      "grad_norm": 5.741850852966309,
      "learning_rate": 5.566577777777778e-06,
      "loss": 1.5295,
      "step": 49900
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 6.379236221313477,
      "learning_rate": 5.557688888888889e-06,
      "loss": 1.5598,
      "step": 50000
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 6.768738269805908,
      "learning_rate": 5.548800000000001e-06,
      "loss": 1.4851,
      "step": 50100
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 6.8568115234375,
      "learning_rate": 5.5399111111111115e-06,
      "loss": 1.5191,
      "step": 50200
    },
    {
      "epoch": 2.2355555555555555,
      "grad_norm": 5.235949516296387,
      "learning_rate": 5.531022222222223e-06,
      "loss": 1.5595,
      "step": 50300
    },
    {
      "epoch": 2.24,
      "grad_norm": 4.760732173919678,
      "learning_rate": 5.5221333333333334e-06,
      "loss": 1.5137,
      "step": 50400
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 6.6893229484558105,
      "learning_rate": 5.513244444444444e-06,
      "loss": 1.5049,
      "step": 50500
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 6.023736953735352,
      "learning_rate": 5.504355555555556e-06,
      "loss": 1.5121,
      "step": 50600
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 6.089096546173096,
      "learning_rate": 5.495466666666667e-06,
      "loss": 1.5034,
      "step": 50700
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 6.3143720626831055,
      "learning_rate": 5.486577777777778e-06,
      "loss": 1.5438,
      "step": 50800
    },
    {
      "epoch": 2.2622222222222224,
      "grad_norm": 6.149752616882324,
      "learning_rate": 5.477688888888889e-06,
      "loss": 1.4925,
      "step": 50900
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 6.142329216003418,
      "learning_rate": 5.468800000000001e-06,
      "loss": 1.5409,
      "step": 51000
    },
    {
      "epoch": 2.2711111111111113,
      "grad_norm": 5.844442844390869,
      "learning_rate": 5.4599111111111114e-06,
      "loss": 1.4973,
      "step": 51100
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 7.2972822189331055,
      "learning_rate": 5.451111111111112e-06,
      "loss": 1.4943,
      "step": 51200
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 5.636104106903076,
      "learning_rate": 5.442222222222223e-06,
      "loss": 1.4793,
      "step": 51300
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 5.185155868530273,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 1.5239,
      "step": 51400
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 7.224742889404297,
      "learning_rate": 5.424444444444444e-06,
      "loss": 1.5058,
      "step": 51500
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 5.577940464019775,
      "learning_rate": 5.415555555555556e-06,
      "loss": 1.4948,
      "step": 51600
    },
    {
      "epoch": 2.2977777777777777,
      "grad_norm": 4.76424503326416,
      "learning_rate": 5.406666666666667e-06,
      "loss": 1.5072,
      "step": 51700
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 6.593615531921387,
      "learning_rate": 5.397777777777778e-06,
      "loss": 1.4853,
      "step": 51800
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 5.591152667999268,
      "learning_rate": 5.388888888888889e-06,
      "loss": 1.5063,
      "step": 51900
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 6.849484443664551,
      "learning_rate": 5.380000000000001e-06,
      "loss": 1.4896,
      "step": 52000
    },
    {
      "epoch": 2.3155555555555556,
      "grad_norm": 8.302868843078613,
      "learning_rate": 5.3711111111111115e-06,
      "loss": 1.5006,
      "step": 52100
    },
    {
      "epoch": 2.32,
      "grad_norm": 6.053877830505371,
      "learning_rate": 5.362222222222223e-06,
      "loss": 1.4979,
      "step": 52200
    },
    {
      "epoch": 2.3244444444444445,
      "grad_norm": 6.882979869842529,
      "learning_rate": 5.3533333333333335e-06,
      "loss": 1.4983,
      "step": 52300
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 5.813206195831299,
      "learning_rate": 5.344444444444446e-06,
      "loss": 1.5494,
      "step": 52400
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 5.880963325500488,
      "learning_rate": 5.335555555555556e-06,
      "loss": 1.5221,
      "step": 52500
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 5.244113445281982,
      "learning_rate": 5.326666666666667e-06,
      "loss": 1.4977,
      "step": 52600
    },
    {
      "epoch": 2.3422222222222224,
      "grad_norm": 5.471473693847656,
      "learning_rate": 5.317777777777778e-06,
      "loss": 1.4919,
      "step": 52700
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 5.4229021072387695,
      "learning_rate": 5.308888888888889e-06,
      "loss": 1.5121,
      "step": 52800
    },
    {
      "epoch": 2.351111111111111,
      "grad_norm": 6.466554164886475,
      "learning_rate": 5.300000000000001e-06,
      "loss": 1.5621,
      "step": 52900
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 5.774801254272461,
      "learning_rate": 5.2911111111111115e-06,
      "loss": 1.4929,
      "step": 53000
    },
    {
      "epoch": 2.36,
      "grad_norm": 5.282663822174072,
      "learning_rate": 5.282222222222223e-06,
      "loss": 1.514,
      "step": 53100
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 7.818274021148682,
      "learning_rate": 5.273422222222223e-06,
      "loss": 1.5295,
      "step": 53200
    },
    {
      "epoch": 2.368888888888889,
      "grad_norm": 5.880062580108643,
      "learning_rate": 5.2645333333333336e-06,
      "loss": 1.49,
      "step": 53300
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 6.674001216888428,
      "learning_rate": 5.255644444444444e-06,
      "loss": 1.4812,
      "step": 53400
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 8.164111137390137,
      "learning_rate": 5.246755555555556e-06,
      "loss": 1.5146,
      "step": 53500
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 7.0122270584106445,
      "learning_rate": 5.237866666666667e-06,
      "loss": 1.5362,
      "step": 53600
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 5.460092544555664,
      "learning_rate": 5.228977777777778e-06,
      "loss": 1.5023,
      "step": 53700
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 5.112514019012451,
      "learning_rate": 5.220088888888889e-06,
      "loss": 1.4976,
      "step": 53800
    },
    {
      "epoch": 2.3955555555555557,
      "grad_norm": 5.488147258758545,
      "learning_rate": 5.211200000000001e-06,
      "loss": 1.4961,
      "step": 53900
    },
    {
      "epoch": 2.4,
      "grad_norm": 5.504265785217285,
      "learning_rate": 5.2023111111111116e-06,
      "loss": 1.4827,
      "step": 54000
    },
    {
      "epoch": 2.4044444444444446,
      "grad_norm": 6.506826400756836,
      "learning_rate": 5.193422222222223e-06,
      "loss": 1.4632,
      "step": 54100
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 5.887313365936279,
      "learning_rate": 5.1845333333333335e-06,
      "loss": 1.514,
      "step": 54200
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 5.640110492706299,
      "learning_rate": 5.175644444444446e-06,
      "loss": 1.5146,
      "step": 54300
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 6.710358619689941,
      "learning_rate": 5.166755555555556e-06,
      "loss": 1.5027,
      "step": 54400
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 6.882486820220947,
      "learning_rate": 5.157866666666667e-06,
      "loss": 1.4981,
      "step": 54500
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 6.703976154327393,
      "learning_rate": 5.148977777777778e-06,
      "loss": 1.5212,
      "step": 54600
    },
    {
      "epoch": 2.431111111111111,
      "grad_norm": 5.121593952178955,
      "learning_rate": 5.140088888888889e-06,
      "loss": 1.5314,
      "step": 54700
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 5.737277507781982,
      "learning_rate": 5.131200000000001e-06,
      "loss": 1.5265,
      "step": 54800
    },
    {
      "epoch": 2.44,
      "grad_norm": 5.255755424499512,
      "learning_rate": 5.1223111111111115e-06,
      "loss": 1.5073,
      "step": 54900
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 5.641067981719971,
      "learning_rate": 5.113422222222223e-06,
      "loss": 1.4575,
      "step": 55000
    },
    {
      "epoch": 2.448888888888889,
      "grad_norm": 6.184950351715088,
      "learning_rate": 5.104533333333333e-06,
      "loss": 1.5082,
      "step": 55100
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 5.064931392669678,
      "learning_rate": 5.095733333333334e-06,
      "loss": 1.5263,
      "step": 55200
    },
    {
      "epoch": 2.457777777777778,
      "grad_norm": 6.856351375579834,
      "learning_rate": 5.086844444444445e-06,
      "loss": 1.4907,
      "step": 55300
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 6.560573101043701,
      "learning_rate": 5.077955555555556e-06,
      "loss": 1.4792,
      "step": 55400
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 5.817989826202393,
      "learning_rate": 5.069066666666667e-06,
      "loss": 1.5239,
      "step": 55500
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 6.017120361328125,
      "learning_rate": 5.060177777777778e-06,
      "loss": 1.4841,
      "step": 55600
    },
    {
      "epoch": 2.4755555555555557,
      "grad_norm": 5.952930450439453,
      "learning_rate": 5.051288888888889e-06,
      "loss": 1.4953,
      "step": 55700
    },
    {
      "epoch": 2.48,
      "grad_norm": 7.287938117980957,
      "learning_rate": 5.0424e-06,
      "loss": 1.5126,
      "step": 55800
    },
    {
      "epoch": 2.4844444444444447,
      "grad_norm": 6.166382312774658,
      "learning_rate": 5.033511111111112e-06,
      "loss": 1.5384,
      "step": 55900
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 5.449310302734375,
      "learning_rate": 5.024622222222223e-06,
      "loss": 1.5216,
      "step": 56000
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 6.579117298126221,
      "learning_rate": 5.0157333333333335e-06,
      "loss": 1.4836,
      "step": 56100
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 6.376302242279053,
      "learning_rate": 5.006844444444445e-06,
      "loss": 1.5391,
      "step": 56200
    },
    {
      "epoch": 2.502222222222222,
      "grad_norm": 5.201292514801025,
      "learning_rate": 4.997955555555556e-06,
      "loss": 1.523,
      "step": 56300
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 4.974180698394775,
      "learning_rate": 4.989066666666667e-06,
      "loss": 1.4858,
      "step": 56400
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 4.642875671386719,
      "learning_rate": 4.980177777777778e-06,
      "loss": 1.4892,
      "step": 56500
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 5.531876564025879,
      "learning_rate": 4.97128888888889e-06,
      "loss": 1.4777,
      "step": 56600
    },
    {
      "epoch": 2.52,
      "grad_norm": 6.389926433563232,
      "learning_rate": 4.962400000000001e-06,
      "loss": 1.4957,
      "step": 56700
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 5.7910075187683105,
      "learning_rate": 4.9535111111111115e-06,
      "loss": 1.5091,
      "step": 56800
    },
    {
      "epoch": 2.528888888888889,
      "grad_norm": 5.244346618652344,
      "learning_rate": 4.944622222222223e-06,
      "loss": 1.4848,
      "step": 56900
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 5.812253952026367,
      "learning_rate": 4.9357333333333334e-06,
      "loss": 1.5177,
      "step": 57000
    },
    {
      "epoch": 2.537777777777778,
      "grad_norm": 6.983330249786377,
      "learning_rate": 4.926844444444445e-06,
      "loss": 1.4998,
      "step": 57100
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 6.082135200500488,
      "learning_rate": 4.918044444444445e-06,
      "loss": 1.4737,
      "step": 57200
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 6.433398246765137,
      "learning_rate": 4.9091555555555555e-06,
      "loss": 1.5118,
      "step": 57300
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 5.940297603607178,
      "learning_rate": 4.900266666666667e-06,
      "loss": 1.4834,
      "step": 57400
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 5.582930088043213,
      "learning_rate": 4.891377777777778e-06,
      "loss": 1.4881,
      "step": 57500
    },
    {
      "epoch": 2.56,
      "grad_norm": 5.967439651489258,
      "learning_rate": 4.88248888888889e-06,
      "loss": 1.4951,
      "step": 57600
    },
    {
      "epoch": 2.5644444444444443,
      "grad_norm": 5.973031044006348,
      "learning_rate": 4.8736e-06,
      "loss": 1.5217,
      "step": 57700
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 6.034971714019775,
      "learning_rate": 4.864711111111112e-06,
      "loss": 1.4981,
      "step": 57800
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 6.0276384353637695,
      "learning_rate": 4.855822222222223e-06,
      "loss": 1.4722,
      "step": 57900
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 5.920516014099121,
      "learning_rate": 4.8469333333333335e-06,
      "loss": 1.496,
      "step": 58000
    },
    {
      "epoch": 2.582222222222222,
      "grad_norm": 6.082977771759033,
      "learning_rate": 4.838044444444445e-06,
      "loss": 1.4882,
      "step": 58100
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 5.561607360839844,
      "learning_rate": 4.8291555555555555e-06,
      "loss": 1.49,
      "step": 58200
    },
    {
      "epoch": 2.591111111111111,
      "grad_norm": 6.030642032623291,
      "learning_rate": 4.820266666666667e-06,
      "loss": 1.508,
      "step": 58300
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 5.320519924163818,
      "learning_rate": 4.811377777777778e-06,
      "loss": 1.5001,
      "step": 58400
    },
    {
      "epoch": 2.6,
      "grad_norm": 7.070597171783447,
      "learning_rate": 4.80248888888889e-06,
      "loss": 1.5065,
      "step": 58500
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 5.735476016998291,
      "learning_rate": 4.7936e-06,
      "loss": 1.5292,
      "step": 58600
    },
    {
      "epoch": 2.608888888888889,
      "grad_norm": 6.948894023895264,
      "learning_rate": 4.7847111111111115e-06,
      "loss": 1.5006,
      "step": 58700
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 6.708220481872559,
      "learning_rate": 4.775822222222223e-06,
      "loss": 1.5029,
      "step": 58800
    },
    {
      "epoch": 2.6177777777777775,
      "grad_norm": 4.850708484649658,
      "learning_rate": 4.766933333333334e-06,
      "loss": 1.5069,
      "step": 58900
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 5.3550872802734375,
      "learning_rate": 4.758044444444445e-06,
      "loss": 1.5058,
      "step": 59000
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 5.763555526733398,
      "learning_rate": 4.749155555555555e-06,
      "loss": 1.4963,
      "step": 59100
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 6.027457237243652,
      "learning_rate": 4.740266666666667e-06,
      "loss": 1.5031,
      "step": 59200
    },
    {
      "epoch": 2.6355555555555554,
      "grad_norm": 8.011354446411133,
      "learning_rate": 4.731466666666667e-06,
      "loss": 1.5186,
      "step": 59300
    },
    {
      "epoch": 2.64,
      "grad_norm": 4.744731903076172,
      "learning_rate": 4.722577777777778e-06,
      "loss": 1.5018,
      "step": 59400
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 6.273828983306885,
      "learning_rate": 4.71368888888889e-06,
      "loss": 1.5114,
      "step": 59500
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 5.922699928283691,
      "learning_rate": 4.7048e-06,
      "loss": 1.5146,
      "step": 59600
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 6.403932094573975,
      "learning_rate": 4.695911111111112e-06,
      "loss": 1.4889,
      "step": 59700
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 5.765309810638428,
      "learning_rate": 4.687022222222223e-06,
      "loss": 1.4811,
      "step": 59800
    },
    {
      "epoch": 2.6622222222222223,
      "grad_norm": 6.039694786071777,
      "learning_rate": 4.6781333333333336e-06,
      "loss": 1.4921,
      "step": 59900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 6.3470683097839355,
      "learning_rate": 4.669244444444445e-06,
      "loss": 1.4926,
      "step": 60000
    },
    {
      "epoch": 2.671111111111111,
      "grad_norm": 6.5463433265686035,
      "learning_rate": 4.6603555555555555e-06,
      "loss": 1.4759,
      "step": 60100
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 6.308573246002197,
      "learning_rate": 4.651466666666667e-06,
      "loss": 1.5457,
      "step": 60200
    },
    {
      "epoch": 2.68,
      "grad_norm": 5.194228649139404,
      "learning_rate": 4.642577777777778e-06,
      "loss": 1.5375,
      "step": 60300
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 5.918785572052002,
      "learning_rate": 4.63368888888889e-06,
      "loss": 1.5117,
      "step": 60400
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 5.234470844268799,
      "learning_rate": 4.6248e-06,
      "loss": 1.5202,
      "step": 60500
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 6.5933709144592285,
      "learning_rate": 4.6159111111111116e-06,
      "loss": 1.5209,
      "step": 60600
    },
    {
      "epoch": 2.6977777777777776,
      "grad_norm": 5.659660339355469,
      "learning_rate": 4.607022222222223e-06,
      "loss": 1.4927,
      "step": 60700
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 6.615653991699219,
      "learning_rate": 4.5981333333333335e-06,
      "loss": 1.4916,
      "step": 60800
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 4.850601673126221,
      "learning_rate": 4.589244444444445e-06,
      "loss": 1.5183,
      "step": 60900
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 7.29971981048584,
      "learning_rate": 4.580355555555555e-06,
      "loss": 1.4776,
      "step": 61000
    },
    {
      "epoch": 2.7155555555555555,
      "grad_norm": 6.2976531982421875,
      "learning_rate": 4.571466666666667e-06,
      "loss": 1.505,
      "step": 61100
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 6.488854885101318,
      "learning_rate": 4.562577777777778e-06,
      "loss": 1.4753,
      "step": 61200
    },
    {
      "epoch": 2.7244444444444444,
      "grad_norm": 5.0250630378723145,
      "learning_rate": 4.553777777777778e-06,
      "loss": 1.5324,
      "step": 61300
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 6.219089031219482,
      "learning_rate": 4.544888888888889e-06,
      "loss": 1.5257,
      "step": 61400
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 5.596930503845215,
      "learning_rate": 4.536e-06,
      "loss": 1.4801,
      "step": 61500
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 7.108383655548096,
      "learning_rate": 4.527111111111112e-06,
      "loss": 1.502,
      "step": 61600
    },
    {
      "epoch": 2.7422222222222223,
      "grad_norm": 6.42667818069458,
      "learning_rate": 4.518222222222223e-06,
      "loss": 1.4781,
      "step": 61700
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 5.03157901763916,
      "learning_rate": 4.509333333333334e-06,
      "loss": 1.498,
      "step": 61800
    },
    {
      "epoch": 2.7511111111111113,
      "grad_norm": 5.326007843017578,
      "learning_rate": 4.500444444444445e-06,
      "loss": 1.4919,
      "step": 61900
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 5.567666053771973,
      "learning_rate": 4.4915555555555555e-06,
      "loss": 1.4846,
      "step": 62000
    },
    {
      "epoch": 2.76,
      "grad_norm": 5.832616806030273,
      "learning_rate": 4.482666666666667e-06,
      "loss": 1.5177,
      "step": 62100
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 5.224260330200195,
      "learning_rate": 4.473777777777778e-06,
      "loss": 1.4839,
      "step": 62200
    },
    {
      "epoch": 2.7688888888888887,
      "grad_norm": 6.000185012817383,
      "learning_rate": 4.464888888888889e-06,
      "loss": 1.5153,
      "step": 62300
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 5.372166156768799,
      "learning_rate": 4.456e-06,
      "loss": 1.467,
      "step": 62400
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 6.32235860824585,
      "learning_rate": 4.447111111111112e-06,
      "loss": 1.5424,
      "step": 62500
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 5.185024738311768,
      "learning_rate": 4.438222222222223e-06,
      "loss": 1.4558,
      "step": 62600
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 5.624604225158691,
      "learning_rate": 4.4293333333333335e-06,
      "loss": 1.4945,
      "step": 62700
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 5.403887748718262,
      "learning_rate": 4.420444444444445e-06,
      "loss": 1.494,
      "step": 62800
    },
    {
      "epoch": 2.7955555555555556,
      "grad_norm": 7.1487321853637695,
      "learning_rate": 4.411555555555556e-06,
      "loss": 1.4864,
      "step": 62900
    },
    {
      "epoch": 2.8,
      "grad_norm": 5.500217437744141,
      "learning_rate": 4.402666666666667e-06,
      "loss": 1.5225,
      "step": 63000
    },
    {
      "epoch": 2.8044444444444445,
      "grad_norm": 6.1774001121521,
      "learning_rate": 4.393777777777778e-06,
      "loss": 1.4667,
      "step": 63100
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 6.519896507263184,
      "learning_rate": 4.384888888888889e-06,
      "loss": 1.4978,
      "step": 63200
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 5.96351432800293,
      "learning_rate": 4.376e-06,
      "loss": 1.4856,
      "step": 63300
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 6.2807536125183105,
      "learning_rate": 4.3672e-06,
      "loss": 1.4966,
      "step": 63400
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 6.320102214813232,
      "learning_rate": 4.358311111111112e-06,
      "loss": 1.5207,
      "step": 63500
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 5.797898292541504,
      "learning_rate": 4.349422222222223e-06,
      "loss": 1.4689,
      "step": 63600
    },
    {
      "epoch": 2.8311111111111114,
      "grad_norm": 6.031149864196777,
      "learning_rate": 4.340533333333334e-06,
      "loss": 1.5319,
      "step": 63700
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 5.318360805511475,
      "learning_rate": 4.331644444444445e-06,
      "loss": 1.5381,
      "step": 63800
    },
    {
      "epoch": 2.84,
      "grad_norm": 6.756066799163818,
      "learning_rate": 4.322755555555556e-06,
      "loss": 1.5183,
      "step": 63900
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 5.406097888946533,
      "learning_rate": 4.313866666666667e-06,
      "loss": 1.4885,
      "step": 64000
    },
    {
      "epoch": 2.848888888888889,
      "grad_norm": 4.3415961265563965,
      "learning_rate": 4.304977777777778e-06,
      "loss": 1.5066,
      "step": 64100
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 6.32993221282959,
      "learning_rate": 4.296088888888889e-06,
      "loss": 1.523,
      "step": 64200
    },
    {
      "epoch": 2.8577777777777778,
      "grad_norm": 5.347588062286377,
      "learning_rate": 4.2872e-06,
      "loss": 1.4821,
      "step": 64300
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 6.655091762542725,
      "learning_rate": 4.278311111111112e-06,
      "loss": 1.489,
      "step": 64400
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 7.709441661834717,
      "learning_rate": 4.269422222222223e-06,
      "loss": 1.4991,
      "step": 64500
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 5.5391845703125,
      "learning_rate": 4.2605333333333335e-06,
      "loss": 1.4956,
      "step": 64600
    },
    {
      "epoch": 2.8755555555555556,
      "grad_norm": 6.048527240753174,
      "learning_rate": 4.251644444444445e-06,
      "loss": 1.4835,
      "step": 64700
    },
    {
      "epoch": 2.88,
      "grad_norm": 6.818532466888428,
      "learning_rate": 4.242755555555556e-06,
      "loss": 1.4911,
      "step": 64800
    },
    {
      "epoch": 2.8844444444444446,
      "grad_norm": 5.508950710296631,
      "learning_rate": 4.233866666666667e-06,
      "loss": 1.4711,
      "step": 64900
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 6.665717601776123,
      "learning_rate": 4.224977777777778e-06,
      "loss": 1.496,
      "step": 65000
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 6.383749961853027,
      "learning_rate": 4.216088888888889e-06,
      "loss": 1.4969,
      "step": 65100
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 5.981866836547852,
      "learning_rate": 4.2072e-06,
      "loss": 1.4885,
      "step": 65200
    },
    {
      "epoch": 2.902222222222222,
      "grad_norm": 5.73339319229126,
      "learning_rate": 4.1983111111111115e-06,
      "loss": 1.4853,
      "step": 65300
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 5.6282639503479,
      "learning_rate": 4.189511111111112e-06,
      "loss": 1.5345,
      "step": 65400
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 5.71094274520874,
      "learning_rate": 4.180622222222222e-06,
      "loss": 1.4722,
      "step": 65500
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 5.275905609130859,
      "learning_rate": 4.171733333333334e-06,
      "loss": 1.5047,
      "step": 65600
    },
    {
      "epoch": 2.92,
      "grad_norm": 6.308121681213379,
      "learning_rate": 4.162844444444445e-06,
      "loss": 1.5198,
      "step": 65700
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 5.949952125549316,
      "learning_rate": 4.153955555555556e-06,
      "loss": 1.488,
      "step": 65800
    },
    {
      "epoch": 2.928888888888889,
      "grad_norm": 5.6444315910339355,
      "learning_rate": 4.145066666666667e-06,
      "loss": 1.5316,
      "step": 65900
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 5.609707355499268,
      "learning_rate": 4.1361777777777775e-06,
      "loss": 1.4958,
      "step": 66000
    },
    {
      "epoch": 2.937777777777778,
      "grad_norm": 5.500119686126709,
      "learning_rate": 4.127288888888889e-06,
      "loss": 1.4934,
      "step": 66100
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 4.816367149353027,
      "learning_rate": 4.1184e-06,
      "loss": 1.4898,
      "step": 66200
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 6.662552356719971,
      "learning_rate": 4.109511111111112e-06,
      "loss": 1.4873,
      "step": 66300
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 5.58004093170166,
      "learning_rate": 4.100622222222222e-06,
      "loss": 1.4525,
      "step": 66400
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 6.6220903396606445,
      "learning_rate": 4.0917333333333336e-06,
      "loss": 1.5193,
      "step": 66500
    },
    {
      "epoch": 2.96,
      "grad_norm": 5.3732733726501465,
      "learning_rate": 4.082844444444445e-06,
      "loss": 1.5135,
      "step": 66600
    },
    {
      "epoch": 2.964444444444444,
      "grad_norm": 5.401058673858643,
      "learning_rate": 4.073955555555556e-06,
      "loss": 1.5078,
      "step": 66700
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 5.136219501495361,
      "learning_rate": 4.065066666666667e-06,
      "loss": 1.4989,
      "step": 66800
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 5.204802513122559,
      "learning_rate": 4.056177777777778e-06,
      "loss": 1.4904,
      "step": 66900
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 5.622727870941162,
      "learning_rate": 4.04728888888889e-06,
      "loss": 1.483,
      "step": 67000
    },
    {
      "epoch": 2.982222222222222,
      "grad_norm": 6.939631938934326,
      "learning_rate": 4.0384e-06,
      "loss": 1.4856,
      "step": 67100
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 4.753209590911865,
      "learning_rate": 4.0295111111111115e-06,
      "loss": 1.4928,
      "step": 67200
    },
    {
      "epoch": 2.991111111111111,
      "grad_norm": 5.267347812652588,
      "learning_rate": 4.020622222222222e-06,
      "loss": 1.481,
      "step": 67300
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 6.722236156463623,
      "learning_rate": 4.0117333333333335e-06,
      "loss": 1.5322,
      "step": 67400
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.010489463806152,
      "learning_rate": 4.002933333333334e-06,
      "loss": 1.4819,
      "step": 67500
    },
    {
      "epoch": 3.0044444444444443,
      "grad_norm": 6.456971168518066,
      "learning_rate": 3.994044444444445e-06,
      "loss": 1.4784,
      "step": 67600
    },
    {
      "epoch": 3.008888888888889,
      "grad_norm": 6.23629093170166,
      "learning_rate": 3.985155555555556e-06,
      "loss": 1.4689,
      "step": 67700
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 5.751826286315918,
      "learning_rate": 3.976266666666667e-06,
      "loss": 1.4776,
      "step": 67800
    },
    {
      "epoch": 3.017777777777778,
      "grad_norm": 5.6091461181640625,
      "learning_rate": 3.967377777777778e-06,
      "loss": 1.4847,
      "step": 67900
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 5.450532913208008,
      "learning_rate": 3.958488888888889e-06,
      "loss": 1.464,
      "step": 68000
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 6.161269187927246,
      "learning_rate": 3.9496e-06,
      "loss": 1.4576,
      "step": 68100
    },
    {
      "epoch": 3.031111111111111,
      "grad_norm": 6.9100117683410645,
      "learning_rate": 3.940711111111112e-06,
      "loss": 1.4632,
      "step": 68200
    },
    {
      "epoch": 3.0355555555555553,
      "grad_norm": 6.692798137664795,
      "learning_rate": 3.931822222222222e-06,
      "loss": 1.5121,
      "step": 68300
    },
    {
      "epoch": 3.04,
      "grad_norm": 6.434842109680176,
      "learning_rate": 3.9229333333333336e-06,
      "loss": 1.4671,
      "step": 68400
    },
    {
      "epoch": 3.0444444444444443,
      "grad_norm": 6.490229606628418,
      "learning_rate": 3.914044444444445e-06,
      "loss": 1.4806,
      "step": 68500
    },
    {
      "epoch": 3.048888888888889,
      "grad_norm": 5.973875045776367,
      "learning_rate": 3.905155555555556e-06,
      "loss": 1.4609,
      "step": 68600
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 6.311092853546143,
      "learning_rate": 3.896266666666667e-06,
      "loss": 1.4987,
      "step": 68700
    },
    {
      "epoch": 3.057777777777778,
      "grad_norm": 5.088950157165527,
      "learning_rate": 3.887377777777778e-06,
      "loss": 1.5074,
      "step": 68800
    },
    {
      "epoch": 3.062222222222222,
      "grad_norm": 6.233273506164551,
      "learning_rate": 3.87848888888889e-06,
      "loss": 1.4813,
      "step": 68900
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 6.843947410583496,
      "learning_rate": 3.8696e-06,
      "loss": 1.453,
      "step": 69000
    },
    {
      "epoch": 3.071111111111111,
      "grad_norm": 5.806759357452393,
      "learning_rate": 3.8607111111111116e-06,
      "loss": 1.4901,
      "step": 69100
    },
    {
      "epoch": 3.0755555555555554,
      "grad_norm": 5.605520248413086,
      "learning_rate": 3.851822222222222e-06,
      "loss": 1.487,
      "step": 69200
    },
    {
      "epoch": 3.08,
      "grad_norm": 7.700567722320557,
      "learning_rate": 3.8429333333333335e-06,
      "loss": 1.4838,
      "step": 69300
    },
    {
      "epoch": 3.0844444444444443,
      "grad_norm": 5.428143501281738,
      "learning_rate": 3.834044444444445e-06,
      "loss": 1.497,
      "step": 69400
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 5.726452350616455,
      "learning_rate": 3.825244444444445e-06,
      "loss": 1.5084,
      "step": 69500
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 5.508836269378662,
      "learning_rate": 3.816355555555556e-06,
      "loss": 1.4987,
      "step": 69600
    },
    {
      "epoch": 3.097777777777778,
      "grad_norm": 6.680547714233398,
      "learning_rate": 3.807466666666667e-06,
      "loss": 1.488,
      "step": 69700
    },
    {
      "epoch": 3.102222222222222,
      "grad_norm": 7.054487228393555,
      "learning_rate": 3.7985777777777784e-06,
      "loss": 1.4773,
      "step": 69800
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 6.310404300689697,
      "learning_rate": 3.7896888888888893e-06,
      "loss": 1.4791,
      "step": 69900
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 5.769993305206299,
      "learning_rate": 3.7808000000000007e-06,
      "loss": 1.4409,
      "step": 70000
    },
    {
      "epoch": 3.1155555555555554,
      "grad_norm": 5.1509904861450195,
      "learning_rate": 3.7719111111111113e-06,
      "loss": 1.4713,
      "step": 70100
    },
    {
      "epoch": 3.12,
      "grad_norm": 4.624653339385986,
      "learning_rate": 3.7630222222222222e-06,
      "loss": 1.4896,
      "step": 70200
    },
    {
      "epoch": 3.1244444444444444,
      "grad_norm": 5.23190975189209,
      "learning_rate": 3.7541333333333336e-06,
      "loss": 1.5015,
      "step": 70300
    },
    {
      "epoch": 3.128888888888889,
      "grad_norm": 5.831934928894043,
      "learning_rate": 3.7452444444444446e-06,
      "loss": 1.4991,
      "step": 70400
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 5.881008148193359,
      "learning_rate": 3.736355555555556e-06,
      "loss": 1.4971,
      "step": 70500
    },
    {
      "epoch": 3.137777777777778,
      "grad_norm": 6.504662036895752,
      "learning_rate": 3.727466666666667e-06,
      "loss": 1.4833,
      "step": 70600
    },
    {
      "epoch": 3.1422222222222222,
      "grad_norm": 6.193457126617432,
      "learning_rate": 3.7185777777777783e-06,
      "loss": 1.4529,
      "step": 70700
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 6.827535629272461,
      "learning_rate": 3.7096888888888892e-06,
      "loss": 1.4792,
      "step": 70800
    },
    {
      "epoch": 3.151111111111111,
      "grad_norm": 5.163330554962158,
      "learning_rate": 3.7008000000000006e-06,
      "loss": 1.478,
      "step": 70900
    },
    {
      "epoch": 3.1555555555555554,
      "grad_norm": 5.801815032958984,
      "learning_rate": 3.6919111111111116e-06,
      "loss": 1.4842,
      "step": 71000
    },
    {
      "epoch": 3.16,
      "grad_norm": 5.748512268066406,
      "learning_rate": 3.683022222222222e-06,
      "loss": 1.4553,
      "step": 71100
    },
    {
      "epoch": 3.1644444444444444,
      "grad_norm": 4.946335792541504,
      "learning_rate": 3.6741333333333335e-06,
      "loss": 1.5002,
      "step": 71200
    },
    {
      "epoch": 3.168888888888889,
      "grad_norm": 7.37860107421875,
      "learning_rate": 3.6652444444444445e-06,
      "loss": 1.4846,
      "step": 71300
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 4.950206279754639,
      "learning_rate": 3.656355555555556e-06,
      "loss": 1.4705,
      "step": 71400
    },
    {
      "epoch": 3.1777777777777776,
      "grad_norm": 6.25102424621582,
      "learning_rate": 3.647555555555556e-06,
      "loss": 1.4672,
      "step": 71500
    },
    {
      "epoch": 3.1822222222222223,
      "grad_norm": 5.933204174041748,
      "learning_rate": 3.638666666666667e-06,
      "loss": 1.485,
      "step": 71600
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 6.293167591094971,
      "learning_rate": 3.629777777777778e-06,
      "loss": 1.4999,
      "step": 71700
    },
    {
      "epoch": 3.1911111111111112,
      "grad_norm": 5.500380992889404,
      "learning_rate": 3.6208888888888894e-06,
      "loss": 1.4871,
      "step": 71800
    },
    {
      "epoch": 3.1955555555555555,
      "grad_norm": 5.314475059509277,
      "learning_rate": 3.6120000000000003e-06,
      "loss": 1.4771,
      "step": 71900
    },
    {
      "epoch": 3.2,
      "grad_norm": 5.918814182281494,
      "learning_rate": 3.6031111111111117e-06,
      "loss": 1.5135,
      "step": 72000
    },
    {
      "epoch": 3.2044444444444444,
      "grad_norm": 4.627537727355957,
      "learning_rate": 3.5942222222222222e-06,
      "loss": 1.4731,
      "step": 72100
    },
    {
      "epoch": 3.2088888888888887,
      "grad_norm": 7.277796745300293,
      "learning_rate": 3.5853333333333336e-06,
      "loss": 1.4986,
      "step": 72200
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 5.7858757972717285,
      "learning_rate": 3.5764444444444446e-06,
      "loss": 1.4279,
      "step": 72300
    },
    {
      "epoch": 3.2177777777777776,
      "grad_norm": 5.33024787902832,
      "learning_rate": 3.567555555555556e-06,
      "loss": 1.4854,
      "step": 72400
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 4.617566108703613,
      "learning_rate": 3.558666666666667e-06,
      "loss": 1.4702,
      "step": 72500
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 5.288885593414307,
      "learning_rate": 3.549777777777778e-06,
      "loss": 1.4604,
      "step": 72600
    },
    {
      "epoch": 3.2311111111111113,
      "grad_norm": 5.84838342666626,
      "learning_rate": 3.5408888888888893e-06,
      "loss": 1.4821,
      "step": 72700
    },
    {
      "epoch": 3.2355555555555555,
      "grad_norm": 5.399189472198486,
      "learning_rate": 3.5320000000000002e-06,
      "loss": 1.4821,
      "step": 72800
    },
    {
      "epoch": 3.24,
      "grad_norm": 8.183982849121094,
      "learning_rate": 3.5231111111111116e-06,
      "loss": 1.5048,
      "step": 72900
    },
    {
      "epoch": 3.2444444444444445,
      "grad_norm": 6.0346479415893555,
      "learning_rate": 3.5142222222222226e-06,
      "loss": 1.513,
      "step": 73000
    },
    {
      "epoch": 3.2488888888888887,
      "grad_norm": 4.88754415512085,
      "learning_rate": 3.5053333333333335e-06,
      "loss": 1.4908,
      "step": 73100
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 6.454187870025635,
      "learning_rate": 3.4964444444444445e-06,
      "loss": 1.47,
      "step": 73200
    },
    {
      "epoch": 3.2577777777777777,
      "grad_norm": 6.0101213455200195,
      "learning_rate": 3.487555555555556e-06,
      "loss": 1.4637,
      "step": 73300
    },
    {
      "epoch": 3.2622222222222224,
      "grad_norm": 6.544532299041748,
      "learning_rate": 3.478666666666667e-06,
      "loss": 1.4994,
      "step": 73400
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 5.672341823577881,
      "learning_rate": 3.469866666666667e-06,
      "loss": 1.5089,
      "step": 73500
    },
    {
      "epoch": 3.2711111111111113,
      "grad_norm": 6.260368824005127,
      "learning_rate": 3.460977777777778e-06,
      "loss": 1.5009,
      "step": 73600
    },
    {
      "epoch": 3.2755555555555556,
      "grad_norm": 6.667388439178467,
      "learning_rate": 3.4520888888888894e-06,
      "loss": 1.4786,
      "step": 73700
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 6.588907718658447,
      "learning_rate": 3.4432000000000003e-06,
      "loss": 1.4937,
      "step": 73800
    },
    {
      "epoch": 3.2844444444444445,
      "grad_norm": 6.877096176147461,
      "learning_rate": 3.4343111111111117e-06,
      "loss": 1.4719,
      "step": 73900
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 4.9716291427612305,
      "learning_rate": 3.4254222222222227e-06,
      "loss": 1.4961,
      "step": 74000
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 5.7824788093566895,
      "learning_rate": 3.4165333333333332e-06,
      "loss": 1.474,
      "step": 74100
    },
    {
      "epoch": 3.2977777777777777,
      "grad_norm": 4.991307735443115,
      "learning_rate": 3.4076444444444446e-06,
      "loss": 1.5111,
      "step": 74200
    },
    {
      "epoch": 3.3022222222222224,
      "grad_norm": 6.096679210662842,
      "learning_rate": 3.3987555555555556e-06,
      "loss": 1.5124,
      "step": 74300
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 6.412217140197754,
      "learning_rate": 3.389866666666667e-06,
      "loss": 1.5067,
      "step": 74400
    },
    {
      "epoch": 3.311111111111111,
      "grad_norm": 6.167520523071289,
      "learning_rate": 3.380977777777778e-06,
      "loss": 1.5005,
      "step": 74500
    },
    {
      "epoch": 3.3155555555555556,
      "grad_norm": 5.6220502853393555,
      "learning_rate": 3.3720888888888893e-06,
      "loss": 1.4873,
      "step": 74600
    },
    {
      "epoch": 3.32,
      "grad_norm": 5.642742156982422,
      "learning_rate": 3.3632000000000003e-06,
      "loss": 1.488,
      "step": 74700
    },
    {
      "epoch": 3.3244444444444445,
      "grad_norm": 5.235828399658203,
      "learning_rate": 3.3543111111111116e-06,
      "loss": 1.4813,
      "step": 74800
    },
    {
      "epoch": 3.328888888888889,
      "grad_norm": 7.0505781173706055,
      "learning_rate": 3.3454222222222226e-06,
      "loss": 1.4905,
      "step": 74900
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 6.213498115539551,
      "learning_rate": 3.336533333333334e-06,
      "loss": 1.519,
      "step": 75000
    },
    {
      "epoch": 3.3377777777777777,
      "grad_norm": 5.498691558837891,
      "learning_rate": 3.3276444444444445e-06,
      "loss": 1.4758,
      "step": 75100
    },
    {
      "epoch": 3.3422222222222224,
      "grad_norm": 6.751542091369629,
      "learning_rate": 3.3187555555555555e-06,
      "loss": 1.4899,
      "step": 75200
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 5.199931621551514,
      "learning_rate": 3.309866666666667e-06,
      "loss": 1.5159,
      "step": 75300
    },
    {
      "epoch": 3.351111111111111,
      "grad_norm": 5.88987398147583,
      "learning_rate": 3.300977777777778e-06,
      "loss": 1.4904,
      "step": 75400
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 5.655282974243164,
      "learning_rate": 3.2920888888888892e-06,
      "loss": 1.4336,
      "step": 75500
    },
    {
      "epoch": 3.36,
      "grad_norm": 6.3228864669799805,
      "learning_rate": 3.2832888888888894e-06,
      "loss": 1.4387,
      "step": 75600
    },
    {
      "epoch": 3.3644444444444446,
      "grad_norm": 4.874483585357666,
      "learning_rate": 3.2744000000000004e-06,
      "loss": 1.4497,
      "step": 75700
    },
    {
      "epoch": 3.368888888888889,
      "grad_norm": 6.095190048217773,
      "learning_rate": 3.2655111111111113e-06,
      "loss": 1.4968,
      "step": 75800
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 5.674749374389648,
      "learning_rate": 3.2566222222222227e-06,
      "loss": 1.5339,
      "step": 75900
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 5.185273170471191,
      "learning_rate": 3.2477333333333337e-06,
      "loss": 1.4812,
      "step": 76000
    },
    {
      "epoch": 3.3822222222222225,
      "grad_norm": 5.0878071784973145,
      "learning_rate": 3.2388444444444446e-06,
      "loss": 1.4735,
      "step": 76100
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 6.037543773651123,
      "learning_rate": 3.2299555555555556e-06,
      "loss": 1.4981,
      "step": 76200
    },
    {
      "epoch": 3.391111111111111,
      "grad_norm": 4.482745170593262,
      "learning_rate": 3.221066666666667e-06,
      "loss": 1.5082,
      "step": 76300
    },
    {
      "epoch": 3.3955555555555557,
      "grad_norm": 6.84913969039917,
      "learning_rate": 3.212177777777778e-06,
      "loss": 1.4517,
      "step": 76400
    },
    {
      "epoch": 3.4,
      "grad_norm": 5.955472946166992,
      "learning_rate": 3.2032888888888893e-06,
      "loss": 1.4712,
      "step": 76500
    },
    {
      "epoch": 3.4044444444444446,
      "grad_norm": 5.478865623474121,
      "learning_rate": 3.1944000000000003e-06,
      "loss": 1.4542,
      "step": 76600
    },
    {
      "epoch": 3.408888888888889,
      "grad_norm": 5.895533561706543,
      "learning_rate": 3.1855111111111112e-06,
      "loss": 1.4927,
      "step": 76700
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 5.396526336669922,
      "learning_rate": 3.1766222222222226e-06,
      "loss": 1.4371,
      "step": 76800
    },
    {
      "epoch": 3.417777777777778,
      "grad_norm": 5.265944957733154,
      "learning_rate": 3.1677333333333336e-06,
      "loss": 1.4577,
      "step": 76900
    },
    {
      "epoch": 3.422222222222222,
      "grad_norm": 5.9266133308410645,
      "learning_rate": 3.158844444444445e-06,
      "loss": 1.4775,
      "step": 77000
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 6.088873386383057,
      "learning_rate": 3.1499555555555555e-06,
      "loss": 1.4859,
      "step": 77100
    },
    {
      "epoch": 3.431111111111111,
      "grad_norm": 7.1954803466796875,
      "learning_rate": 3.141066666666667e-06,
      "loss": 1.4586,
      "step": 77200
    },
    {
      "epoch": 3.4355555555555557,
      "grad_norm": 6.238870620727539,
      "learning_rate": 3.132177777777778e-06,
      "loss": 1.4472,
      "step": 77300
    },
    {
      "epoch": 3.44,
      "grad_norm": 5.8458709716796875,
      "learning_rate": 3.1232888888888892e-06,
      "loss": 1.464,
      "step": 77400
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 6.232944965362549,
      "learning_rate": 3.1144e-06,
      "loss": 1.4436,
      "step": 77500
    },
    {
      "epoch": 3.448888888888889,
      "grad_norm": 5.768046855926514,
      "learning_rate": 3.105511111111111e-06,
      "loss": 1.4906,
      "step": 77600
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 5.622902870178223,
      "learning_rate": 3.0967111111111113e-06,
      "loss": 1.4549,
      "step": 77700
    },
    {
      "epoch": 3.457777777777778,
      "grad_norm": 6.063200950622559,
      "learning_rate": 3.0878222222222227e-06,
      "loss": 1.4753,
      "step": 77800
    },
    {
      "epoch": 3.462222222222222,
      "grad_norm": 7.093649387359619,
      "learning_rate": 3.0789333333333337e-06,
      "loss": 1.4575,
      "step": 77900
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 6.172305583953857,
      "learning_rate": 3.070044444444445e-06,
      "loss": 1.4626,
      "step": 78000
    },
    {
      "epoch": 3.471111111111111,
      "grad_norm": 7.065784454345703,
      "learning_rate": 3.0611555555555556e-06,
      "loss": 1.5199,
      "step": 78100
    },
    {
      "epoch": 3.4755555555555557,
      "grad_norm": 5.617783069610596,
      "learning_rate": 3.0522666666666666e-06,
      "loss": 1.4896,
      "step": 78200
    },
    {
      "epoch": 3.48,
      "grad_norm": 6.714874744415283,
      "learning_rate": 3.043377777777778e-06,
      "loss": 1.494,
      "step": 78300
    },
    {
      "epoch": 3.4844444444444447,
      "grad_norm": 5.472385883331299,
      "learning_rate": 3.034488888888889e-06,
      "loss": 1.482,
      "step": 78400
    },
    {
      "epoch": 3.488888888888889,
      "grad_norm": 5.516600608825684,
      "learning_rate": 3.0256000000000003e-06,
      "loss": 1.4577,
      "step": 78500
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 5.53046989440918,
      "learning_rate": 3.0167111111111113e-06,
      "loss": 1.4613,
      "step": 78600
    },
    {
      "epoch": 3.497777777777778,
      "grad_norm": 6.263238906860352,
      "learning_rate": 3.0078222222222227e-06,
      "loss": 1.4414,
      "step": 78700
    },
    {
      "epoch": 3.502222222222222,
      "grad_norm": 6.013365268707275,
      "learning_rate": 2.9989333333333336e-06,
      "loss": 1.4689,
      "step": 78800
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 6.321290493011475,
      "learning_rate": 2.990044444444445e-06,
      "loss": 1.4577,
      "step": 78900
    },
    {
      "epoch": 3.511111111111111,
      "grad_norm": 8.205909729003906,
      "learning_rate": 2.981155555555556e-06,
      "loss": 1.474,
      "step": 79000
    },
    {
      "epoch": 3.5155555555555553,
      "grad_norm": 6.314091682434082,
      "learning_rate": 2.9722666666666665e-06,
      "loss": 1.4982,
      "step": 79100
    },
    {
      "epoch": 3.52,
      "grad_norm": 5.178350448608398,
      "learning_rate": 2.963377777777778e-06,
      "loss": 1.4953,
      "step": 79200
    },
    {
      "epoch": 3.5244444444444447,
      "grad_norm": 6.452359676361084,
      "learning_rate": 2.954488888888889e-06,
      "loss": 1.4674,
      "step": 79300
    },
    {
      "epoch": 3.528888888888889,
      "grad_norm": 5.745318412780762,
      "learning_rate": 2.9456000000000002e-06,
      "loss": 1.4774,
      "step": 79400
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 5.92656946182251,
      "learning_rate": 2.936711111111111e-06,
      "loss": 1.4729,
      "step": 79500
    },
    {
      "epoch": 3.537777777777778,
      "grad_norm": 6.3130950927734375,
      "learning_rate": 2.9278222222222226e-06,
      "loss": 1.4555,
      "step": 79600
    },
    {
      "epoch": 3.542222222222222,
      "grad_norm": 5.6452436447143555,
      "learning_rate": 2.9190222222222223e-06,
      "loss": 1.4867,
      "step": 79700
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 6.028998374938965,
      "learning_rate": 2.9101333333333337e-06,
      "loss": 1.4729,
      "step": 79800
    },
    {
      "epoch": 3.551111111111111,
      "grad_norm": 6.4552459716796875,
      "learning_rate": 2.9012444444444447e-06,
      "loss": 1.4655,
      "step": 79900
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 6.396975040435791,
      "learning_rate": 2.892355555555556e-06,
      "loss": 1.484,
      "step": 80000
    },
    {
      "epoch": 3.56,
      "grad_norm": 5.559775352478027,
      "learning_rate": 2.8834666666666666e-06,
      "loss": 1.4759,
      "step": 80100
    },
    {
      "epoch": 3.5644444444444443,
      "grad_norm": 5.721710205078125,
      "learning_rate": 2.874577777777778e-06,
      "loss": 1.4939,
      "step": 80200
    },
    {
      "epoch": 3.568888888888889,
      "grad_norm": 6.613556385040283,
      "learning_rate": 2.865688888888889e-06,
      "loss": 1.4553,
      "step": 80300
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 5.795932769775391,
      "learning_rate": 2.8568000000000003e-06,
      "loss": 1.4581,
      "step": 80400
    },
    {
      "epoch": 3.5777777777777775,
      "grad_norm": 6.397973537445068,
      "learning_rate": 2.8479111111111113e-06,
      "loss": 1.4493,
      "step": 80500
    },
    {
      "epoch": 3.582222222222222,
      "grad_norm": 5.10132360458374,
      "learning_rate": 2.8390222222222223e-06,
      "loss": 1.449,
      "step": 80600
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 6.435302734375,
      "learning_rate": 2.8301333333333336e-06,
      "loss": 1.4394,
      "step": 80700
    },
    {
      "epoch": 3.591111111111111,
      "grad_norm": 5.645084381103516,
      "learning_rate": 2.8212444444444446e-06,
      "loss": 1.4658,
      "step": 80800
    },
    {
      "epoch": 3.5955555555555554,
      "grad_norm": 6.676713943481445,
      "learning_rate": 2.812355555555556e-06,
      "loss": 1.4998,
      "step": 80900
    },
    {
      "epoch": 3.6,
      "grad_norm": 6.176899433135986,
      "learning_rate": 2.803466666666667e-06,
      "loss": 1.4882,
      "step": 81000
    },
    {
      "epoch": 3.6044444444444443,
      "grad_norm": 5.3763556480407715,
      "learning_rate": 2.7945777777777783e-06,
      "loss": 1.4388,
      "step": 81100
    },
    {
      "epoch": 3.608888888888889,
      "grad_norm": 5.51738166809082,
      "learning_rate": 2.785688888888889e-06,
      "loss": 1.513,
      "step": 81200
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 6.114760398864746,
      "learning_rate": 2.7768000000000002e-06,
      "loss": 1.4545,
      "step": 81300
    },
    {
      "epoch": 3.6177777777777775,
      "grad_norm": 6.055105686187744,
      "learning_rate": 2.767911111111111e-06,
      "loss": 1.4804,
      "step": 81400
    },
    {
      "epoch": 3.6222222222222222,
      "grad_norm": 5.831375598907471,
      "learning_rate": 2.759022222222222e-06,
      "loss": 1.5023,
      "step": 81500
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 6.47007417678833,
      "learning_rate": 2.7501333333333336e-06,
      "loss": 1.481,
      "step": 81600
    },
    {
      "epoch": 3.631111111111111,
      "grad_norm": 6.083168029785156,
      "learning_rate": 2.7412444444444445e-06,
      "loss": 1.4875,
      "step": 81700
    },
    {
      "epoch": 3.6355555555555554,
      "grad_norm": 7.284368991851807,
      "learning_rate": 2.7324444444444447e-06,
      "loss": 1.4483,
      "step": 81800
    },
    {
      "epoch": 3.64,
      "grad_norm": 5.553062915802002,
      "learning_rate": 2.723555555555556e-06,
      "loss": 1.4934,
      "step": 81900
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 5.737816333770752,
      "learning_rate": 2.714666666666667e-06,
      "loss": 1.4556,
      "step": 82000
    },
    {
      "epoch": 3.648888888888889,
      "grad_norm": 6.463519096374512,
      "learning_rate": 2.7057777777777776e-06,
      "loss": 1.4798,
      "step": 82100
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 6.61394739151001,
      "learning_rate": 2.696888888888889e-06,
      "loss": 1.5071,
      "step": 82200
    },
    {
      "epoch": 3.6577777777777776,
      "grad_norm": 5.780580043792725,
      "learning_rate": 2.688e-06,
      "loss": 1.4795,
      "step": 82300
    },
    {
      "epoch": 3.6622222222222223,
      "grad_norm": 6.994561195373535,
      "learning_rate": 2.6791111111111113e-06,
      "loss": 1.493,
      "step": 82400
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 5.392574787139893,
      "learning_rate": 2.6702222222222223e-06,
      "loss": 1.503,
      "step": 82500
    },
    {
      "epoch": 3.671111111111111,
      "grad_norm": 6.888299942016602,
      "learning_rate": 2.6613333333333337e-06,
      "loss": 1.4787,
      "step": 82600
    },
    {
      "epoch": 3.6755555555555555,
      "grad_norm": 5.4014892578125,
      "learning_rate": 2.6524444444444446e-06,
      "loss": 1.4506,
      "step": 82700
    },
    {
      "epoch": 3.68,
      "grad_norm": 6.886924743652344,
      "learning_rate": 2.643555555555556e-06,
      "loss": 1.4927,
      "step": 82800
    },
    {
      "epoch": 3.6844444444444444,
      "grad_norm": 5.084714412689209,
      "learning_rate": 2.634666666666667e-06,
      "loss": 1.4866,
      "step": 82900
    },
    {
      "epoch": 3.688888888888889,
      "grad_norm": 6.514251708984375,
      "learning_rate": 2.6257777777777783e-06,
      "loss": 1.4695,
      "step": 83000
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 6.548316478729248,
      "learning_rate": 2.6168888888888893e-06,
      "loss": 1.4788,
      "step": 83100
    },
    {
      "epoch": 3.6977777777777776,
      "grad_norm": 5.997863292694092,
      "learning_rate": 2.608e-06,
      "loss": 1.4959,
      "step": 83200
    },
    {
      "epoch": 3.7022222222222223,
      "grad_norm": 6.067978858947754,
      "learning_rate": 2.5991111111111112e-06,
      "loss": 1.5126,
      "step": 83300
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 5.753020763397217,
      "learning_rate": 2.590222222222222e-06,
      "loss": 1.4723,
      "step": 83400
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 5.67930793762207,
      "learning_rate": 2.5813333333333336e-06,
      "loss": 1.4457,
      "step": 83500
    },
    {
      "epoch": 3.7155555555555555,
      "grad_norm": 5.754179000854492,
      "learning_rate": 2.5724444444444445e-06,
      "loss": 1.4706,
      "step": 83600
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 6.850240230560303,
      "learning_rate": 2.563555555555556e-06,
      "loss": 1.475,
      "step": 83700
    },
    {
      "epoch": 3.7244444444444444,
      "grad_norm": 6.298110485076904,
      "learning_rate": 2.5547555555555557e-06,
      "loss": 1.4801,
      "step": 83800
    },
    {
      "epoch": 3.728888888888889,
      "grad_norm": 5.886129856109619,
      "learning_rate": 2.545866666666667e-06,
      "loss": 1.4725,
      "step": 83900
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 5.315913200378418,
      "learning_rate": 2.536977777777778e-06,
      "loss": 1.4546,
      "step": 84000
    },
    {
      "epoch": 3.7377777777777776,
      "grad_norm": 5.611718654632568,
      "learning_rate": 2.5280888888888894e-06,
      "loss": 1.445,
      "step": 84100
    },
    {
      "epoch": 3.7422222222222223,
      "grad_norm": 6.184893608093262,
      "learning_rate": 2.5192e-06,
      "loss": 1.4497,
      "step": 84200
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 6.290193557739258,
      "learning_rate": 2.5103111111111113e-06,
      "loss": 1.4941,
      "step": 84300
    },
    {
      "epoch": 3.7511111111111113,
      "grad_norm": 6.400639057159424,
      "learning_rate": 2.5014222222222223e-06,
      "loss": 1.4921,
      "step": 84400
    },
    {
      "epoch": 3.7555555555555555,
      "grad_norm": 7.843289375305176,
      "learning_rate": 2.4925333333333337e-06,
      "loss": 1.4958,
      "step": 84500
    },
    {
      "epoch": 3.76,
      "grad_norm": 6.301848411560059,
      "learning_rate": 2.4836444444444446e-06,
      "loss": 1.4887,
      "step": 84600
    },
    {
      "epoch": 3.7644444444444445,
      "grad_norm": 6.512951850891113,
      "learning_rate": 2.4747555555555556e-06,
      "loss": 1.475,
      "step": 84700
    },
    {
      "epoch": 3.7688888888888887,
      "grad_norm": 5.72511100769043,
      "learning_rate": 2.465866666666667e-06,
      "loss": 1.5002,
      "step": 84800
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 4.795119285583496,
      "learning_rate": 2.456977777777778e-06,
      "loss": 1.4876,
      "step": 84900
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 5.793945789337158,
      "learning_rate": 2.448088888888889e-06,
      "loss": 1.4629,
      "step": 85000
    },
    {
      "epoch": 3.7822222222222224,
      "grad_norm": 5.594667434692383,
      "learning_rate": 2.4392000000000003e-06,
      "loss": 1.449,
      "step": 85100
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 5.289131164550781,
      "learning_rate": 2.4303111111111113e-06,
      "loss": 1.4375,
      "step": 85200
    },
    {
      "epoch": 3.7911111111111113,
      "grad_norm": 6.408015727996826,
      "learning_rate": 2.4214222222222226e-06,
      "loss": 1.4636,
      "step": 85300
    },
    {
      "epoch": 3.7955555555555556,
      "grad_norm": 6.116429328918457,
      "learning_rate": 2.4125333333333336e-06,
      "loss": 1.4426,
      "step": 85400
    },
    {
      "epoch": 3.8,
      "grad_norm": 5.315277576446533,
      "learning_rate": 2.4036444444444446e-06,
      "loss": 1.4902,
      "step": 85500
    },
    {
      "epoch": 3.8044444444444445,
      "grad_norm": 8.665156364440918,
      "learning_rate": 2.3947555555555555e-06,
      "loss": 1.4725,
      "step": 85600
    },
    {
      "epoch": 3.8088888888888888,
      "grad_norm": 5.568695068359375,
      "learning_rate": 2.385866666666667e-06,
      "loss": 1.4566,
      "step": 85700
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 5.946971416473389,
      "learning_rate": 2.376977777777778e-06,
      "loss": 1.5048,
      "step": 85800
    },
    {
      "epoch": 3.8177777777777777,
      "grad_norm": 5.770876407623291,
      "learning_rate": 2.368177777777778e-06,
      "loss": 1.4609,
      "step": 85900
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 5.67747688293457,
      "learning_rate": 2.359288888888889e-06,
      "loss": 1.4477,
      "step": 86000
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 5.977498531341553,
      "learning_rate": 2.3504e-06,
      "loss": 1.4871,
      "step": 86100
    },
    {
      "epoch": 3.8311111111111114,
      "grad_norm": 6.229110240936279,
      "learning_rate": 2.3415111111111114e-06,
      "loss": 1.4235,
      "step": 86200
    },
    {
      "epoch": 3.8355555555555556,
      "grad_norm": 5.816751956939697,
      "learning_rate": 2.3326222222222223e-06,
      "loss": 1.4943,
      "step": 86300
    },
    {
      "epoch": 3.84,
      "grad_norm": 5.453084468841553,
      "learning_rate": 2.3237333333333333e-06,
      "loss": 1.5073,
      "step": 86400
    },
    {
      "epoch": 3.8444444444444446,
      "grad_norm": 5.748610019683838,
      "learning_rate": 2.3148444444444447e-06,
      "loss": 1.4915,
      "step": 86500
    },
    {
      "epoch": 3.848888888888889,
      "grad_norm": 6.5627241134643555,
      "learning_rate": 2.3059555555555556e-06,
      "loss": 1.458,
      "step": 86600
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 6.02609395980835,
      "learning_rate": 2.297066666666667e-06,
      "loss": 1.4489,
      "step": 86700
    },
    {
      "epoch": 3.8577777777777778,
      "grad_norm": 5.873152256011963,
      "learning_rate": 2.288177777777778e-06,
      "loss": 1.4966,
      "step": 86800
    },
    {
      "epoch": 3.862222222222222,
      "grad_norm": 5.2707200050354,
      "learning_rate": 2.279288888888889e-06,
      "loss": 1.4618,
      "step": 86900
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 5.243300437927246,
      "learning_rate": 2.2704e-06,
      "loss": 1.4569,
      "step": 87000
    },
    {
      "epoch": 3.871111111111111,
      "grad_norm": 6.555243492126465,
      "learning_rate": 2.2615111111111113e-06,
      "loss": 1.4992,
      "step": 87100
    },
    {
      "epoch": 3.8755555555555556,
      "grad_norm": 5.864575386047363,
      "learning_rate": 2.2526222222222222e-06,
      "loss": 1.4562,
      "step": 87200
    },
    {
      "epoch": 3.88,
      "grad_norm": 6.064406394958496,
      "learning_rate": 2.2437333333333336e-06,
      "loss": 1.4864,
      "step": 87300
    },
    {
      "epoch": 3.8844444444444446,
      "grad_norm": 6.4289326667785645,
      "learning_rate": 2.2348444444444446e-06,
      "loss": 1.4683,
      "step": 87400
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 6.209162712097168,
      "learning_rate": 2.2259555555555555e-06,
      "loss": 1.4622,
      "step": 87500
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 7.085419654846191,
      "learning_rate": 2.217066666666667e-06,
      "loss": 1.4986,
      "step": 87600
    },
    {
      "epoch": 3.897777777777778,
      "grad_norm": 5.4114603996276855,
      "learning_rate": 2.208177777777778e-06,
      "loss": 1.4773,
      "step": 87700
    },
    {
      "epoch": 3.902222222222222,
      "grad_norm": 6.0949788093566895,
      "learning_rate": 2.1992888888888893e-06,
      "loss": 1.4638,
      "step": 87800
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 6.367434501647949,
      "learning_rate": 2.190488888888889e-06,
      "loss": 1.4627,
      "step": 87900
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 6.251357078552246,
      "learning_rate": 2.1816e-06,
      "loss": 1.4783,
      "step": 88000
    },
    {
      "epoch": 3.9155555555555557,
      "grad_norm": 6.2452616691589355,
      "learning_rate": 2.1727111111111114e-06,
      "loss": 1.448,
      "step": 88100
    },
    {
      "epoch": 3.92,
      "grad_norm": 7.117393970489502,
      "learning_rate": 2.1638222222222223e-06,
      "loss": 1.4553,
      "step": 88200
    },
    {
      "epoch": 3.924444444444444,
      "grad_norm": 5.848743915557861,
      "learning_rate": 2.1549333333333337e-06,
      "loss": 1.4386,
      "step": 88300
    },
    {
      "epoch": 3.928888888888889,
      "grad_norm": 5.032512664794922,
      "learning_rate": 2.1460444444444447e-06,
      "loss": 1.5027,
      "step": 88400
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 5.097212791442871,
      "learning_rate": 2.1371555555555557e-06,
      "loss": 1.4922,
      "step": 88500
    },
    {
      "epoch": 3.937777777777778,
      "grad_norm": 5.168776035308838,
      "learning_rate": 2.1282666666666666e-06,
      "loss": 1.458,
      "step": 88600
    },
    {
      "epoch": 3.942222222222222,
      "grad_norm": 7.001770496368408,
      "learning_rate": 2.119377777777778e-06,
      "loss": 1.4451,
      "step": 88700
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 6.935302734375,
      "learning_rate": 2.110488888888889e-06,
      "loss": 1.4622,
      "step": 88800
    },
    {
      "epoch": 3.951111111111111,
      "grad_norm": 5.767982482910156,
      "learning_rate": 2.1016000000000003e-06,
      "loss": 1.4653,
      "step": 88900
    },
    {
      "epoch": 3.9555555555555557,
      "grad_norm": 6.1770453453063965,
      "learning_rate": 2.0927111111111113e-06,
      "loss": 1.4809,
      "step": 89000
    },
    {
      "epoch": 3.96,
      "grad_norm": 5.755301475524902,
      "learning_rate": 2.0838222222222223e-06,
      "loss": 1.4501,
      "step": 89100
    },
    {
      "epoch": 3.964444444444444,
      "grad_norm": 5.646642208099365,
      "learning_rate": 2.0749333333333336e-06,
      "loss": 1.4586,
      "step": 89200
    },
    {
      "epoch": 3.968888888888889,
      "grad_norm": 6.5859222412109375,
      "learning_rate": 2.0660444444444446e-06,
      "loss": 1.4917,
      "step": 89300
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 5.752042770385742,
      "learning_rate": 2.057155555555556e-06,
      "loss": 1.531,
      "step": 89400
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 5.757594585418701,
      "learning_rate": 2.0482666666666665e-06,
      "loss": 1.4679,
      "step": 89500
    },
    {
      "epoch": 3.982222222222222,
      "grad_norm": 7.0830864906311035,
      "learning_rate": 2.039377777777778e-06,
      "loss": 1.4878,
      "step": 89600
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 5.22099494934082,
      "learning_rate": 2.030488888888889e-06,
      "loss": 1.5051,
      "step": 89700
    },
    {
      "epoch": 3.991111111111111,
      "grad_norm": 4.855579376220703,
      "learning_rate": 2.0216000000000003e-06,
      "loss": 1.4563,
      "step": 89800
    },
    {
      "epoch": 3.9955555555555557,
      "grad_norm": 5.169380187988281,
      "learning_rate": 2.0128e-06,
      "loss": 1.4914,
      "step": 89900
    },
    {
      "epoch": 4.0,
      "grad_norm": 5.464121341705322,
      "learning_rate": 2.003911111111111e-06,
      "loss": 1.4571,
      "step": 90000
    },
    {
      "epoch": 4.004444444444444,
      "grad_norm": 5.717636585235596,
      "learning_rate": 1.9950222222222224e-06,
      "loss": 1.475,
      "step": 90100
    },
    {
      "epoch": 4.0088888888888885,
      "grad_norm": 6.452152729034424,
      "learning_rate": 1.9861333333333333e-06,
      "loss": 1.4771,
      "step": 90200
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 6.054673671722412,
      "learning_rate": 1.9772444444444447e-06,
      "loss": 1.4775,
      "step": 90300
    },
    {
      "epoch": 4.017777777777778,
      "grad_norm": 6.880422115325928,
      "learning_rate": 1.9683555555555557e-06,
      "loss": 1.4429,
      "step": 90400
    },
    {
      "epoch": 4.022222222222222,
      "grad_norm": 6.652312755584717,
      "learning_rate": 1.9594666666666666e-06,
      "loss": 1.4251,
      "step": 90500
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 4.691494941711426,
      "learning_rate": 1.950577777777778e-06,
      "loss": 1.4714,
      "step": 90600
    },
    {
      "epoch": 4.0311111111111115,
      "grad_norm": 6.9395527839660645,
      "learning_rate": 1.941688888888889e-06,
      "loss": 1.4818,
      "step": 90700
    },
    {
      "epoch": 4.035555555555556,
      "grad_norm": 6.544039249420166,
      "learning_rate": 1.9328000000000004e-06,
      "loss": 1.4506,
      "step": 90800
    },
    {
      "epoch": 4.04,
      "grad_norm": 5.471756458282471,
      "learning_rate": 1.9239111111111113e-06,
      "loss": 1.4484,
      "step": 90900
    },
    {
      "epoch": 4.044444444444444,
      "grad_norm": 7.148268222808838,
      "learning_rate": 1.9150222222222223e-06,
      "loss": 1.4338,
      "step": 91000
    },
    {
      "epoch": 4.0488888888888885,
      "grad_norm": 5.525921821594238,
      "learning_rate": 1.9061333333333335e-06,
      "loss": 1.4738,
      "step": 91100
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 5.404995918273926,
      "learning_rate": 1.8972444444444446e-06,
      "loss": 1.4746,
      "step": 91200
    },
    {
      "epoch": 4.057777777777778,
      "grad_norm": 6.0300469398498535,
      "learning_rate": 1.8883555555555558e-06,
      "loss": 1.4833,
      "step": 91300
    },
    {
      "epoch": 4.062222222222222,
      "grad_norm": 5.615963935852051,
      "learning_rate": 1.879466666666667e-06,
      "loss": 1.4473,
      "step": 91400
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 5.653198719024658,
      "learning_rate": 1.8705777777777777e-06,
      "loss": 1.4297,
      "step": 91500
    },
    {
      "epoch": 4.071111111111111,
      "grad_norm": 5.966580867767334,
      "learning_rate": 1.861688888888889e-06,
      "loss": 1.4834,
      "step": 91600
    },
    {
      "epoch": 4.075555555555556,
      "grad_norm": 6.429533004760742,
      "learning_rate": 1.8528e-06,
      "loss": 1.4748,
      "step": 91700
    },
    {
      "epoch": 4.08,
      "grad_norm": 5.717661380767822,
      "learning_rate": 1.8439111111111112e-06,
      "loss": 1.4524,
      "step": 91800
    },
    {
      "epoch": 4.084444444444444,
      "grad_norm": 6.148539066314697,
      "learning_rate": 1.8351111111111114e-06,
      "loss": 1.4398,
      "step": 91900
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 6.0812087059021,
      "learning_rate": 1.8262222222222222e-06,
      "loss": 1.4411,
      "step": 92000
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 5.202348709106445,
      "learning_rate": 1.8173333333333334e-06,
      "loss": 1.4979,
      "step": 92100
    },
    {
      "epoch": 4.097777777777778,
      "grad_norm": 5.843108177185059,
      "learning_rate": 1.8084444444444445e-06,
      "loss": 1.4595,
      "step": 92200
    },
    {
      "epoch": 4.102222222222222,
      "grad_norm": 6.138033866882324,
      "learning_rate": 1.7995555555555557e-06,
      "loss": 1.4591,
      "step": 92300
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 6.948319911956787,
      "learning_rate": 1.7906666666666669e-06,
      "loss": 1.4759,
      "step": 92400
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 5.580836772918701,
      "learning_rate": 1.7817777777777778e-06,
      "loss": 1.499,
      "step": 92500
    },
    {
      "epoch": 4.115555555555556,
      "grad_norm": 5.709314346313477,
      "learning_rate": 1.772888888888889e-06,
      "loss": 1.4998,
      "step": 92600
    },
    {
      "epoch": 4.12,
      "grad_norm": 5.595238208770752,
      "learning_rate": 1.7640000000000002e-06,
      "loss": 1.4919,
      "step": 92700
    },
    {
      "epoch": 4.124444444444444,
      "grad_norm": 5.944914817810059,
      "learning_rate": 1.7551111111111114e-06,
      "loss": 1.484,
      "step": 92800
    },
    {
      "epoch": 4.128888888888889,
      "grad_norm": 4.783907890319824,
      "learning_rate": 1.7462222222222225e-06,
      "loss": 1.472,
      "step": 92900
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 6.931858062744141,
      "learning_rate": 1.7373333333333333e-06,
      "loss": 1.4611,
      "step": 93000
    },
    {
      "epoch": 4.137777777777778,
      "grad_norm": 5.4562602043151855,
      "learning_rate": 1.7284444444444444e-06,
      "loss": 1.4456,
      "step": 93100
    },
    {
      "epoch": 4.142222222222222,
      "grad_norm": 6.857855796813965,
      "learning_rate": 1.7195555555555556e-06,
      "loss": 1.4749,
      "step": 93200
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 5.794741630554199,
      "learning_rate": 1.7106666666666668e-06,
      "loss": 1.4431,
      "step": 93300
    },
    {
      "epoch": 4.151111111111111,
      "grad_norm": 5.899787425994873,
      "learning_rate": 1.701777777777778e-06,
      "loss": 1.4337,
      "step": 93400
    },
    {
      "epoch": 4.155555555555556,
      "grad_norm": 6.777344703674316,
      "learning_rate": 1.692888888888889e-06,
      "loss": 1.4631,
      "step": 93500
    },
    {
      "epoch": 4.16,
      "grad_norm": 5.255881309509277,
      "learning_rate": 1.684e-06,
      "loss": 1.4727,
      "step": 93600
    },
    {
      "epoch": 4.164444444444444,
      "grad_norm": 6.042337894439697,
      "learning_rate": 1.6751111111111113e-06,
      "loss": 1.4623,
      "step": 93700
    },
    {
      "epoch": 4.168888888888889,
      "grad_norm": 5.695792198181152,
      "learning_rate": 1.6662222222222224e-06,
      "loss": 1.4312,
      "step": 93800
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 5.632971286773682,
      "learning_rate": 1.6573333333333336e-06,
      "loss": 1.4357,
      "step": 93900
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 6.096491813659668,
      "learning_rate": 1.6485333333333334e-06,
      "loss": 1.4681,
      "step": 94000
    },
    {
      "epoch": 4.182222222222222,
      "grad_norm": 6.124307632446289,
      "learning_rate": 1.6396444444444446e-06,
      "loss": 1.4901,
      "step": 94100
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 5.642000198364258,
      "learning_rate": 1.6307555555555557e-06,
      "loss": 1.4659,
      "step": 94200
    },
    {
      "epoch": 4.191111111111111,
      "grad_norm": 5.49066686630249,
      "learning_rate": 1.621866666666667e-06,
      "loss": 1.4485,
      "step": 94300
    },
    {
      "epoch": 4.195555555555556,
      "grad_norm": 5.986698627471924,
      "learning_rate": 1.612977777777778e-06,
      "loss": 1.462,
      "step": 94400
    },
    {
      "epoch": 4.2,
      "grad_norm": 5.647345066070557,
      "learning_rate": 1.6040888888888888e-06,
      "loss": 1.4516,
      "step": 94500
    },
    {
      "epoch": 4.204444444444444,
      "grad_norm": 6.966411590576172,
      "learning_rate": 1.5952e-06,
      "loss": 1.4975,
      "step": 94600
    },
    {
      "epoch": 4.208888888888889,
      "grad_norm": 6.3130340576171875,
      "learning_rate": 1.5863111111111112e-06,
      "loss": 1.446,
      "step": 94700
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 6.474659442901611,
      "learning_rate": 1.5774222222222223e-06,
      "loss": 1.4784,
      "step": 94800
    },
    {
      "epoch": 4.217777777777778,
      "grad_norm": 4.534820556640625,
      "learning_rate": 1.5685333333333335e-06,
      "loss": 1.4629,
      "step": 94900
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 5.672637462615967,
      "learning_rate": 1.5596444444444445e-06,
      "loss": 1.4774,
      "step": 95000
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 7.0561323165893555,
      "learning_rate": 1.5507555555555556e-06,
      "loss": 1.4757,
      "step": 95100
    },
    {
      "epoch": 4.231111111111111,
      "grad_norm": 5.929336071014404,
      "learning_rate": 1.5418666666666668e-06,
      "loss": 1.4662,
      "step": 95200
    },
    {
      "epoch": 4.235555555555556,
      "grad_norm": 5.334821701049805,
      "learning_rate": 1.532977777777778e-06,
      "loss": 1.4338,
      "step": 95300
    },
    {
      "epoch": 4.24,
      "grad_norm": 5.582449436187744,
      "learning_rate": 1.5240888888888892e-06,
      "loss": 1.4262,
      "step": 95400
    },
    {
      "epoch": 4.2444444444444445,
      "grad_norm": 6.467552661895752,
      "learning_rate": 1.5152e-06,
      "loss": 1.4559,
      "step": 95500
    },
    {
      "epoch": 4.248888888888889,
      "grad_norm": 6.5976715087890625,
      "learning_rate": 1.506311111111111e-06,
      "loss": 1.4511,
      "step": 95600
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 5.364105701446533,
      "learning_rate": 1.4974222222222223e-06,
      "loss": 1.4718,
      "step": 95700
    },
    {
      "epoch": 4.257777777777778,
      "grad_norm": 6.4009108543396,
      "learning_rate": 1.4885333333333334e-06,
      "loss": 1.4919,
      "step": 95800
    },
    {
      "epoch": 4.262222222222222,
      "grad_norm": 5.448566913604736,
      "learning_rate": 1.4796444444444446e-06,
      "loss": 1.4705,
      "step": 95900
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 5.5686936378479,
      "learning_rate": 1.4708444444444444e-06,
      "loss": 1.4433,
      "step": 96000
    },
    {
      "epoch": 4.271111111111111,
      "grad_norm": 5.567440509796143,
      "learning_rate": 1.4619555555555555e-06,
      "loss": 1.4672,
      "step": 96100
    },
    {
      "epoch": 4.275555555555556,
      "grad_norm": 4.823049068450928,
      "learning_rate": 1.4530666666666667e-06,
      "loss": 1.4427,
      "step": 96200
    },
    {
      "epoch": 4.28,
      "grad_norm": 6.498258590698242,
      "learning_rate": 1.4441777777777779e-06,
      "loss": 1.4942,
      "step": 96300
    },
    {
      "epoch": 4.2844444444444445,
      "grad_norm": 5.807685852050781,
      "learning_rate": 1.435288888888889e-06,
      "loss": 1.4476,
      "step": 96400
    },
    {
      "epoch": 4.288888888888889,
      "grad_norm": 6.143428802490234,
      "learning_rate": 1.4264e-06,
      "loss": 1.4688,
      "step": 96500
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 6.4391679763793945,
      "learning_rate": 1.4175111111111112e-06,
      "loss": 1.4512,
      "step": 96600
    },
    {
      "epoch": 4.297777777777778,
      "grad_norm": 6.691664695739746,
      "learning_rate": 1.4086222222222224e-06,
      "loss": 1.4704,
      "step": 96700
    },
    {
      "epoch": 4.302222222222222,
      "grad_norm": 5.731221675872803,
      "learning_rate": 1.3997333333333335e-06,
      "loss": 1.45,
      "step": 96800
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 6.569011688232422,
      "learning_rate": 1.3908444444444447e-06,
      "loss": 1.4383,
      "step": 96900
    },
    {
      "epoch": 4.311111111111111,
      "grad_norm": 5.808239936828613,
      "learning_rate": 1.3819555555555555e-06,
      "loss": 1.4699,
      "step": 97000
    },
    {
      "epoch": 4.315555555555555,
      "grad_norm": 5.99342155456543,
      "learning_rate": 1.3730666666666666e-06,
      "loss": 1.5139,
      "step": 97100
    },
    {
      "epoch": 4.32,
      "grad_norm": 6.331967830657959,
      "learning_rate": 1.3641777777777778e-06,
      "loss": 1.486,
      "step": 97200
    },
    {
      "epoch": 4.3244444444444445,
      "grad_norm": 5.728707790374756,
      "learning_rate": 1.355288888888889e-06,
      "loss": 1.4677,
      "step": 97300
    },
    {
      "epoch": 4.328888888888889,
      "grad_norm": 5.79250955581665,
      "learning_rate": 1.3464000000000001e-06,
      "loss": 1.4675,
      "step": 97400
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 5.870486259460449,
      "learning_rate": 1.337511111111111e-06,
      "loss": 1.4409,
      "step": 97500
    },
    {
      "epoch": 4.337777777777778,
      "grad_norm": 6.207481861114502,
      "learning_rate": 1.3286222222222223e-06,
      "loss": 1.4271,
      "step": 97600
    },
    {
      "epoch": 4.342222222222222,
      "grad_norm": 4.744669437408447,
      "learning_rate": 1.3197333333333335e-06,
      "loss": 1.4313,
      "step": 97700
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 8.293895721435547,
      "learning_rate": 1.3108444444444446e-06,
      "loss": 1.4919,
      "step": 97800
    },
    {
      "epoch": 4.351111111111111,
      "grad_norm": 6.367178916931152,
      "learning_rate": 1.3019555555555558e-06,
      "loss": 1.487,
      "step": 97900
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 5.550983905792236,
      "learning_rate": 1.2930666666666665e-06,
      "loss": 1.4556,
      "step": 98000
    },
    {
      "epoch": 4.36,
      "grad_norm": 4.73635721206665,
      "learning_rate": 1.2842666666666667e-06,
      "loss": 1.4564,
      "step": 98100
    },
    {
      "epoch": 4.364444444444445,
      "grad_norm": 6.090312480926514,
      "learning_rate": 1.275377777777778e-06,
      "loss": 1.4653,
      "step": 98200
    },
    {
      "epoch": 4.368888888888889,
      "grad_norm": 4.916983127593994,
      "learning_rate": 1.266488888888889e-06,
      "loss": 1.4457,
      "step": 98300
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 4.965715408325195,
      "learning_rate": 1.2576000000000002e-06,
      "loss": 1.4645,
      "step": 98400
    },
    {
      "epoch": 4.377777777777778,
      "grad_norm": 5.9778008460998535,
      "learning_rate": 1.2487111111111112e-06,
      "loss": 1.4819,
      "step": 98500
    },
    {
      "epoch": 4.3822222222222225,
      "grad_norm": 5.598294258117676,
      "learning_rate": 1.2398222222222222e-06,
      "loss": 1.4495,
      "step": 98600
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 4.552911281585693,
      "learning_rate": 1.2309333333333333e-06,
      "loss": 1.4576,
      "step": 98700
    },
    {
      "epoch": 4.391111111111111,
      "grad_norm": 5.565591812133789,
      "learning_rate": 1.2220444444444445e-06,
      "loss": 1.4662,
      "step": 98800
    },
    {
      "epoch": 4.395555555555555,
      "grad_norm": 5.745782375335693,
      "learning_rate": 1.2131555555555557e-06,
      "loss": 1.4588,
      "step": 98900
    },
    {
      "epoch": 4.4,
      "grad_norm": 6.320018768310547,
      "learning_rate": 1.2042666666666669e-06,
      "loss": 1.4596,
      "step": 99000
    },
    {
      "epoch": 4.404444444444445,
      "grad_norm": 6.994563102722168,
      "learning_rate": 1.195377777777778e-06,
      "loss": 1.4774,
      "step": 99100
    },
    {
      "epoch": 4.408888888888889,
      "grad_norm": 5.355563640594482,
      "learning_rate": 1.186488888888889e-06,
      "loss": 1.4615,
      "step": 99200
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 6.470183849334717,
      "learning_rate": 1.1776000000000002e-06,
      "loss": 1.4436,
      "step": 99300
    },
    {
      "epoch": 4.417777777777777,
      "grad_norm": 5.527093887329102,
      "learning_rate": 1.1687111111111111e-06,
      "loss": 1.4657,
      "step": 99400
    },
    {
      "epoch": 4.4222222222222225,
      "grad_norm": 5.246734619140625,
      "learning_rate": 1.1598222222222223e-06,
      "loss": 1.4507,
      "step": 99500
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 5.317460536956787,
      "learning_rate": 1.1509333333333335e-06,
      "loss": 1.4396,
      "step": 99600
    },
    {
      "epoch": 4.431111111111111,
      "grad_norm": 6.125283241271973,
      "learning_rate": 1.1420444444444444e-06,
      "loss": 1.4598,
      "step": 99700
    },
    {
      "epoch": 4.435555555555555,
      "grad_norm": 5.926527976989746,
      "learning_rate": 1.1331555555555556e-06,
      "loss": 1.4529,
      "step": 99800
    },
    {
      "epoch": 4.44,
      "grad_norm": 5.542158603668213,
      "learning_rate": 1.1242666666666668e-06,
      "loss": 1.4516,
      "step": 99900
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 6.173229694366455,
      "learning_rate": 1.115377777777778e-06,
      "loss": 1.4838,
      "step": 100000
    },
    {
      "epoch": 4.448888888888889,
      "grad_norm": 5.433622360229492,
      "learning_rate": 1.1065777777777777e-06,
      "loss": 1.4304,
      "step": 100100
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 5.67618465423584,
      "learning_rate": 1.0976888888888889e-06,
      "loss": 1.4557,
      "step": 100200
    },
    {
      "epoch": 4.457777777777777,
      "grad_norm": 5.997843265533447,
      "learning_rate": 1.0888e-06,
      "loss": 1.4545,
      "step": 100300
    },
    {
      "epoch": 4.4622222222222225,
      "grad_norm": 7.00557279586792,
      "learning_rate": 1.0799111111111112e-06,
      "loss": 1.4768,
      "step": 100400
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 5.914608955383301,
      "learning_rate": 1.0710222222222224e-06,
      "loss": 1.4533,
      "step": 100500
    },
    {
      "epoch": 4.471111111111111,
      "grad_norm": 5.21259069442749,
      "learning_rate": 1.0621333333333336e-06,
      "loss": 1.4694,
      "step": 100600
    },
    {
      "epoch": 4.475555555555555,
      "grad_norm": 6.129192352294922,
      "learning_rate": 1.0532444444444445e-06,
      "loss": 1.4666,
      "step": 100700
    },
    {
      "epoch": 4.48,
      "grad_norm": 6.293127059936523,
      "learning_rate": 1.0443555555555557e-06,
      "loss": 1.44,
      "step": 100800
    },
    {
      "epoch": 4.484444444444445,
      "grad_norm": 6.08936882019043,
      "learning_rate": 1.0354666666666667e-06,
      "loss": 1.4637,
      "step": 100900
    },
    {
      "epoch": 4.488888888888889,
      "grad_norm": 5.724727153778076,
      "learning_rate": 1.0265777777777778e-06,
      "loss": 1.4635,
      "step": 101000
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 5.614858627319336,
      "learning_rate": 1.017688888888889e-06,
      "loss": 1.5004,
      "step": 101100
    },
    {
      "epoch": 4.497777777777777,
      "grad_norm": 6.1114301681518555,
      "learning_rate": 1.0088e-06,
      "loss": 1.4722,
      "step": 101200
    },
    {
      "epoch": 4.502222222222223,
      "grad_norm": 5.086793899536133,
      "learning_rate": 9.999111111111112e-07,
      "loss": 1.4943,
      "step": 101300
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 5.553809642791748,
      "learning_rate": 9.910222222222223e-07,
      "loss": 1.4903,
      "step": 101400
    },
    {
      "epoch": 4.511111111111111,
      "grad_norm": 5.438779354095459,
      "learning_rate": 9.821333333333335e-07,
      "loss": 1.4893,
      "step": 101500
    },
    {
      "epoch": 4.515555555555555,
      "grad_norm": 7.456002712249756,
      "learning_rate": 9.732444444444447e-07,
      "loss": 1.4356,
      "step": 101600
    },
    {
      "epoch": 4.52,
      "grad_norm": 5.3717546463012695,
      "learning_rate": 9.643555555555556e-07,
      "loss": 1.4746,
      "step": 101700
    },
    {
      "epoch": 4.524444444444445,
      "grad_norm": 6.589951038360596,
      "learning_rate": 9.554666666666668e-07,
      "loss": 1.457,
      "step": 101800
    },
    {
      "epoch": 4.528888888888889,
      "grad_norm": 5.940182209014893,
      "learning_rate": 9.465777777777778e-07,
      "loss": 1.4586,
      "step": 101900
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 6.582196235656738,
      "learning_rate": 9.376888888888889e-07,
      "loss": 1.4985,
      "step": 102000
    },
    {
      "epoch": 4.5377777777777775,
      "grad_norm": 5.004903316497803,
      "learning_rate": 9.288000000000001e-07,
      "loss": 1.4827,
      "step": 102100
    },
    {
      "epoch": 4.542222222222223,
      "grad_norm": 4.974508762359619,
      "learning_rate": 9.200000000000001e-07,
      "loss": 1.4684,
      "step": 102200
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 5.590771675109863,
      "learning_rate": 9.111111111111113e-07,
      "loss": 1.4646,
      "step": 102300
    },
    {
      "epoch": 4.551111111111111,
      "grad_norm": 6.194372177124023,
      "learning_rate": 9.022222222222222e-07,
      "loss": 1.4842,
      "step": 102400
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 5.559220314025879,
      "learning_rate": 8.933333333333334e-07,
      "loss": 1.517,
      "step": 102500
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 6.595807075500488,
      "learning_rate": 8.844444444444446e-07,
      "loss": 1.4643,
      "step": 102600
    },
    {
      "epoch": 4.564444444444445,
      "grad_norm": 4.539565563201904,
      "learning_rate": 8.755555555555556e-07,
      "loss": 1.4616,
      "step": 102700
    },
    {
      "epoch": 4.568888888888889,
      "grad_norm": 5.117699146270752,
      "learning_rate": 8.666666666666668e-07,
      "loss": 1.4562,
      "step": 102800
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 6.817371368408203,
      "learning_rate": 8.577777777777778e-07,
      "loss": 1.4698,
      "step": 102900
    },
    {
      "epoch": 4.5777777777777775,
      "grad_norm": 5.118162155151367,
      "learning_rate": 8.488888888888889e-07,
      "loss": 1.4503,
      "step": 103000
    },
    {
      "epoch": 4.582222222222223,
      "grad_norm": 5.916085720062256,
      "learning_rate": 8.400000000000001e-07,
      "loss": 1.4471,
      "step": 103100
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 5.5114617347717285,
      "learning_rate": 8.311111111111112e-07,
      "loss": 1.4483,
      "step": 103200
    },
    {
      "epoch": 4.591111111111111,
      "grad_norm": 5.606804847717285,
      "learning_rate": 8.222222222222223e-07,
      "loss": 1.4952,
      "step": 103300
    },
    {
      "epoch": 4.595555555555555,
      "grad_norm": 6.051234245300293,
      "learning_rate": 8.133333333333333e-07,
      "loss": 1.4781,
      "step": 103400
    },
    {
      "epoch": 4.6,
      "grad_norm": 5.780882358551025,
      "learning_rate": 8.044444444444445e-07,
      "loss": 1.4686,
      "step": 103500
    },
    {
      "epoch": 4.604444444444445,
      "grad_norm": 5.271137237548828,
      "learning_rate": 7.955555555555557e-07,
      "loss": 1.4601,
      "step": 103600
    },
    {
      "epoch": 4.608888888888889,
      "grad_norm": 5.567619323730469,
      "learning_rate": 7.866666666666667e-07,
      "loss": 1.4481,
      "step": 103700
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 6.3453593254089355,
      "learning_rate": 7.777777777777779e-07,
      "loss": 1.4823,
      "step": 103800
    },
    {
      "epoch": 4.6177777777777775,
      "grad_norm": 7.1086578369140625,
      "learning_rate": 7.688888888888891e-07,
      "loss": 1.5016,
      "step": 103900
    },
    {
      "epoch": 4.622222222222222,
      "grad_norm": 5.673357009887695,
      "learning_rate": 7.6e-07,
      "loss": 1.4508,
      "step": 104000
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 5.108613967895508,
      "learning_rate": 7.511111111111112e-07,
      "loss": 1.4438,
      "step": 104100
    },
    {
      "epoch": 4.631111111111111,
      "grad_norm": 5.772858142852783,
      "learning_rate": 7.422222222222223e-07,
      "loss": 1.4719,
      "step": 104200
    },
    {
      "epoch": 4.635555555555555,
      "grad_norm": 5.130529403686523,
      "learning_rate": 7.334222222222223e-07,
      "loss": 1.4403,
      "step": 104300
    },
    {
      "epoch": 4.64,
      "grad_norm": 6.230302333831787,
      "learning_rate": 7.245333333333333e-07,
      "loss": 1.4688,
      "step": 104400
    },
    {
      "epoch": 4.644444444444445,
      "grad_norm": 6.764962673187256,
      "learning_rate": 7.156444444444445e-07,
      "loss": 1.4749,
      "step": 104500
    },
    {
      "epoch": 4.648888888888889,
      "grad_norm": 5.979761123657227,
      "learning_rate": 7.067555555555557e-07,
      "loss": 1.4537,
      "step": 104600
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 5.448606014251709,
      "learning_rate": 6.978666666666667e-07,
      "loss": 1.4421,
      "step": 104700
    },
    {
      "epoch": 4.657777777777778,
      "grad_norm": 5.669739246368408,
      "learning_rate": 6.889777777777779e-07,
      "loss": 1.4169,
      "step": 104800
    },
    {
      "epoch": 4.662222222222223,
      "grad_norm": 6.838133811950684,
      "learning_rate": 6.800888888888889e-07,
      "loss": 1.4689,
      "step": 104900
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 6.236388206481934,
      "learning_rate": 6.712e-07,
      "loss": 1.4356,
      "step": 105000
    },
    {
      "epoch": 4.671111111111111,
      "grad_norm": 6.403168201446533,
      "learning_rate": 6.623111111111112e-07,
      "loss": 1.4631,
      "step": 105100
    },
    {
      "epoch": 4.6755555555555555,
      "grad_norm": 6.411676406860352,
      "learning_rate": 6.534222222222223e-07,
      "loss": 1.4594,
      "step": 105200
    },
    {
      "epoch": 4.68,
      "grad_norm": 5.863963603973389,
      "learning_rate": 6.445333333333334e-07,
      "loss": 1.4775,
      "step": 105300
    },
    {
      "epoch": 4.684444444444445,
      "grad_norm": 5.757351875305176,
      "learning_rate": 6.356444444444446e-07,
      "loss": 1.4049,
      "step": 105400
    },
    {
      "epoch": 4.688888888888889,
      "grad_norm": 5.668099880218506,
      "learning_rate": 6.267555555555556e-07,
      "loss": 1.4706,
      "step": 105500
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 6.80448055267334,
      "learning_rate": 6.178666666666666e-07,
      "loss": 1.467,
      "step": 105600
    },
    {
      "epoch": 4.697777777777778,
      "grad_norm": 5.574192523956299,
      "learning_rate": 6.089777777777778e-07,
      "loss": 1.4847,
      "step": 105700
    },
    {
      "epoch": 4.702222222222222,
      "grad_norm": 5.63320779800415,
      "learning_rate": 6.00088888888889e-07,
      "loss": 1.4723,
      "step": 105800
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 6.713346481323242,
      "learning_rate": 5.912e-07,
      "loss": 1.4804,
      "step": 105900
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 6.701361656188965,
      "learning_rate": 5.823111111111111e-07,
      "loss": 1.4268,
      "step": 106000
    },
    {
      "epoch": 4.7155555555555555,
      "grad_norm": 6.0786943435668945,
      "learning_rate": 5.734222222222223e-07,
      "loss": 1.474,
      "step": 106100
    },
    {
      "epoch": 4.72,
      "grad_norm": 5.86102819442749,
      "learning_rate": 5.645333333333334e-07,
      "loss": 1.504,
      "step": 106200
    },
    {
      "epoch": 4.724444444444444,
      "grad_norm": 6.012783527374268,
      "learning_rate": 5.557333333333334e-07,
      "loss": 1.4645,
      "step": 106300
    },
    {
      "epoch": 4.728888888888889,
      "grad_norm": 6.6905741691589355,
      "learning_rate": 5.468444444444445e-07,
      "loss": 1.4729,
      "step": 106400
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 6.413095474243164,
      "learning_rate": 5.379555555555556e-07,
      "loss": 1.4682,
      "step": 106500
    },
    {
      "epoch": 4.737777777777778,
      "grad_norm": 5.00433349609375,
      "learning_rate": 5.290666666666666e-07,
      "loss": 1.4523,
      "step": 106600
    },
    {
      "epoch": 4.742222222222222,
      "grad_norm": 6.90960168838501,
      "learning_rate": 5.201777777777778e-07,
      "loss": 1.4804,
      "step": 106700
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 5.061540603637695,
      "learning_rate": 5.11288888888889e-07,
      "loss": 1.475,
      "step": 106800
    },
    {
      "epoch": 4.751111111111111,
      "grad_norm": 7.02651309967041,
      "learning_rate": 5.024e-07,
      "loss": 1.4791,
      "step": 106900
    },
    {
      "epoch": 4.7555555555555555,
      "grad_norm": 6.744498252868652,
      "learning_rate": 4.935111111111111e-07,
      "loss": 1.4337,
      "step": 107000
    },
    {
      "epoch": 4.76,
      "grad_norm": 8.213024139404297,
      "learning_rate": 4.846222222222222e-07,
      "loss": 1.4524,
      "step": 107100
    },
    {
      "epoch": 4.764444444444445,
      "grad_norm": 5.174575328826904,
      "learning_rate": 4.757333333333334e-07,
      "loss": 1.449,
      "step": 107200
    },
    {
      "epoch": 4.768888888888889,
      "grad_norm": 6.089974880218506,
      "learning_rate": 4.668444444444445e-07,
      "loss": 1.4765,
      "step": 107300
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 8.099469184875488,
      "learning_rate": 4.579555555555556e-07,
      "loss": 1.4702,
      "step": 107400
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 6.453033447265625,
      "learning_rate": 4.4906666666666666e-07,
      "loss": 1.4636,
      "step": 107500
    },
    {
      "epoch": 4.782222222222222,
      "grad_norm": 6.242104530334473,
      "learning_rate": 4.4017777777777784e-07,
      "loss": 1.4333,
      "step": 107600
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 6.011796951293945,
      "learning_rate": 4.3128888888888896e-07,
      "loss": 1.4514,
      "step": 107700
    },
    {
      "epoch": 4.791111111111111,
      "grad_norm": 5.579388618469238,
      "learning_rate": 4.224e-07,
      "loss": 1.4703,
      "step": 107800
    },
    {
      "epoch": 4.795555555555556,
      "grad_norm": 5.958630561828613,
      "learning_rate": 4.1351111111111114e-07,
      "loss": 1.4544,
      "step": 107900
    },
    {
      "epoch": 4.8,
      "grad_norm": 6.977032661437988,
      "learning_rate": 4.046222222222222e-07,
      "loss": 1.4497,
      "step": 108000
    },
    {
      "epoch": 4.804444444444444,
      "grad_norm": 7.174180507659912,
      "learning_rate": 3.957333333333334e-07,
      "loss": 1.4882,
      "step": 108100
    },
    {
      "epoch": 4.808888888888889,
      "grad_norm": 6.275386333465576,
      "learning_rate": 3.868444444444445e-07,
      "loss": 1.4488,
      "step": 108200
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 8.35708999633789,
      "learning_rate": 3.780444444444445e-07,
      "loss": 1.4651,
      "step": 108300
    },
    {
      "epoch": 4.817777777777778,
      "grad_norm": 6.147104263305664,
      "learning_rate": 3.691555555555556e-07,
      "loss": 1.4443,
      "step": 108400
    },
    {
      "epoch": 4.822222222222222,
      "grad_norm": 5.720384120941162,
      "learning_rate": 3.6026666666666666e-07,
      "loss": 1.4627,
      "step": 108500
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 6.762266635894775,
      "learning_rate": 3.513777777777778e-07,
      "loss": 1.454,
      "step": 108600
    },
    {
      "epoch": 4.831111111111111,
      "grad_norm": 5.2505669593811035,
      "learning_rate": 3.4248888888888896e-07,
      "loss": 1.5007,
      "step": 108700
    },
    {
      "epoch": 4.835555555555556,
      "grad_norm": 6.249788761138916,
      "learning_rate": 3.336e-07,
      "loss": 1.4619,
      "step": 108800
    },
    {
      "epoch": 4.84,
      "grad_norm": 6.55705451965332,
      "learning_rate": 3.2471111111111114e-07,
      "loss": 1.4586,
      "step": 108900
    },
    {
      "epoch": 4.844444444444444,
      "grad_norm": 5.370385646820068,
      "learning_rate": 3.158222222222222e-07,
      "loss": 1.4512,
      "step": 109000
    },
    {
      "epoch": 4.848888888888889,
      "grad_norm": 5.679711818695068,
      "learning_rate": 3.0693333333333333e-07,
      "loss": 1.4521,
      "step": 109100
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 6.2606611251831055,
      "learning_rate": 2.980444444444445e-07,
      "loss": 1.4326,
      "step": 109200
    },
    {
      "epoch": 4.857777777777778,
      "grad_norm": 5.739768028259277,
      "learning_rate": 2.8915555555555557e-07,
      "loss": 1.4363,
      "step": 109300
    },
    {
      "epoch": 4.862222222222222,
      "grad_norm": 6.105921268463135,
      "learning_rate": 2.802666666666667e-07,
      "loss": 1.4266,
      "step": 109400
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 7.147911548614502,
      "learning_rate": 2.713777777777778e-07,
      "loss": 1.4389,
      "step": 109500
    },
    {
      "epoch": 4.871111111111111,
      "grad_norm": 6.3040547370910645,
      "learning_rate": 2.624888888888889e-07,
      "loss": 1.4399,
      "step": 109600
    },
    {
      "epoch": 4.875555555555556,
      "grad_norm": 6.282522201538086,
      "learning_rate": 2.5360000000000005e-07,
      "loss": 1.4749,
      "step": 109700
    },
    {
      "epoch": 4.88,
      "grad_norm": 5.767916202545166,
      "learning_rate": 2.447111111111111e-07,
      "loss": 1.4486,
      "step": 109800
    },
    {
      "epoch": 4.884444444444444,
      "grad_norm": 5.848240852355957,
      "learning_rate": 2.3582222222222226e-07,
      "loss": 1.4219,
      "step": 109900
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 6.074346542358398,
      "learning_rate": 2.2693333333333335e-07,
      "loss": 1.4792,
      "step": 110000
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 5.611324787139893,
      "learning_rate": 2.1804444444444447e-07,
      "loss": 1.4666,
      "step": 110100
    },
    {
      "epoch": 4.897777777777778,
      "grad_norm": 5.431663513183594,
      "learning_rate": 2.0915555555555557e-07,
      "loss": 1.4658,
      "step": 110200
    },
    {
      "epoch": 4.902222222222222,
      "grad_norm": 6.112133502960205,
      "learning_rate": 2.003555555555556e-07,
      "loss": 1.4439,
      "step": 110300
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 6.4361443519592285,
      "learning_rate": 1.914666666666667e-07,
      "loss": 1.477,
      "step": 110400
    },
    {
      "epoch": 4.911111111111111,
      "grad_norm": 6.13197135925293,
      "learning_rate": 1.825777777777778e-07,
      "loss": 1.4887,
      "step": 110500
    },
    {
      "epoch": 4.915555555555556,
      "grad_norm": 5.580016613006592,
      "learning_rate": 1.736888888888889e-07,
      "loss": 1.4438,
      "step": 110600
    },
    {
      "epoch": 4.92,
      "grad_norm": 7.236052513122559,
      "learning_rate": 1.6480000000000002e-07,
      "loss": 1.4772,
      "step": 110700
    },
    {
      "epoch": 4.924444444444444,
      "grad_norm": 5.264605522155762,
      "learning_rate": 1.559111111111111e-07,
      "loss": 1.4895,
      "step": 110800
    },
    {
      "epoch": 4.928888888888888,
      "grad_norm": 5.911496162414551,
      "learning_rate": 1.4702222222222223e-07,
      "loss": 1.4528,
      "step": 110900
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 5.179041385650635,
      "learning_rate": 1.3813333333333335e-07,
      "loss": 1.4228,
      "step": 111000
    },
    {
      "epoch": 4.937777777777778,
      "grad_norm": 7.6038384437561035,
      "learning_rate": 1.2924444444444447e-07,
      "loss": 1.4445,
      "step": 111100
    },
    {
      "epoch": 4.942222222222222,
      "grad_norm": 6.782458305358887,
      "learning_rate": 1.2035555555555557e-07,
      "loss": 1.4488,
      "step": 111200
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 7.120882987976074,
      "learning_rate": 1.1146666666666667e-07,
      "loss": 1.4537,
      "step": 111300
    },
    {
      "epoch": 4.9511111111111115,
      "grad_norm": 5.992689609527588,
      "learning_rate": 1.0257777777777778e-07,
      "loss": 1.4725,
      "step": 111400
    },
    {
      "epoch": 4.955555555555556,
      "grad_norm": 6.698713779449463,
      "learning_rate": 9.368888888888889e-08,
      "loss": 1.4354,
      "step": 111500
    },
    {
      "epoch": 4.96,
      "grad_norm": 6.13614559173584,
      "learning_rate": 8.48e-08,
      "loss": 1.4497,
      "step": 111600
    },
    {
      "epoch": 4.964444444444444,
      "grad_norm": 5.799744606018066,
      "learning_rate": 7.591111111111111e-08,
      "loss": 1.4338,
      "step": 111700
    },
    {
      "epoch": 4.968888888888889,
      "grad_norm": 5.525567054748535,
      "learning_rate": 6.702222222222223e-08,
      "loss": 1.4809,
      "step": 111800
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 5.766046524047852,
      "learning_rate": 5.813333333333334e-08,
      "loss": 1.4226,
      "step": 111900
    },
    {
      "epoch": 4.977777777777778,
      "grad_norm": 5.9698615074157715,
      "learning_rate": 4.924444444444445e-08,
      "loss": 1.4936,
      "step": 112000
    },
    {
      "epoch": 4.982222222222222,
      "grad_norm": 6.404995918273926,
      "learning_rate": 4.035555555555556e-08,
      "loss": 1.4399,
      "step": 112100
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 5.676665306091309,
      "learning_rate": 3.146666666666667e-08,
      "loss": 1.4686,
      "step": 112200
    },
    {
      "epoch": 4.9911111111111115,
      "grad_norm": 5.009476661682129,
      "learning_rate": 2.266666666666667e-08,
      "loss": 1.4616,
      "step": 112300
    },
    {
      "epoch": 4.995555555555556,
      "grad_norm": 6.2681756019592285,
      "learning_rate": 1.3777777777777778e-08,
      "loss": 1.4688,
      "step": 112400
    },
    {
      "epoch": 5.0,
      "grad_norm": 7.130458354949951,
      "learning_rate": 4.888888888888889e-09,
      "loss": 1.444,
      "step": 112500
    }
  ],
  "logging_steps": 100,
  "max_steps": 112500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.1845906845184e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
