{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3bef65f0-0384-47b9-8ce9-4fc49e898410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainerCallback, pipeline\n",
    "import smtplib\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afcdd6ec-c05d-41e2-a38f-7032f31b0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dca53f0f-bdd4-4950-9a19-147abcb771b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using GPU: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "094aa7b1-a965-407a-933a-41b05f4619f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "def keep_awake():\n",
    "    subprocess.call([\n",
    "        \"powershell.exe\",\n",
    "        \"-Command\",\n",
    "        \"Add-Type -TypeDefinition @'\\nusing System;\\nusing System.Runtime.InteropServices;\\npublic class SleepPreventer {\\n    [DllImport(\\\"kernel32.dll\\\")]\\n    public static extern uint SetThreadExecutionState(uint esFlags);\\n}\\n'@\\n[SleepPreventer]::SetThreadExecutionState(0x80000002 -bor 0x00000001)\"\n",
    "    ])\n",
    "    print(\"Laptop is set to stay awake during training...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecd5f6-b082-4fe7-9385-ffacd0fd8744",
   "metadata": {},
   "source": [
    "import smtplib\n",
    "\n",
    "def send_email_notification():\n",
    "    sender_email = \"sadhvi.chandragiri@outlook.com\"  # Replace with your Outlook email\n",
    "    recipient_email = \"recipient_email@example.com\"  # Replace with the recipient's email\n",
    "    password = \"your_outlook_password\"  # Replace with your Outlook email password\n",
    "\n",
    "    subject = \"Training Complete\"\n",
    "    body = \"Your model training has completed successfully!\"\n",
    "\n",
    "    email_message = f\"Subject: {subject}\\n\\n{body}\"\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP(\"smtp.office365.com\", 587) as server:\n",
    "            server.starttls()  # Secure the connection\n",
    "            server.login(sender_email, password)  # Login to the SMTP server\n",
    "            server.sendmail(sender_email, recipient_email, email_message)\n",
    "            print(\"Email notification sent!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7944703c-7927-4deb-bef7-ae56e997c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log GPU usage\n",
    "def log_gpu_usage():\n",
    "    with open(\"gpu_usage_log.txt\", \"a\") as f:\n",
    "        while True:\n",
    "            gpu_status = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "            f.write(gpu_status)\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "            time.sleep(300)  # Log every 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c15a01-6ff5-4ea2-9480-fecddbc6a607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f2cee34a-7643-4636-9b71-bafd2e981af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Logging Callback for Training\n",
    "#Add the custom callback for logging training progress:\n",
    "                       \n",
    "class CustomLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            with open(\"training_log.txt\", \"a\") as f:\n",
    "                f.write(f\"Step {state.global_step}: {logs}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87655d68-0603-43c5-a2a6-00e7dc04fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output paths\n",
    "num_epochs = 3  # Replace with your actual epoch count\n",
    "dataset_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac70a5-34fb-4947-9421-7896a05f463e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8cab2a1c-7d17-49ee-97b8-19823269c5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset and select first 10,000 rows\n",
    "data = pd.read_csv(\"../data/processed/recipeNLG_final.csv\").head(dataset_size)\n",
    "print(data.shape)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5e12dc0-d18b-4807-b425-e3ee4c9f0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu_metric = load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Prepare references as a list of lists\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    # Compute BLEU score\n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5bb1824-b297-44f1-8905-07898449cb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7624a4a7e149918a3aa7864a66db4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add padding token to tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [\n",
    "        ingredient + \" \" + direction\n",
    "        for ingredient, direction in zip(examples[\"ingredients\"], examples[\"directions\"])\n",
    "    ]\n",
    "    return tokenizer(combined_texts, truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05fcd605-8213-4de8-b631-16b415dd46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset into train and eval sets\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)  # 10% for evaluation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "#eval_dataset = split_dataset[\"test\"]\n",
    "# Get the dataset size dynamically\n",
    "#dataset_size = len(eval_dataset)\n",
    "#eval_dataset = eval_dataset.select(range(min(500, dataset_size)))  # Select up to 2000 rows, or all rows if smaller\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b3c97c0-5845-4b58-90ce-5e10a4b8e7e4",
   "metadata": {},
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results/50k_epoch_{num_epochs}\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # Smaller training batch\n",
    "    #per_device_eval_batch_size=1,  # Smaller evaluation batch\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = per_device_train_batch_size * 4\n",
    "    num_train_epochs=num_epochs,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"../logs\",\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d95e7c-12fb-45d2-b5ed-e9e1641036a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results/50k_epoch_{num_epochs}_{learning_rate}\",  # Save directory\n",
    "    eval_strategy=\"no\",  # No validation during training\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of every epoch\n",
    "    learning_rate=1e-5,  # Lower learning rate for stability\n",
    "    per_device_train_batch_size=2,  # Increase batch size if possible\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = batch_size * steps\n",
    "    num_train_epochs=5,  # Increase epochs to ensure sufficient training\n",
    "    save_total_limit=2,  # Keep only the 2 latest checkpoints\n",
    "    logging_dir=\"../logs\",  # Logging directory\n",
    "    load_best_model_at_end=False,  # No validation, so no best model selection\n",
    "    fp16=torch.cuda.is_available(),  # Enable FP16 for faster training if GPU is available\n",
    "    report_to=\"none\"  # Avoid unnecessary logs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "383971f5-a5cd-4add-be82-fe9fe585710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to False for causal language modeling like GPT-2\n",
    ")\n",
    "\n",
    "# Update Trainer to use data_collator instead of tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,  # No evaluation for Fix 1\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,  # Use data collator instead of tokenizer\n",
    "    callbacks=[CustomLoggingCallback()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1fb01dc7-c854-4a28-a936-e22552b23e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Your script code here\n",
    "time.sleep(5)  # Simulating script execution with a delay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3f06cb5e-adb6-4eef-9243-fcf1fa8eb46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot convert argument \"esFlags\", with value: \"-2147483645\", for \"SetThreadExecutionState\" to type \"System.UInt32\": \n",
      "\"Cannot convert value \"-2147483645\" to type \"System.UInt32\". Error: \"Value was either too large or too small for a \n",
      "UInt32.\"\"\n",
      "At line:9 char:1\n",
      "+ [SleepPreventer]::SetThreadExecutionState(0x80000002 -bor 0x00000001)\n",
      "+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    + CategoryInfo          : NotSpecified: (:) [], MethodException\n",
      "    + FullyQualifiedErrorId : MethodArgumentConversionInvalidCastArgument\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laptop is set to stay awake during training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33750' max='33750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33750/33750 2:40:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.476300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>1.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.383300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start GPU monitoring (optional)\n",
    "gpu_logger = threading.Thread(target=log_gpu_usage)\n",
    "gpu_logger.start()\n",
    "\n",
    "# Start training\n",
    "keep_awake()  # Ensure the laptop doesn't sleep\n",
    "\n",
    "# Clear GPU cache before evaluation\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()  # Free memory before evaluation\n",
    "#trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9def3d-84e2-4747-9656-d4ba4b8e713e",
   "metadata": {},
   "source": [
    "\n",
    "# Notify when training completes\n",
    "send_email_notification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "81d9c820-90b0-4e9c-a840-fcb37e896500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/fine_tuned_gpt2/50000k_epoch_3/tokenizer_config.json',\n",
       " '../model/fine_tuned_gpt2/50000k_epoch_3/special_tokens_map.json',\n",
       " '../model/fine_tuned_gpt2/50000k_epoch_3/vocab.json',\n",
       " '../model/fine_tuned_gpt2/50000k_epoch_3/merges.txt',\n",
       " '../model/fine_tuned_gpt2/50000k_epoch_3/added_tokens.json')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(f\"../model/fine_tuned_gpt2/{dataset_size}k_epoch_{num_epochs}\")  # Save the model\n",
    "tokenizer.save_pretrained(f\"../model/fine_tuned_gpt2/{dataset_size}k_epoch_{num_epochs}\")  # Save the tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fadf2859-4d5b-473d-8b6a-c66f992716e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2024-12-14 23:34:28\n",
      "Training ended at: 2024-12-15 02:15:01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# End time\n",
    "end_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"Training started at: {start_time}\")\n",
    "print(f\"Training ended at: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec208a42-7c8d-49ed-a63b-c6b2bebefd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "67fde26c-5729-45cd-8885-02eab7107a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the function to save the generated recipe\n",
    "def write_generated_recipes_to_csv(output_file, recipes):\n",
    "    \"\"\"\n",
    "    Save a list of recipes to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(recipes)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Recipes saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "77ef8a0f-e5a6-4d98-a4aa-91b1d2f4fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the recipe generator\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_recipes_batch(dataset, generator, max_length=200):\n",
    "    \"\"\"\n",
    "    Generate recipes in batch for all parameter combinations.\n",
    "    \"\"\"\n",
    "    recipes = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        # Extract parameters and input ingredients\n",
    "        input_ingredients = row[\"input_ingredients\"]\n",
    "        top_k = row[\"top_k\"]\n",
    "        top_p = row[\"top_p\"]\n",
    "        temperature = row[\"temperature\"]\n",
    "\n",
    "        # Generate a recipe with the current parameters\n",
    "        recipe = generator(\n",
    "            input_ingredients,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Append the recipe and parameters to the results\n",
    "        recipes.append({\n",
    "            \"input_ingredients\": input_ingredients,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "            \"recipe\": recipe\n",
    "        })\n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c262c044-7408-427e-842e-fe1004e4147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recipes in batch...\n",
      "Recipes saved to ../results/recipes/50000K_3_batch_decoding_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input ingredients\n",
    "    input_ingredients = \"chicken, garlic, onion, spices\"\n",
    "\n",
    "    # Define parameter ranges\n",
    "    top_k_values = [30, 50, 100]\n",
    "    top_p_values = [0.8, 0.9, 0.95]\n",
    "    temperature_values = [0.7, 0.8, 1.0]\n",
    "\n",
    "    # Create all combinations of parameters\n",
    "    parameter_combinations = list(itertools.product(top_k_values, top_p_values, temperature_values))\n",
    "\n",
    "    # Prepare the dataset as a DataFrame\n",
    "    dataset = pd.DataFrame(parameter_combinations, columns=[\"top_k\", \"top_p\", \"temperature\"])\n",
    "    dataset[\"input_ingredients\"] = input_ingredients\n",
    "\n",
    "    # Output file\n",
    "    output_dir = \"../results/recipes\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = f\"{output_dir}/{dataset_size}K_{num_epochs}_batch_decoding_results.csv\"\n",
    "\n",
    "    # Generate recipes in batch\n",
    "    print(\"Generating recipes in batch...\")\n",
    "    results = generate_recipes_batch(dataset, generator)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    write_generated_recipes_to_csv(output_file, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddae42-a411-4f09-a41f-58254ee4f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define parameter ranges\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "\n",
    "# Generate recipe\n",
    "input_prompt = \"Chicken curry with garlic and spices, Indian cuisine, baked\"\n",
    "generated_recipe = generator(\n",
    "    input_prompt,\n",
    "    max_length=150,  # Maximum length of output\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    num_return_sequences=1  # Single output\n",
    ")\n",
    "\n",
    "print(\"Generated Recipe:\", generated_recipe[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261abe8-7a9e-4b43-9930-f52bb09e0c80",
   "metadata": {},
   "source": [
    "input_ingredients = \"Ingredients: tofu, blueberries, thyme. Instructions:\"\n",
    "generated_recipe = generator(\n",
    "    input_ingredients,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"Generated Recipe:\\n\", generated_recipe[0][\"generated_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608ed08-a94c-43cc-927f-a1b92c2a668a",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Inspect tokenized output\n",
    "tokenized_sample = tokenizer(\"chicken, garlic, onion, spices\", return_tensors=\"pt\")\n",
    "print(tokenized_sample)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_sample[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539666c2-3c41-4465-ad96-e1c72c889a65",
   "metadata": {},
   "source": [
    "for param in model.transformer.wte.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.wpe.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75139288-0372-439e-a3ec-9607b116d405",
   "metadata": {},
   "source": [
    "# Tokenize a sample of your dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load your dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/processed/recipeNLG_final.csv\").head(1000)  # Load a subset\n",
    "sample_texts = data[\"text_column\"]  # Replace 'text_column' with your dataset's text column\n",
    "\n",
    "# Tokenize and analyze splits\n",
    "for text in sample_texts[:10]:  # Check the first 10 samples\n",
    "    tokenized_output = tokenizer(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_output[\"input_ids\"])\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c6839c7-8171-4a73-afab-fce0fa7edacd",
   "metadata": {},
   "source": [
    "Step 2: Look for Problematic Splits\n",
    "\n",
    "Identify:\n",
    "\n",
    "    Words that are unnecessarily split (e.g., [\"ch\", \"icken\"] for \"chicken\").\n",
    "    Words that appear frequently but are represented by multiple tokens.\n",
    "\n",
    "Step 3: Generate a Frequency List\n",
    "\n",
    "Count the occurrences of tokens in the dataset to identify frequently used subwords.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "all_tokens = []\n",
    "for text in sample_texts:\n",
    "    tokenized_output = tokenizer(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_output[\"input_ids\"])\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Count token frequencies\n",
    "token_counts = Counter(all_tokens)\n",
    "print(token_counts.most_common(50))  # Check the 50 most common tokens\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41dc744c-66fc-4f18-8a0c-779889c55e3b",
   "metadata": {},
   "source": [
    "Options to Handle This\n",
    "1. Stick with eval_strategy=\"no\"\n",
    "\n",
    "    If memory constraints are critical, skip the evaluation during training (eval_strategy=\"no\").\n",
    "    Alternative: Manually evaluate the model after training is complete by loading the validation dataset separately and running predictions.\n",
    "\n",
    "2. Reduce Memory Usage\n",
    "\n",
    "If you want to use eval_strategy, try these optimizations:\n",
    "\n",
    "    Reduce Batch Size for Evaluation:\n",
    "        Use a smaller per_device_eval_batch_size in your TrainingArguments:\n",
    "\n",
    "    per_device_eval_batch_size=1\n",
    "\n",
    "    This minimizes memory usage during evaluation.\n",
    "\n",
    "Shorten Sequence Length:\n",
    "\n",
    "    Reduce max_length for both training and evaluation datasets:\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "    tokenized_dataset = tokenized_dataset.map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    )\n",
    "\n",
    "Enable Gradient Checkpointing:\n",
    "\n",
    "    Use gradient checkpointing to save memory during training and evaluation:\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "Use Mixed Precision:\n",
    "\n",
    "    Train and evaluate in half-precision (FP16) mode if your GPU supports it:\n",
    "\n",
    "        fp16=True\n",
    "\n",
    "Alternative: Evaluate Manually After Training\n",
    "\n",
    "If enabling eval_strategy isnâ€™t feasible, you can still evaluate manually after training:\n",
    "\n",
    "    Save the Trained Model:\n",
    "        Save the model after training completes:\n",
    "\n",
    "    model.save_pretrained(\"path/to/saved_model\")\n",
    "    tokenizer.save_pretrained(\"path/to/saved_model\")\n",
    "\n",
    "Load the Model for Evaluation:\n",
    "\n",
    "    Load the saved model and tokenizer:\n",
    "\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"path/to/saved_model\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"path/to/saved_model\")\n",
    "\n",
    "Run Evaluation Separately:\n",
    "\n",
    "    Use a validation dataset to calculate metrics or observe outputs:\n",
    "\n",
    "        for batch in validation_dataset:\n",
    "            inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            outputs = model.generate(**inputs)\n",
    "            print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "Next Steps\n",
    "\n",
    "    Run Without Evaluation:\n",
    "        Stick with eval_strategy=\"no\" for now to focus on completing training runs.\n",
    "    Evaluate After Training:\n",
    "        Save and load the model separately for evaluation.\n",
    "    Switch to DistilGPT-2:\n",
    "        If memory issues persist, consider switching to DistilGPT-2, which requires significantly less memory.\n",
    "\n",
    "Would you like help with setting up manual evaluation or switching to a lighter model like DistilGPT-2?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d6cc15c-0b76-4c8c-9f9d-4f8589836374",
   "metadata": {},
   "source": [
    "Step 3: Reload Saved Models for Evaluation\n",
    "\n",
    "You can reload a specific model for evaluation or inference:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../saved_models/50k_epoch_1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../saved_models/50k_epoch_1\")\n",
    "\n",
    "# Use the model for evaluation or generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textanalytics_env)",
   "language": "python",
   "name": "textanalytics_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
