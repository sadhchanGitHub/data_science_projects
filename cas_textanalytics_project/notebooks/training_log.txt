Step 500: {'loss': 2.0758, 'grad_norm': 23.83847427368164, 'learning_rate': 3.897777777777778e-05, 'epoch': 0.2222222222222222}
Step 1000: {'loss': 1.8915, 'grad_norm': 13.971112251281738, 'learning_rate': 2.786666666666667e-05, 'epoch': 0.4444444444444444}
Step 1500: {'loss': 1.8168, 'grad_norm': 15.595458984375, 'learning_rate': 1.6755555555555557e-05, 'epoch': 0.6666666666666666}
Step 2000: {'loss': 1.7902, 'grad_norm': 16.14358901977539, 'learning_rate': 5.6444444444444445e-06, 'epoch': 0.8888888888888888}
Step 2250: {'train_runtime': 809.2592, 'train_samples_per_second': 11.121, 'train_steps_per_second': 2.78, 'total_flos': 1726055867520000.0, 'train_loss': 1.8793237982855904, 'epoch': 1.0}
Step 500: {'loss': 1.7348, 'grad_norm': 15.827262878417969, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.2222222222222222}
Step 1000: {'loss': 1.7082, 'grad_norm': 10.327255249023438, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.4444444444444444}
Step 1500: {'loss': 1.6877, 'grad_norm': 14.330263137817383, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.6666666666666666}
Step 2000: {'loss': 1.688, 'grad_norm': 13.478740692138672, 'learning_rate': 5.555555555555556e-06, 'epoch': 0.8888888888888888}
Step 2250: {'train_runtime': 809.027, 'train_samples_per_second': 11.124, 'train_steps_per_second': 2.781, 'total_flos': 1726055867520000.0, 'train_loss': 1.7021818983289931, 'epoch': 1.0}
Step 500: {'loss': 2.0843, 'grad_norm': 22.410314559936523, 'learning_rate': 4.779555555555556e-05, 'epoch': 0.044444444444444446}
Step 1000: {'loss': 1.8825, 'grad_norm': 14.643261909484863, 'learning_rate': 4.557333333333333e-05, 'epoch': 0.08888888888888889}
Step 1500: {'loss': 1.8035, 'grad_norm': 12.185515403747559, 'learning_rate': 4.3351111111111115e-05, 'epoch': 0.13333333333333333}
Step 2000: {'loss': 1.7766, 'grad_norm': 11.354965209960938, 'learning_rate': 4.112888888888889e-05, 'epoch': 0.17777777777777778}
Step 2500: {'loss': 1.7558, 'grad_norm': 10.831315040588379, 'learning_rate': 3.8911111111111117e-05, 'epoch': 0.2222222222222222}
Step 3000: {'loss': 1.7323, 'grad_norm': 9.921269416809082, 'learning_rate': 3.668888888888889e-05, 'epoch': 0.26666666666666666}
Step 3500: {'loss': 1.7043, 'grad_norm': 8.727527618408203, 'learning_rate': 3.4466666666666666e-05, 'epoch': 0.3111111111111111}
Step 4000: {'loss': 1.667, 'grad_norm': 8.950416564941406, 'learning_rate': 3.224444444444444e-05, 'epoch': 0.35555555555555557}
Step 4500: {'loss': 1.6591, 'grad_norm': 8.648307800292969, 'learning_rate': 3.0022222222222223e-05, 'epoch': 0.4}
Step 5000: {'loss': 1.6319, 'grad_norm': 10.93738842010498, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.4444444444444444}
Step 5500: {'loss': 1.6423, 'grad_norm': 11.450265884399414, 'learning_rate': 2.557777777777778e-05, 'epoch': 0.4888888888888889}
Step 6000: {'loss': 1.6356, 'grad_norm': 8.6139554977417, 'learning_rate': 2.3355555555555555e-05, 'epoch': 0.5333333333333333}
Step 6500: {'loss': 1.6195, 'grad_norm': 7.514315605163574, 'learning_rate': 2.113777777777778e-05, 'epoch': 0.5777777777777777}
Step 7000: {'loss': 1.6048, 'grad_norm': 12.635519027709961, 'learning_rate': 1.8915555555555557e-05, 'epoch': 0.6222222222222222}
Step 7500: {'loss': 1.6191, 'grad_norm': 7.76188850402832, 'learning_rate': 1.669333333333333e-05, 'epoch': 0.6666666666666666}
Step 8000: {'loss': 1.5958, 'grad_norm': 13.735716819763184, 'learning_rate': 1.4471111111111113e-05, 'epoch': 0.7111111111111111}
Step 8500: {'loss': 1.6018, 'grad_norm': 9.118963241577148, 'learning_rate': 1.2253333333333333e-05, 'epoch': 0.7555555555555555}
Step 9000: {'loss': 1.5936, 'grad_norm': 9.018338203430176, 'learning_rate': 1.0031111111111112e-05, 'epoch': 0.8}
Step 9500: {'loss': 1.5759, 'grad_norm': 9.455402374267578, 'learning_rate': 7.808888888888888e-06, 'epoch': 0.8444444444444444}
Step 10000: {'loss': 1.5782, 'grad_norm': 10.697173118591309, 'learning_rate': 5.586666666666667e-06, 'epoch': 0.8888888888888888}
Step 10500: {'loss': 1.5758, 'grad_norm': 14.780938148498535, 'learning_rate': 3.3688888888888886e-06, 'epoch': 0.9333333333333333}
Step 11000: {'loss': 1.5575, 'grad_norm': 7.368407726287842, 'learning_rate': 1.1466666666666668e-06, 'epoch': 0.9777777777777777}
Step 11250: {'train_runtime': 3487.5888, 'train_samples_per_second': 12.903, 'train_steps_per_second': 3.226, 'total_flos': 8494000232832000.0, 'train_loss': 1.6748186116536459, 'epoch': 1.0}
Step 500: {'loss': 2.0844, 'grad_norm': 22.428438186645508, 'learning_rate': 4.779555555555556e-05, 'epoch': 0.044444444444444446}
Step 1000: {'loss': 1.8825, 'grad_norm': 14.63752269744873, 'learning_rate': 4.557333333333333e-05, 'epoch': 0.08888888888888889}
Step 1500: {'loss': 1.8036, 'grad_norm': 12.182947158813477, 'learning_rate': 4.3351111111111115e-05, 'epoch': 0.13333333333333333}
Step 2000: {'loss': 1.7766, 'grad_norm': 11.465450286865234, 'learning_rate': 4.112888888888889e-05, 'epoch': 0.17777777777777778}
Step 2500: {'loss': 1.7558, 'grad_norm': 10.822922706604004, 'learning_rate': 3.8911111111111117e-05, 'epoch': 0.2222222222222222}
Step 3000: {'loss': 1.7322, 'grad_norm': 9.933605194091797, 'learning_rate': 3.668888888888889e-05, 'epoch': 0.26666666666666666}
Step 3500: {'loss': 1.7043, 'grad_norm': 8.658955574035645, 'learning_rate': 3.4466666666666666e-05, 'epoch': 0.3111111111111111}
Step 4000: {'loss': 1.6669, 'grad_norm': 8.984619140625, 'learning_rate': 3.224444444444444e-05, 'epoch': 0.35555555555555557}
Step 4500: {'loss': 1.6589, 'grad_norm': 8.888479232788086, 'learning_rate': 3.0022222222222223e-05, 'epoch': 0.4}
Step 5000: {'loss': 1.632, 'grad_norm': 10.933759689331055, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.4444444444444444}
Step 5500: {'loss': 1.6422, 'grad_norm': 11.447604179382324, 'learning_rate': 2.557777777777778e-05, 'epoch': 0.4888888888888889}
Step 6000: {'loss': 1.6356, 'grad_norm': 8.613249778747559, 'learning_rate': 2.3355555555555555e-05, 'epoch': 0.5333333333333333}
Step 6500: {'loss': 1.6194, 'grad_norm': 7.497155666351318, 'learning_rate': 2.113777777777778e-05, 'epoch': 0.5777777777777777}
Step 7000: {'loss': 1.6047, 'grad_norm': 12.587228775024414, 'learning_rate': 1.8915555555555557e-05, 'epoch': 0.6222222222222222}
Step 7500: {'loss': 1.619, 'grad_norm': 7.760223865509033, 'learning_rate': 1.669333333333333e-05, 'epoch': 0.6666666666666666}
Step 8000: {'loss': 1.5958, 'grad_norm': 13.68042278289795, 'learning_rate': 1.4471111111111113e-05, 'epoch': 0.7111111111111111}
Step 8500: {'loss': 1.6016, 'grad_norm': 9.13276195526123, 'learning_rate': 1.2253333333333333e-05, 'epoch': 0.7555555555555555}
Step 9000: {'loss': 1.5936, 'grad_norm': 8.994640350341797, 'learning_rate': 1.0031111111111112e-05, 'epoch': 0.8}
Step 9500: {'loss': 1.5758, 'grad_norm': 9.44002628326416, 'learning_rate': 7.808888888888888e-06, 'epoch': 0.8444444444444444}
Step 10000: {'loss': 1.5781, 'grad_norm': 10.629926681518555, 'learning_rate': 5.586666666666667e-06, 'epoch': 0.8888888888888888}
Step 10500: {'loss': 1.5759, 'grad_norm': 14.763631820678711, 'learning_rate': 3.3688888888888886e-06, 'epoch': 0.9333333333333333}
Step 11000: {'loss': 1.5575, 'grad_norm': 7.366488933563232, 'learning_rate': 1.1466666666666668e-06, 'epoch': 0.9777777777777777}
Step 11250: {'train_runtime': 6139.4738, 'train_samples_per_second': 7.33, 'train_steps_per_second': 1.832, 'total_flos': 8494000232832000.0, 'train_loss': 1.674780615234375, 'epoch': 1.0}
Step 500: {'loss': 2.0843, 'grad_norm': 22.185319900512695, 'learning_rate': 4.926518518518518e-05, 'epoch': 0.044444444444444446}
Step 1000: {'loss': 1.8824, 'grad_norm': 14.324646949768066, 'learning_rate': 4.852444444444444e-05, 'epoch': 0.08888888888888889}
Step 1500: {'loss': 1.8029, 'grad_norm': 11.69240951538086, 'learning_rate': 4.7783703703703706e-05, 'epoch': 0.13333333333333333}
Step 2000: {'loss': 1.7759, 'grad_norm': 11.071653366088867, 'learning_rate': 4.7042962962962965e-05, 'epoch': 0.17777777777777778}
Step 2500: {'loss': 1.7542, 'grad_norm': 10.418721199035645, 'learning_rate': 4.630370370370371e-05, 'epoch': 0.2222222222222222}
Step 3000: {'loss': 1.7301, 'grad_norm': 9.386791229248047, 'learning_rate': 4.5562962962962965e-05, 'epoch': 0.26666666666666666}
Step 3500: {'loss': 1.7016, 'grad_norm': 8.378546714782715, 'learning_rate': 4.4822222222222224e-05, 'epoch': 0.3111111111111111}
Step 4000: {'loss': 1.6638, 'grad_norm': 8.472025871276855, 'learning_rate': 4.408148148148148e-05, 'epoch': 0.35555555555555557}
Step 4500: {'loss': 1.6544, 'grad_norm': 8.17807674407959, 'learning_rate': 4.334074074074074e-05, 'epoch': 0.4}
Step 5000: {'loss': 1.6272, 'grad_norm': 10.168551445007324, 'learning_rate': 4.26e-05, 'epoch': 0.4444444444444444}
Step 5500: {'loss': 1.6364, 'grad_norm': 10.430829048156738, 'learning_rate': 4.185925925925926e-05, 'epoch': 0.4888888888888889}
Step 6000: {'loss': 1.6286, 'grad_norm': 7.708828449249268, 'learning_rate': 4.111851851851852e-05, 'epoch': 0.5333333333333333}
Step 6500: {'loss': 1.6117, 'grad_norm': 6.814898490905762, 'learning_rate': 4.037777777777778e-05, 'epoch': 0.5777777777777777}
Step 7000: {'loss': 1.5962, 'grad_norm': 11.544855117797852, 'learning_rate': 3.9638518518518516e-05, 'epoch': 0.6222222222222222}
Step 7500: {'loss': 1.6089, 'grad_norm': 6.943668842315674, 'learning_rate': 3.8897777777777774e-05, 'epoch': 0.6666666666666666}
Step 8000: {'loss': 1.5845, 'grad_norm': 12.458205223083496, 'learning_rate': 3.815703703703704e-05, 'epoch': 0.7111111111111111}
Step 8500: {'loss': 1.5887, 'grad_norm': 8.125927925109863, 'learning_rate': 3.74162962962963e-05, 'epoch': 0.7555555555555555}
Step 9000: {'loss': 1.5795, 'grad_norm': 8.558561325073242, 'learning_rate': 3.667703703703704e-05, 'epoch': 0.8}
Step 9500: {'loss': 1.5585, 'grad_norm': 8.247987747192383, 'learning_rate': 3.59362962962963e-05, 'epoch': 0.8444444444444444}
Step 10000: {'loss': 1.559, 'grad_norm': 8.908944129943848, 'learning_rate': 3.519555555555556e-05, 'epoch': 0.8888888888888888}
Step 10500: {'loss': 1.5548, 'grad_norm': 12.393251419067383, 'learning_rate': 3.4454814814814815e-05, 'epoch': 0.9333333333333333}
Step 11000: {'loss': 1.5325, 'grad_norm': 6.642855644226074, 'learning_rate': 3.371407407407407e-05, 'epoch': 0.9777777777777777}
Step 11500: {'loss': 1.5186, 'grad_norm': 11.559700965881348, 'learning_rate': 3.2974814814814816e-05, 'epoch': 1.0222222222222221}
Step 12000: {'loss': 1.5112, 'grad_norm': 8.218401908874512, 'learning_rate': 3.223407407407408e-05, 'epoch': 1.0666666666666667}
Step 12500: {'loss': 1.4994, 'grad_norm': 8.74992847442627, 'learning_rate': 3.149333333333334e-05, 'epoch': 1.1111111111111112}
Step 13000: {'loss': 1.4948, 'grad_norm': 7.742073059082031, 'learning_rate': 3.07525925925926e-05, 'epoch': 1.1555555555555554}
Step 13500: {'loss': 1.483, 'grad_norm': 8.690921783447266, 'learning_rate': 3.0013333333333333e-05, 'epoch': 1.2}
Step 14000: {'loss': 1.4765, 'grad_norm': 9.457795143127441, 'learning_rate': 2.9272592592592595e-05, 'epoch': 1.2444444444444445}
Step 14500: {'loss': 1.4762, 'grad_norm': 8.01525592803955, 'learning_rate': 2.8531851851851853e-05, 'epoch': 1.2888888888888888}
Step 15000: {'loss': 1.4883, 'grad_norm': 8.013822555541992, 'learning_rate': 2.779111111111111e-05, 'epoch': 1.3333333333333333}
Step 15500: {'loss': 1.4766, 'grad_norm': 8.51690673828125, 'learning_rate': 2.705037037037037e-05, 'epoch': 1.3777777777777778}
Step 16000: {'loss': 1.4624, 'grad_norm': 10.140066146850586, 'learning_rate': 2.6311111111111115e-05, 'epoch': 1.4222222222222223}
Step 16500: {'loss': 1.4924, 'grad_norm': 7.549137115478516, 'learning_rate': 2.5570370370370374e-05, 'epoch': 1.4666666666666668}
Step 17000: {'loss': 1.4777, 'grad_norm': 6.956478118896484, 'learning_rate': 2.4829629629629632e-05, 'epoch': 1.511111111111111}
Step 17500: {'loss': 1.468, 'grad_norm': 9.183493614196777, 'learning_rate': 2.408888888888889e-05, 'epoch': 1.5555555555555556}
Step 18000: {'loss': 1.4583, 'grad_norm': inf, 'learning_rate': 2.334962962962963e-05, 'epoch': 1.6}
Step 18500: {'loss': 1.4619, 'grad_norm': 9.846075057983398, 'learning_rate': 2.260888888888889e-05, 'epoch': 1.6444444444444444}
Step 19000: {'loss': 1.4763, 'grad_norm': 7.5384368896484375, 'learning_rate': 2.186814814814815e-05, 'epoch': 1.6888888888888889}
Step 19500: {'loss': 1.4609, 'grad_norm': 9.080018997192383, 'learning_rate': 2.112888888888889e-05, 'epoch': 1.7333333333333334}
Step 20000: {'loss': 1.4609, 'grad_norm': 11.327052116394043, 'learning_rate': 2.0388148148148146e-05, 'epoch': 1.7777777777777777}
Step 20500: {'loss': 1.4596, 'grad_norm': 7.296541690826416, 'learning_rate': 1.9647407407407408e-05, 'epoch': 1.8222222222222222}
Step 21000: {'loss': 1.4626, 'grad_norm': 9.469728469848633, 'learning_rate': 1.8906666666666666e-05, 'epoch': 1.8666666666666667}
Step 21500: {'loss': 1.4433, 'grad_norm': 8.128583908081055, 'learning_rate': 1.8165925925925928e-05, 'epoch': 1.911111111111111}
Step 22000: {'loss': 1.4681, 'grad_norm': 8.093542098999023, 'learning_rate': 1.7425185185185186e-05, 'epoch': 1.9555555555555557}
Step 22500: {'loss': 1.4517, 'grad_norm': 7.869649887084961, 'learning_rate': 1.6684444444444445e-05, 'epoch': 2.0}
Step 23000: {'loss': 1.4037, 'grad_norm': 10.255434036254883, 'learning_rate': 1.5943703703703706e-05, 'epoch': 2.0444444444444443}
Step 23500: {'loss': 1.4195, 'grad_norm': 10.087589263916016, 'learning_rate': 1.5204444444444445e-05, 'epoch': 2.088888888888889}
Step 24000: {'loss': 1.4011, 'grad_norm': 10.717789649963379, 'learning_rate': 1.4463703703703704e-05, 'epoch': 2.1333333333333333}
Step 24500: {'loss': 1.4209, 'grad_norm': 8.290473937988281, 'learning_rate': 1.3722962962962962e-05, 'epoch': 2.1777777777777776}
Step 25000: {'loss': 1.3997, 'grad_norm': 8.659655570983887, 'learning_rate': 1.2982222222222224e-05, 'epoch': 2.2222222222222223}
Step 25500: {'loss': 1.4202, 'grad_norm': 7.464287281036377, 'learning_rate': 1.2241481481481482e-05, 'epoch': 2.2666666666666666}
Step 26000: {'loss': 1.4058, 'grad_norm': 10.73477554321289, 'learning_rate': 1.1502222222222223e-05, 'epoch': 2.311111111111111}
Step 26500: {'loss': 1.401, 'grad_norm': 9.128948211669922, 'learning_rate': 1.0761481481481483e-05, 'epoch': 2.3555555555555556}
Step 27000: {'loss': 1.4053, 'grad_norm': 9.344803810119629, 'learning_rate': 1.0020740740740741e-05, 'epoch': 2.4}
Step 27500: {'loss': 1.4026, 'grad_norm': 8.68384075164795, 'learning_rate': 9.28e-06, 'epoch': 2.4444444444444446}
Step 28000: {'loss': 1.4086, 'grad_norm': 8.761444091796875, 'learning_rate': 8.53925925925926e-06, 'epoch': 2.488888888888889}
Step 28500: {'loss': 1.4003, 'grad_norm': 8.938729286193848, 'learning_rate': 7.8e-06, 'epoch': 2.533333333333333}
Step 29000: {'loss': 1.3989, 'grad_norm': 10.211456298828125, 'learning_rate': 7.05925925925926e-06, 'epoch': 2.5777777777777775}
Step 29500: {'loss': 1.4052, 'grad_norm': 8.243492126464844, 'learning_rate': 6.318518518518519e-06, 'epoch': 2.6222222222222222}
Step 30000: {'loss': 1.4131, 'grad_norm': 11.806243896484375, 'learning_rate': 5.577777777777778e-06, 'epoch': 2.6666666666666665}
Step 30500: {'loss': 1.3981, 'grad_norm': 9.271256446838379, 'learning_rate': 4.838518518518519e-06, 'epoch': 2.7111111111111112}
Step 31000: {'loss': 1.416, 'grad_norm': 10.168973922729492, 'learning_rate': 4.097777777777778e-06, 'epoch': 2.7555555555555555}
Step 31500: {'loss': 1.4053, 'grad_norm': 9.358492851257324, 'learning_rate': 3.357037037037037e-06, 'epoch': 2.8}
Step 32000: {'loss': 1.3999, 'grad_norm': 8.237030029296875, 'learning_rate': 2.6162962962962967e-06, 'epoch': 2.8444444444444446}
Step 32500: {'loss': 1.3945, 'grad_norm': 8.614872932434082, 'learning_rate': 1.8770370370370371e-06, 'epoch': 2.888888888888889}
Step 33000: {'loss': 1.3956, 'grad_norm': 8.053933143615723, 'learning_rate': 1.1362962962962965e-06, 'epoch': 2.9333333333333336}
Step 33500: {'loss': 1.3833, 'grad_norm': 8.207995414733887, 'learning_rate': 3.955555555555556e-07, 'epoch': 2.977777777777778}
Step 33750: {'train_runtime': 9625.3114, 'train_samples_per_second': 14.026, 'train_steps_per_second': 3.506, 'total_flos': 2.5482000698496e+16, 'train_loss': 1.5148263192635996, 'epoch': 3.0}
Step 500: {'loss': 2.2025, 'grad_norm': 21.229164123535156, 'learning_rate': 9.911822222222223e-06, 'epoch': 0.044444444444444446}
Step 1000: {'loss': 2.0273, 'grad_norm': 19.486173629760742, 'learning_rate': 9.822933333333334e-06, 'epoch': 0.08888888888888889}
Step 1500: {'loss': 1.9472, 'grad_norm': 18.38562774658203, 'learning_rate': 9.734044444444444e-06, 'epoch': 0.13333333333333333}
Step 2000: {'loss': 1.9213, 'grad_norm': 13.413763046264648, 'learning_rate': 9.645155555555556e-06, 'epoch': 0.17777777777777778}
Step 2500: {'loss': 1.8818, 'grad_norm': 17.207075119018555, 'learning_rate': 9.556266666666667e-06, 'epoch': 0.2222222222222222}
Step 3000: {'loss': 1.8502, 'grad_norm': 15.64237117767334, 'learning_rate': 9.467555555555557e-06, 'epoch': 0.26666666666666666}
Step 3500: {'loss': 1.8296, 'grad_norm': 16.456501007080078, 'learning_rate': 9.378666666666667e-06, 'epoch': 0.3111111111111111}
Step 4000: {'loss': 1.8223, 'grad_norm': 16.77545166015625, 'learning_rate': 9.28977777777778e-06, 'epoch': 0.35555555555555557}
Step 4500: {'loss': 1.8023, 'grad_norm': 18.82173728942871, 'learning_rate': 9.20088888888889e-06, 'epoch': 0.4}
Step 5000: {'loss': 1.7979, 'grad_norm': 11.586014747619629, 'learning_rate': 9.112e-06, 'epoch': 0.4444444444444444}
Step 5500: {'loss': 1.7798, 'grad_norm': 11.529667854309082, 'learning_rate': 9.02311111111111e-06, 'epoch': 0.4888888888888889}
Step 6000: {'loss': 1.767, 'grad_norm': 11.739168167114258, 'learning_rate': 8.934222222222223e-06, 'epoch': 0.5333333333333333}
Step 6500: {'loss': 1.7725, 'grad_norm': 12.549696922302246, 'learning_rate': 8.845333333333333e-06, 'epoch': 0.5777777777777777}
Step 7000: {'loss': 1.7567, 'grad_norm': 11.809123039245605, 'learning_rate': 8.756622222222223e-06, 'epoch': 0.6222222222222222}
Step 7500: {'loss': 1.7481, 'grad_norm': 10.575901985168457, 'learning_rate': 8.667733333333333e-06, 'epoch': 0.6666666666666666}
Step 8000: {'loss': 1.7334, 'grad_norm': 11.106318473815918, 'learning_rate': 8.578844444444446e-06, 'epoch': 0.7111111111111111}
Step 8500: {'loss': 1.742, 'grad_norm': 9.542976379394531, 'learning_rate': 8.489955555555556e-06, 'epoch': 0.7555555555555555}
Step 9000: {'loss': 1.7498, 'grad_norm': 9.84304428100586, 'learning_rate': 8.401244444444446e-06, 'epoch': 0.8}
Step 9500: {'loss': 1.7105, 'grad_norm': 9.920923233032227, 'learning_rate': 8.312355555555556e-06, 'epoch': 0.8444444444444444}
Step 10000: {'loss': 1.7236, 'grad_norm': 9.925490379333496, 'learning_rate': 8.223466666666667e-06, 'epoch': 0.8888888888888888}
Step 10500: {'loss': 1.7059, 'grad_norm': 7.947761535644531, 'learning_rate': 8.134577777777777e-06, 'epoch': 0.9333333333333333}
Step 11000: {'loss': 1.6924, 'grad_norm': 9.833102226257324, 'learning_rate': 8.045866666666667e-06, 'epoch': 0.9777777777777777}
Step 11500: {'loss': 1.704, 'grad_norm': 9.690666198730469, 'learning_rate': 7.956977777777777e-06, 'epoch': 1.0222222222222221}
Step 12000: {'loss': 1.6625, 'grad_norm': 11.794687271118164, 'learning_rate': 7.86808888888889e-06, 'epoch': 1.0666666666666667}
Step 12500: {'loss': 1.6582, 'grad_norm': 11.947836875915527, 'learning_rate': 7.7792e-06, 'epoch': 1.1111111111111112}
Step 13000: {'loss': 1.6504, 'grad_norm': 10.273759841918945, 'learning_rate': 7.69048888888889e-06, 'epoch': 1.1555555555555554}
Step 13500: {'loss': 1.6499, 'grad_norm': 8.157757759094238, 'learning_rate': 7.6016e-06, 'epoch': 1.2}
Step 14000: {'loss': 1.6559, 'grad_norm': 6.891366958618164, 'learning_rate': 7.5127111111111115e-06, 'epoch': 1.2444444444444445}
Step 14500: {'loss': 1.6738, 'grad_norm': 8.408618927001953, 'learning_rate': 7.423822222222224e-06, 'epoch': 1.2888888888888888}
Step 15000: {'loss': 1.652, 'grad_norm': 11.26991081237793, 'learning_rate': 7.335111111111112e-06, 'epoch': 1.3333333333333333}
Step 15500: {'loss': 1.6543, 'grad_norm': 8.261808395385742, 'learning_rate': 7.246222222222222e-06, 'epoch': 1.3777777777777778}
Step 16000: {'loss': 1.6409, 'grad_norm': 8.286911964416504, 'learning_rate': 7.157333333333334e-06, 'epoch': 1.4222222222222223}
Step 16500: {'loss': 1.6512, 'grad_norm': 8.122204780578613, 'learning_rate': 7.068444444444446e-06, 'epoch': 1.4666666666666668}
Step 17000: {'loss': 1.6547, 'grad_norm': 8.405709266662598, 'learning_rate': 6.979733333333334e-06, 'epoch': 1.511111111111111}
Step 17500: {'loss': 1.6502, 'grad_norm': 9.784050941467285, 'learning_rate': 6.890844444444446e-06, 'epoch': 1.5555555555555556}
Step 18000: {'loss': 1.6363, 'grad_norm': 9.391106605529785, 'learning_rate': 6.801955555555556e-06, 'epoch': 1.6}
Step 18500: {'loss': 1.6482, 'grad_norm': 8.497251510620117, 'learning_rate': 6.713066666666668e-06, 'epoch': 1.6444444444444444}
Step 19000: {'loss': 1.6251, 'grad_norm': 10.097593307495117, 'learning_rate': 6.624177777777778e-06, 'epoch': 1.6888888888888889}
Step 19500: {'loss': 1.6259, 'grad_norm': 11.387328147888184, 'learning_rate': 6.535466666666668e-06, 'epoch': 1.7333333333333334}
Step 20000: {'loss': 1.6305, 'grad_norm': 9.60416030883789, 'learning_rate': 6.446577777777778e-06, 'epoch': 1.7777777777777777}
Step 20500: {'loss': 1.6204, 'grad_norm': 7.186903476715088, 'learning_rate': 6.3576888888888896e-06, 'epoch': 1.8222222222222222}
Step 21000: {'loss': 1.6039, 'grad_norm': 7.939542770385742, 'learning_rate': 6.2688e-06, 'epoch': 1.8666666666666667}
Step 21500: {'loss': 1.6217, 'grad_norm': 9.203922271728516, 'learning_rate': 6.18008888888889e-06, 'epoch': 1.911111111111111}
Step 22000: {'loss': 1.6124, 'grad_norm': 12.40890884399414, 'learning_rate': 6.0912e-06, 'epoch': 1.9555555555555557}
Step 22500: {'loss': 1.6115, 'grad_norm': 7.900426387786865, 'learning_rate': 6.002311111111112e-06, 'epoch': 2.0}
Step 23000: {'loss': 1.5936, 'grad_norm': 8.006888389587402, 'learning_rate': 5.913422222222223e-06, 'epoch': 2.0444444444444443}
Step 23500: {'loss': 1.5997, 'grad_norm': 7.2179155349731445, 'learning_rate': 5.824533333333334e-06, 'epoch': 2.088888888888889}
Step 24000: {'loss': 1.6104, 'grad_norm': 9.299473762512207, 'learning_rate': 5.735822222222223e-06, 'epoch': 2.1333333333333333}
Step 24500: {'loss': 1.5809, 'grad_norm': 10.118560791015625, 'learning_rate': 5.646933333333334e-06, 'epoch': 2.1777777777777776}
Step 25000: {'loss': 1.5868, 'grad_norm': 9.942992210388184, 'learning_rate': 5.558044444444445e-06, 'epoch': 2.2222222222222223}
Step 25500: {'loss': 1.5939, 'grad_norm': 10.11740779876709, 'learning_rate': 5.469155555555556e-06, 'epoch': 2.2666666666666666}
Step 26000: {'loss': 1.5814, 'grad_norm': 8.359234809875488, 'learning_rate': 5.3802666666666666e-06, 'epoch': 2.311111111111111}
Step 26500: {'loss': 1.5887, 'grad_norm': 6.906927108764648, 'learning_rate': 5.291555555555556e-06, 'epoch': 2.3555555555555556}
Step 27000: {'loss': 1.5875, 'grad_norm': 7.50798225402832, 'learning_rate': 5.202666666666667e-06, 'epoch': 2.4}
Step 27500: {'loss': 1.5833, 'grad_norm': 7.29223108291626, 'learning_rate': 5.113777777777779e-06, 'epoch': 2.4444444444444446}
Step 28000: {'loss': 1.5884, 'grad_norm': 6.21547794342041, 'learning_rate': 5.024888888888889e-06, 'epoch': 2.488888888888889}
Step 28500: {'loss': 1.5716, 'grad_norm': 6.435062408447266, 'learning_rate': 4.936177777777778e-06, 'epoch': 2.533333333333333}
Step 29000: {'loss': 1.5932, 'grad_norm': 7.407968521118164, 'learning_rate': 4.847288888888889e-06, 'epoch': 2.5777777777777775}
Step 29500: {'loss': 1.589, 'grad_norm': 8.755285263061523, 'learning_rate': 4.7584e-06, 'epoch': 2.6222222222222222}
Step 30000: {'loss': 1.5757, 'grad_norm': 9.095144271850586, 'learning_rate': 4.669511111111111e-06, 'epoch': 2.6666666666666665}
Step 30500: {'loss': 1.5686, 'grad_norm': 8.661087989807129, 'learning_rate': 4.580622222222223e-06, 'epoch': 2.7111111111111112}
Step 31000: {'loss': 1.5706, 'grad_norm': 8.940404891967773, 'learning_rate': 4.491733333333333e-06, 'epoch': 2.7555555555555555}
Step 31500: {'loss': 1.5765, 'grad_norm': 7.035806179046631, 'learning_rate': 4.402844444444444e-06, 'epoch': 2.8}
Step 32000: {'loss': 1.5681, 'grad_norm': 7.381285667419434, 'learning_rate': 4.314133333333333e-06, 'epoch': 2.8444444444444446}
Step 32500: {'loss': 1.5637, 'grad_norm': 8.864272117614746, 'learning_rate': 4.225244444444445e-06, 'epoch': 2.888888888888889}
Step 33000: {'loss': 1.5891, 'grad_norm': 6.682323932647705, 'learning_rate': 4.136355555555556e-06, 'epoch': 2.9333333333333336}
Step 33500: {'loss': 1.5861, 'grad_norm': 7.037337779998779, 'learning_rate': 4.047466666666666e-06, 'epoch': 2.977777777777778}
Step 34000: {'loss': 1.5634, 'grad_norm': 7.447547912597656, 'learning_rate': 3.958755555555555e-06, 'epoch': 3.022222222222222}
Step 34500: {'loss': 1.559, 'grad_norm': 8.971569061279297, 'learning_rate': 3.869866666666667e-06, 'epoch': 3.066666666666667}
Step 35000: {'loss': 1.559, 'grad_norm': 6.182006359100342, 'learning_rate': 3.780977777777778e-06, 'epoch': 3.111111111111111}
Step 35500: {'loss': 1.5689, 'grad_norm': 7.3606767654418945, 'learning_rate': 3.6920888888888888e-06, 'epoch': 3.1555555555555554}
Step 36000: {'loss': 1.5674, 'grad_norm': 6.515121936798096, 'learning_rate': 3.6033777777777777e-06, 'epoch': 3.2}
Step 36500: {'loss': 1.5586, 'grad_norm': 10.787449836730957, 'learning_rate': 3.514488888888889e-06, 'epoch': 3.2444444444444445}
Step 37000: {'loss': 1.543, 'grad_norm': 8.306221008300781, 'learning_rate': 3.4256e-06, 'epoch': 3.2888888888888888}
Step 37500: {'loss': 1.5513, 'grad_norm': 8.690393447875977, 'learning_rate': 3.336711111111111e-06, 'epoch': 3.3333333333333335}
Step 38000: {'loss': 1.5535, 'grad_norm': 6.840286731719971, 'learning_rate': 3.247822222222223e-06, 'epoch': 3.3777777777777778}
Step 38500: {'loss': 1.5556, 'grad_norm': 9.0873384475708, 'learning_rate': 3.159111111111111e-06, 'epoch': 3.422222222222222}
Step 39000: {'loss': 1.5484, 'grad_norm': 7.39402961730957, 'learning_rate': 3.0702222222222223e-06, 'epoch': 3.466666666666667}
Step 39500: {'loss': 1.5411, 'grad_norm': 8.830465316772461, 'learning_rate': 2.981333333333333e-06, 'epoch': 3.511111111111111}
Step 40000: {'loss': 1.5659, 'grad_norm': 6.183566570281982, 'learning_rate': 2.892444444444445e-06, 'epoch': 3.5555555555555554}
Step 40500: {'loss': 1.5553, 'grad_norm': 7.886727333068848, 'learning_rate': 2.803555555555556e-06, 'epoch': 3.6}
Step 41000: {'loss': 1.5449, 'grad_norm': 6.7104878425598145, 'learning_rate': 2.714666666666667e-06, 'epoch': 3.6444444444444444}
Step 41500: {'loss': 1.5417, 'grad_norm': 8.200972557067871, 'learning_rate': 2.6257777777777783e-06, 'epoch': 3.688888888888889}
Step 42000: {'loss': 1.5383, 'grad_norm': 8.039353370666504, 'learning_rate': 2.5368888888888892e-06, 'epoch': 3.7333333333333334}
Step 42500: {'loss': 1.5666, 'grad_norm': 7.620815277099609, 'learning_rate': 2.448177777777778e-06, 'epoch': 3.7777777777777777}
Step 43000: {'loss': 1.5424, 'grad_norm': 9.446874618530273, 'learning_rate': 2.359288888888889e-06, 'epoch': 3.822222222222222}
Step 43500: {'loss': 1.555, 'grad_norm': 7.967768669128418, 'learning_rate': 2.2704e-06, 'epoch': 3.8666666666666667}
Step 44000: {'loss': 1.5612, 'grad_norm': 8.872255325317383, 'learning_rate': 2.181511111111111e-06, 'epoch': 3.911111111111111}
Step 44500: {'loss': 1.5347, 'grad_norm': 7.036606788635254, 'learning_rate': 2.092622222222222e-06, 'epoch': 3.9555555555555557}
Step 45000: {'loss': 1.5577, 'grad_norm': 6.905464172363281, 'learning_rate': 2.0037333333333334e-06, 'epoch': 4.0}
Step 45500: {'loss': 1.5337, 'grad_norm': 7.658291339874268, 'learning_rate': 1.9150222222222223e-06, 'epoch': 4.044444444444444}
Step 46000: {'loss': 1.5373, 'grad_norm': 7.097583293914795, 'learning_rate': 1.8261333333333334e-06, 'epoch': 4.088888888888889}
Step 46500: {'loss': 1.535, 'grad_norm': 6.831254482269287, 'learning_rate': 1.7372444444444445e-06, 'epoch': 4.133333333333334}
Step 47000: {'loss': 1.5476, 'grad_norm': 6.870185375213623, 'learning_rate': 1.6483555555555558e-06, 'epoch': 4.177777777777778}
Step 47500: {'loss': 1.5516, 'grad_norm': 7.549745082855225, 'learning_rate': 1.5594666666666669e-06, 'epoch': 4.222222222222222}
Step 48000: {'loss': 1.5377, 'grad_norm': 5.37954044342041, 'learning_rate': 1.470577777777778e-06, 'epoch': 4.266666666666667}
Step 48500: {'loss': 1.5194, 'grad_norm': 7.089263916015625, 'learning_rate': 1.381688888888889e-06, 'epoch': 4.311111111111111}
Step 49000: {'loss': 1.5399, 'grad_norm': 9.074530601501465, 'learning_rate': 1.2928000000000001e-06, 'epoch': 4.355555555555555}
Step 49500: {'loss': 1.5444, 'grad_norm': 7.770559310913086, 'learning_rate': 1.204088888888889e-06, 'epoch': 4.4}
Step 50000: {'loss': 1.5261, 'grad_norm': 9.17616081237793, 'learning_rate': 1.115377777777778e-06, 'epoch': 4.444444444444445}
Step 50500: {'loss': 1.539, 'grad_norm': 8.371129989624023, 'learning_rate': 1.026488888888889e-06, 'epoch': 4.488888888888889}
Step 51000: {'loss': 1.5318, 'grad_norm': 6.279748439788818, 'learning_rate': 9.376e-07, 'epoch': 4.533333333333333}
Step 51500: {'loss': 1.5431, 'grad_norm': 7.320054531097412, 'learning_rate': 8.487111111111111e-07, 'epoch': 4.5777777777777775}
Step 52000: {'loss': 1.5284, 'grad_norm': 8.054677963256836, 'learning_rate': 7.598222222222223e-07, 'epoch': 4.622222222222222}
Step 52500: {'loss': 1.5254, 'grad_norm': 7.313570976257324, 'learning_rate': 6.709333333333334e-07, 'epoch': 4.666666666666667}
Step 53000: {'loss': 1.5428, 'grad_norm': 7.820794105529785, 'learning_rate': 5.820444444444445e-07, 'epoch': 4.711111111111111}
Step 53500: {'loss': 1.5271, 'grad_norm': 6.511903285980225, 'learning_rate': 4.933333333333334e-07, 'epoch': 4.7555555555555555}
Step 54000: {'loss': 1.5484, 'grad_norm': 5.781069278717041, 'learning_rate': 4.0444444444444445e-07, 'epoch': 4.8}
Step 54500: {'loss': 1.5437, 'grad_norm': 8.878251075744629, 'learning_rate': 3.155555555555556e-07, 'epoch': 4.844444444444444}
Step 55000: {'loss': 1.5238, 'grad_norm': 6.851917266845703, 'learning_rate': 2.266666666666667e-07, 'epoch': 4.888888888888889}
Step 55500: {'loss': 1.5499, 'grad_norm': 7.2997894287109375, 'learning_rate': 1.3777777777777778e-07, 'epoch': 4.933333333333334}
Step 56000: {'loss': 1.5409, 'grad_norm': 7.909402847290039, 'learning_rate': 4.906666666666667e-08, 'epoch': 4.977777777777778}
Step 56250: {'train_runtime': 16348.961, 'train_samples_per_second': 13.762, 'train_steps_per_second': 3.441, 'total_flos': 4.4718214454784e+16, 'train_loss': 1.6260043885633682, 'epoch': 5.0}
Step 100: {'loss': 2.5239, 'grad_norm': 17.388330459594727, 'learning_rate': 9.99137777777778e-06, 'epoch': 0.0044444444444444444}
Step 200: {'loss': 2.2017, 'grad_norm': 15.297152519226074, 'learning_rate': 9.982577777777779e-06, 'epoch': 0.008888888888888889}
Step 300: {'loss': 2.1843, 'grad_norm': 16.997190475463867, 'learning_rate': 9.97368888888889e-06, 'epoch': 0.013333333333333334}
Step 400: {'loss': 2.1175, 'grad_norm': 18.57970428466797, 'learning_rate': 9.964800000000002e-06, 'epoch': 0.017777777777777778}
Step 500: {'loss': 2.0691, 'grad_norm': 18.253337860107422, 'learning_rate': 9.955911111111111e-06, 'epoch': 0.022222222222222223}
Step 600: {'loss': 2.0603, 'grad_norm': 18.772064208984375, 'learning_rate': 9.947022222222223e-06, 'epoch': 0.02666666666666667}
Step 700: {'loss': 2.0136, 'grad_norm': 18.24104881286621, 'learning_rate': 9.938133333333334e-06, 'epoch': 0.03111111111111111}
Step 800: {'loss': 2.0571, 'grad_norm': 18.533178329467773, 'learning_rate': 9.929244444444445e-06, 'epoch': 0.035555555555555556}
Step 900: {'loss': 1.9986, 'grad_norm': 17.04436492919922, 'learning_rate': 9.920355555555557e-06, 'epoch': 0.04}
Step 1000: {'loss': 1.9825, 'grad_norm': 18.302810668945312, 'learning_rate': 9.911466666666666e-06, 'epoch': 0.044444444444444446}
Step 1100: {'loss': 1.9476, 'grad_norm': 17.93073081970215, 'learning_rate': 9.90257777777778e-06, 'epoch': 0.04888888888888889}
Step 1200: {'loss': 1.9469, 'grad_norm': 19.28477668762207, 'learning_rate': 9.89368888888889e-06, 'epoch': 0.05333333333333334}
Step 1300: {'loss': 1.9748, 'grad_norm': 17.502145767211914, 'learning_rate': 9.8848e-06, 'epoch': 0.057777777777777775}
Step 1400: {'loss': 1.9157, 'grad_norm': 16.757835388183594, 'learning_rate': 9.875911111111112e-06, 'epoch': 0.06222222222222222}
Step 1500: {'loss': 1.9759, 'grad_norm': 17.788881301879883, 'learning_rate': 9.867022222222223e-06, 'epoch': 0.06666666666666667}
Step 1600: {'loss': 1.9128, 'grad_norm': 15.665871620178223, 'learning_rate': 9.858133333333335e-06, 'epoch': 0.07111111111111111}
Step 1700: {'loss': 1.9164, 'grad_norm': 14.819282531738281, 'learning_rate': 9.849244444444446e-06, 'epoch': 0.07555555555555556}
Step 1800: {'loss': 1.9463, 'grad_norm': 18.77085304260254, 'learning_rate': 9.840355555555556e-06, 'epoch': 0.08}
Step 1900: {'loss': 1.9569, 'grad_norm': 16.496150970458984, 'learning_rate': 9.831466666666667e-06, 'epoch': 0.08444444444444445}
Step 2000: {'loss': 1.9127, 'grad_norm': 25.103160858154297, 'learning_rate': 9.822577777777779e-06, 'epoch': 0.08888888888888889}
Step 2100: {'loss': 1.9225, 'grad_norm': 16.085351943969727, 'learning_rate': 9.81368888888889e-06, 'epoch': 0.09333333333333334}
Step 2200: {'loss': 1.913, 'grad_norm': 16.417327880859375, 'learning_rate': 9.804800000000001e-06, 'epoch': 0.09777777777777778}
Step 2300: {'loss': 1.8561, 'grad_norm': 16.416656494140625, 'learning_rate': 9.795911111111111e-06, 'epoch': 0.10222222222222223}
Step 2400: {'loss': 1.8757, 'grad_norm': 13.902851104736328, 'learning_rate': 9.787022222222222e-06, 'epoch': 0.10666666666666667}
Step 2500: {'loss': 1.8831, 'grad_norm': 18.490571975708008, 'learning_rate': 9.778133333333334e-06, 'epoch': 0.1111111111111111}
Step 2600: {'loss': 1.8659, 'grad_norm': 15.05556583404541, 'learning_rate': 9.769244444444445e-06, 'epoch': 0.11555555555555555}
Step 2700: {'loss': 1.8669, 'grad_norm': 13.764554023742676, 'learning_rate': 9.760355555555557e-06, 'epoch': 0.12}
Step 2800: {'loss': 1.8532, 'grad_norm': 14.832228660583496, 'learning_rate': 9.751466666666668e-06, 'epoch': 0.12444444444444444}
Step 2900: {'loss': 1.8403, 'grad_norm': 17.416955947875977, 'learning_rate': 9.74257777777778e-06, 'epoch': 0.1288888888888889}
Step 3000: {'loss': 1.8437, 'grad_norm': 13.371906280517578, 'learning_rate': 9.733688888888889e-06, 'epoch': 0.13333333333333333}
Step 3100: {'loss': 1.8593, 'grad_norm': 18.324811935424805, 'learning_rate': 9.7248e-06, 'epoch': 0.13777777777777778}
Step 3200: {'loss': 1.8662, 'grad_norm': 17.474910736083984, 'learning_rate': 9.715911111111112e-06, 'epoch': 0.14222222222222222}
Step 3300: {'loss': 1.8786, 'grad_norm': 18.10265350341797, 'learning_rate': 9.707022222222223e-06, 'epoch': 0.14666666666666667}
Step 3400: {'loss': 1.8268, 'grad_norm': 16.60758399963379, 'learning_rate': 9.698133333333335e-06, 'epoch': 0.1511111111111111}
Step 3500: {'loss': 1.7954, 'grad_norm': 17.44681167602539, 'learning_rate': 9.689244444444446e-06, 'epoch': 0.15555555555555556}
Step 3600: {'loss': 1.8502, 'grad_norm': 17.312105178833008, 'learning_rate': 9.680355555555556e-06, 'epoch': 0.16}
Step 3700: {'loss': 1.8138, 'grad_norm': 12.048954963684082, 'learning_rate': 9.671466666666667e-06, 'epoch': 0.16444444444444445}
Step 3800: {'loss': 1.8076, 'grad_norm': 13.084237098693848, 'learning_rate': 9.662577777777778e-06, 'epoch': 0.1688888888888889}
Step 3900: {'loss': 1.8413, 'grad_norm': 16.204092025756836, 'learning_rate': 9.65368888888889e-06, 'epoch': 0.17333333333333334}
Step 4000: {'loss': 1.7997, 'grad_norm': 13.554156303405762, 'learning_rate': 9.644800000000001e-06, 'epoch': 0.17777777777777778}
Step 4100: {'loss': 1.8335, 'grad_norm': 14.89533805847168, 'learning_rate': 9.635911111111111e-06, 'epoch': 0.18222222222222223}
Step 4200: {'loss': 1.8226, 'grad_norm': 11.877056121826172, 'learning_rate': 9.627111111111112e-06, 'epoch': 0.18666666666666668}
Step 4300: {'loss': 1.8352, 'grad_norm': 12.016780853271484, 'learning_rate': 9.618222222222223e-06, 'epoch': 0.19111111111111112}
Step 4400: {'loss': 1.7861, 'grad_norm': 15.573159217834473, 'learning_rate': 9.609333333333333e-06, 'epoch': 0.19555555555555557}
Step 4500: {'loss': 1.8184, 'grad_norm': 10.65854549407959, 'learning_rate': 9.600444444444446e-06, 'epoch': 0.2}
Step 4600: {'loss': 1.8023, 'grad_norm': 13.277341842651367, 'learning_rate': 9.591555555555556e-06, 'epoch': 0.20444444444444446}
Step 4700: {'loss': 1.8113, 'grad_norm': 15.705934524536133, 'learning_rate': 9.582666666666667e-06, 'epoch': 0.2088888888888889}
Step 4800: {'loss': 1.8082, 'grad_norm': 16.005813598632812, 'learning_rate': 9.573777777777779e-06, 'epoch': 0.21333333333333335}
Step 4900: {'loss': 1.8059, 'grad_norm': 11.886817932128906, 'learning_rate': 9.56488888888889e-06, 'epoch': 0.21777777777777776}
Step 5000: {'loss': 1.8324, 'grad_norm': 14.310324668884277, 'learning_rate': 9.556000000000001e-06, 'epoch': 0.2222222222222222}
Step 5100: {'loss': 1.7786, 'grad_norm': 15.168248176574707, 'learning_rate': 9.547111111111111e-06, 'epoch': 0.22666666666666666}
Step 5200: {'loss': 1.7824, 'grad_norm': 13.86662769317627, 'learning_rate': 9.538222222222222e-06, 'epoch': 0.2311111111111111}
Step 5300: {'loss': 1.7969, 'grad_norm': 14.12816333770752, 'learning_rate': 9.529333333333334e-06, 'epoch': 0.23555555555555555}
Step 5400: {'loss': 1.8066, 'grad_norm': 13.691091537475586, 'learning_rate': 9.520444444444445e-06, 'epoch': 0.24}
Step 5500: {'loss': 1.7616, 'grad_norm': 11.227616310119629, 'learning_rate': 9.511555555555557e-06, 'epoch': 0.24444444444444444}
Step 5600: {'loss': 1.8105, 'grad_norm': 9.762173652648926, 'learning_rate': 9.502666666666668e-06, 'epoch': 0.24888888888888888}
Step 5700: {'loss': 1.7768, 'grad_norm': 16.057554244995117, 'learning_rate': 9.493777777777778e-06, 'epoch': 0.25333333333333335}
Step 5800: {'loss': 1.7561, 'grad_norm': 12.824868202209473, 'learning_rate': 9.48488888888889e-06, 'epoch': 0.2577777777777778}
Step 5900: {'loss': 1.783, 'grad_norm': 12.752138137817383, 'learning_rate': 9.476e-06, 'epoch': 0.26222222222222225}
Step 6000: {'loss': 1.7715, 'grad_norm': 14.644753456115723, 'learning_rate': 9.467111111111112e-06, 'epoch': 0.26666666666666666}
Step 6100: {'loss': 1.7784, 'grad_norm': 12.042257308959961, 'learning_rate': 9.458222222222223e-06, 'epoch': 0.27111111111111114}
Step 6200: {'loss': 1.7772, 'grad_norm': 16.984880447387695, 'learning_rate': 9.449422222222223e-06, 'epoch': 0.27555555555555555}
Step 6300: {'loss': 1.8327, 'grad_norm': 10.879103660583496, 'learning_rate': 9.440533333333334e-06, 'epoch': 0.28}
Step 6400: {'loss': 1.7628, 'grad_norm': 11.500114440917969, 'learning_rate': 9.431644444444445e-06, 'epoch': 0.28444444444444444}
Step 6500: {'loss': 1.7208, 'grad_norm': 13.261302947998047, 'learning_rate': 9.422755555555557e-06, 'epoch': 0.28888888888888886}
Step 6600: {'loss': 1.7596, 'grad_norm': 10.421128273010254, 'learning_rate': 9.413866666666668e-06, 'epoch': 0.29333333333333333}
Step 6700: {'loss': 1.7502, 'grad_norm': 9.510852813720703, 'learning_rate': 9.404977777777778e-06, 'epoch': 0.29777777777777775}
Step 6800: {'loss': 1.7449, 'grad_norm': 10.070283889770508, 'learning_rate': 9.39608888888889e-06, 'epoch': 0.3022222222222222}
Step 6900: {'loss': 1.7262, 'grad_norm': 11.000155448913574, 'learning_rate': 9.3872e-06, 'epoch': 0.30666666666666664}
Step 7000: {'loss': 1.7516, 'grad_norm': 12.140169143676758, 'learning_rate': 9.378311111111112e-06, 'epoch': 0.3111111111111111}
Step 7100: {'loss': 1.7347, 'grad_norm': 11.949451446533203, 'learning_rate': 9.369422222222223e-06, 'epoch': 0.31555555555555553}
Step 7200: {'loss': 1.7336, 'grad_norm': 9.843961715698242, 'learning_rate': 9.360533333333333e-06, 'epoch': 0.32}
Step 7300: {'loss': 1.7079, 'grad_norm': 12.260293006896973, 'learning_rate': 9.351644444444446e-06, 'epoch': 0.3244444444444444}
Step 7400: {'loss': 1.711, 'grad_norm': 10.42822265625, 'learning_rate': 9.342755555555556e-06, 'epoch': 0.3288888888888889}
Step 7500: {'loss': 1.7562, 'grad_norm': 9.431721687316895, 'learning_rate': 9.333866666666667e-06, 'epoch': 0.3333333333333333}
Step 7600: {'loss': 1.7265, 'grad_norm': 9.12606430053711, 'learning_rate': 9.324977777777779e-06, 'epoch': 0.3377777777777778}
Step 7700: {'loss': 1.7779, 'grad_norm': 10.723414421081543, 'learning_rate': 9.31608888888889e-06, 'epoch': 0.3422222222222222}
Step 7800: {'loss': 1.7318, 'grad_norm': 9.323163986206055, 'learning_rate': 9.307200000000001e-06, 'epoch': 0.3466666666666667}
Step 7900: {'loss': 1.7343, 'grad_norm': 10.922049522399902, 'learning_rate': 9.298311111111113e-06, 'epoch': 0.3511111111111111}
Step 8000: {'loss': 1.7317, 'grad_norm': 13.73877239227295, 'learning_rate': 9.289422222222222e-06, 'epoch': 0.35555555555555557}
Step 8100: {'loss': 1.6896, 'grad_norm': 17.39781951904297, 'learning_rate': 9.280533333333334e-06, 'epoch': 0.36}
Step 8200: {'loss': 1.7476, 'grad_norm': 12.814717292785645, 'learning_rate': 9.271733333333333e-06, 'epoch': 0.36444444444444446}
Step 8300: {'loss': 1.733, 'grad_norm': 10.385541915893555, 'learning_rate': 9.262844444444446e-06, 'epoch': 0.3688888888888889}
Step 8400: {'loss': 1.7421, 'grad_norm': 10.067367553710938, 'learning_rate': 9.253955555555556e-06, 'epoch': 0.37333333333333335}
Step 8500: {'loss': 1.7096, 'grad_norm': 10.894796371459961, 'learning_rate': 9.245066666666667e-06, 'epoch': 0.37777777777777777}
Step 8600: {'loss': 1.7398, 'grad_norm': 12.3491792678833, 'learning_rate': 9.236177777777779e-06, 'epoch': 0.38222222222222224}
Step 8700: {'loss': 1.7039, 'grad_norm': 10.280139923095703, 'learning_rate': 9.22728888888889e-06, 'epoch': 0.38666666666666666}
Step 8800: {'loss': 1.7074, 'grad_norm': 10.620744705200195, 'learning_rate': 9.218400000000001e-06, 'epoch': 0.39111111111111113}
Step 8900: {'loss': 1.6719, 'grad_norm': 10.807515144348145, 'learning_rate': 9.209511111111113e-06, 'epoch': 0.39555555555555555}
Step 9000: {'loss': 1.7314, 'grad_norm': 10.173757553100586, 'learning_rate': 9.200622222222222e-06, 'epoch': 0.4}
Step 9100: {'loss': 1.7055, 'grad_norm': 10.942221641540527, 'learning_rate': 9.191733333333334e-06, 'epoch': 0.40444444444444444}
Step 9200: {'loss': 1.6639, 'grad_norm': 11.87822437286377, 'learning_rate': 9.182844444444445e-06, 'epoch': 0.4088888888888889}
Step 9300: {'loss': 1.703, 'grad_norm': 13.548543930053711, 'learning_rate': 9.173955555555557e-06, 'epoch': 0.41333333333333333}
Step 9400: {'loss': 1.7348, 'grad_norm': 10.284761428833008, 'learning_rate': 9.165066666666668e-06, 'epoch': 0.4177777777777778}
Step 9500: {'loss': 1.6905, 'grad_norm': 10.872702598571777, 'learning_rate': 9.156177777777778e-06, 'epoch': 0.4222222222222222}
Step 9600: {'loss': 1.7279, 'grad_norm': 9.855941772460938, 'learning_rate': 9.14728888888889e-06, 'epoch': 0.4266666666666667}
Step 9700: {'loss': 1.7317, 'grad_norm': 11.923169136047363, 'learning_rate': 9.1384e-06, 'epoch': 0.4311111111111111}
Step 9800: {'loss': 1.6809, 'grad_norm': 9.642537117004395, 'learning_rate': 9.129511111111112e-06, 'epoch': 0.43555555555555553}
Step 9900: {'loss': 1.699, 'grad_norm': 9.469614028930664, 'learning_rate': 9.120622222222223e-06, 'epoch': 0.44}
Step 10000: {'loss': 1.7359, 'grad_norm': 10.01758861541748, 'learning_rate': 9.111733333333333e-06, 'epoch': 0.4444444444444444}
Step 10100: {'loss': 1.706, 'grad_norm': 9.720852851867676, 'learning_rate': 9.102844444444446e-06, 'epoch': 0.4488888888888889}
Step 10200: {'loss': 1.7063, 'grad_norm': 9.986241340637207, 'learning_rate': 9.093955555555556e-06, 'epoch': 0.4533333333333333}
Step 10300: {'loss': 1.7272, 'grad_norm': 10.34422492980957, 'learning_rate': 9.085155555555557e-06, 'epoch': 0.4577777777777778}
Step 10400: {'loss': 1.7181, 'grad_norm': 12.663330078125, 'learning_rate': 9.076266666666668e-06, 'epoch': 0.4622222222222222}
Step 10500: {'loss': 1.6938, 'grad_norm': 10.301664352416992, 'learning_rate': 9.067377777777778e-06, 'epoch': 0.4666666666666667}
Step 10600: {'loss': 1.6953, 'grad_norm': 11.301789283752441, 'learning_rate': 9.05848888888889e-06, 'epoch': 0.4711111111111111}
Step 10700: {'loss': 1.702, 'grad_norm': 9.808507919311523, 'learning_rate': 9.0496e-06, 'epoch': 0.47555555555555556}
Step 10800: {'loss': 1.7045, 'grad_norm': 10.915820121765137, 'learning_rate': 9.040711111111112e-06, 'epoch': 0.48}
Step 10900: {'loss': 1.7329, 'grad_norm': 10.194230079650879, 'learning_rate': 9.031822222222223e-06, 'epoch': 0.48444444444444446}
Step 11000: {'loss': 1.6767, 'grad_norm': 9.148147583007812, 'learning_rate': 9.022933333333333e-06, 'epoch': 0.4888888888888889}
Step 11100: {'loss': 1.7107, 'grad_norm': 9.405537605285645, 'learning_rate': 9.014044444444446e-06, 'epoch': 0.49333333333333335}
Step 11200: {'loss': 1.6887, 'grad_norm': 10.658920288085938, 'learning_rate': 9.005155555555556e-06, 'epoch': 0.49777777777777776}
Step 11300: {'loss': 1.7156, 'grad_norm': 11.727605819702148, 'learning_rate': 8.996266666666667e-06, 'epoch': 0.5022222222222222}
Step 11400: {'loss': 1.6817, 'grad_norm': 9.040508270263672, 'learning_rate': 8.987377777777779e-06, 'epoch': 0.5066666666666667}
Step 11500: {'loss': 1.7147, 'grad_norm': 10.149596214294434, 'learning_rate': 8.97848888888889e-06, 'epoch': 0.5111111111111111}
Step 11600: {'loss': 1.7036, 'grad_norm': 9.129467010498047, 'learning_rate': 8.969600000000001e-06, 'epoch': 0.5155555555555555}
Step 11700: {'loss': 1.6912, 'grad_norm': 8.030245780944824, 'learning_rate': 8.960711111111113e-06, 'epoch': 0.52}
Step 11800: {'loss': 1.6763, 'grad_norm': 8.971735954284668, 'learning_rate': 8.951822222222222e-06, 'epoch': 0.5244444444444445}
Step 11900: {'loss': 1.7113, 'grad_norm': 10.432539939880371, 'learning_rate': 8.942933333333334e-06, 'epoch': 0.5288888888888889}
Step 12000: {'loss': 1.7049, 'grad_norm': 7.971395015716553, 'learning_rate': 8.934044444444445e-06, 'epoch': 0.5333333333333333}
Step 12100: {'loss': 1.6999, 'grad_norm': 10.118154525756836, 'learning_rate': 8.925155555555557e-06, 'epoch': 0.5377777777777778}
Step 12200: {'loss': 1.6674, 'grad_norm': 10.822587966918945, 'learning_rate': 8.916266666666668e-06, 'epoch': 0.5422222222222223}
Step 12300: {'loss': 1.7044, 'grad_norm': 7.188018321990967, 'learning_rate': 8.907466666666667e-06, 'epoch': 0.5466666666666666}
Step 12400: {'loss': 1.6603, 'grad_norm': 10.695361137390137, 'learning_rate': 8.898577777777779e-06, 'epoch': 0.5511111111111111}
Step 12500: {'loss': 1.7018, 'grad_norm': 8.900588035583496, 'learning_rate': 8.88968888888889e-06, 'epoch': 0.5555555555555556}
Step 12600: {'loss': 1.6879, 'grad_norm': 12.057779312133789, 'learning_rate': 8.8808e-06, 'epoch': 0.56}
Step 12700: {'loss': 1.7067, 'grad_norm': 10.189815521240234, 'learning_rate': 8.871911111111113e-06, 'epoch': 0.5644444444444444}
Step 12800: {'loss': 1.6716, 'grad_norm': 10.131999969482422, 'learning_rate': 8.863022222222223e-06, 'epoch': 0.5688888888888889}
Step 12900: {'loss': 1.6619, 'grad_norm': 9.462944030761719, 'learning_rate': 8.854133333333334e-06, 'epoch': 0.5733333333333334}
Step 13000: {'loss': 1.6877, 'grad_norm': 8.634381294250488, 'learning_rate': 8.845244444444445e-06, 'epoch': 0.5777777777777777}
Step 13100: {'loss': 1.68, 'grad_norm': 7.763330936431885, 'learning_rate': 8.836355555555555e-06, 'epoch': 0.5822222222222222}
Step 13200: {'loss': 1.6403, 'grad_norm': 9.522828102111816, 'learning_rate': 8.827466666666668e-06, 'epoch': 0.5866666666666667}
Step 13300: {'loss': 1.6779, 'grad_norm': 8.237302780151367, 'learning_rate': 8.818577777777778e-06, 'epoch': 0.5911111111111111}
Step 13400: {'loss': 1.7058, 'grad_norm': 7.0724945068359375, 'learning_rate': 8.809688888888889e-06, 'epoch': 0.5955555555555555}
Step 13500: {'loss': 1.6762, 'grad_norm': 9.826648712158203, 'learning_rate': 8.8008e-06, 'epoch': 0.6}
Step 13600: {'loss': 1.6663, 'grad_norm': 9.752035140991211, 'learning_rate': 8.791911111111112e-06, 'epoch': 0.6044444444444445}
Step 13700: {'loss': 1.6946, 'grad_norm': 8.073930740356445, 'learning_rate': 8.783022222222223e-06, 'epoch': 0.6088888888888889}
Step 13800: {'loss': 1.6309, 'grad_norm': 9.249505043029785, 'learning_rate': 8.774133333333335e-06, 'epoch': 0.6133333333333333}
Step 13900: {'loss': 1.6672, 'grad_norm': 9.818511009216309, 'learning_rate': 8.765244444444444e-06, 'epoch': 0.6177777777777778}
Step 14000: {'loss': 1.6845, 'grad_norm': 9.497021675109863, 'learning_rate': 8.756355555555556e-06, 'epoch': 0.6222222222222222}
Step 14100: {'loss': 1.6371, 'grad_norm': 7.652246952056885, 'learning_rate': 8.747466666666667e-06, 'epoch': 0.6266666666666667}
Step 14200: {'loss': 1.6246, 'grad_norm': 9.560464859008789, 'learning_rate': 8.738577777777779e-06, 'epoch': 0.6311111111111111}
Step 14300: {'loss': 1.6771, 'grad_norm': 7.357038497924805, 'learning_rate': 8.72968888888889e-06, 'epoch': 0.6355555555555555}
Step 14400: {'loss': 1.6371, 'grad_norm': 9.394508361816406, 'learning_rate': 8.7208e-06, 'epoch': 0.64}
Step 14500: {'loss': 1.6223, 'grad_norm': 7.964039325714111, 'learning_rate': 8.711911111111113e-06, 'epoch': 0.6444444444444445}
Step 14600: {'loss': 1.672, 'grad_norm': 10.275206565856934, 'learning_rate': 8.703111111111112e-06, 'epoch': 0.6488888888888888}
Step 14700: {'loss': 1.62, 'grad_norm': 8.031740188598633, 'learning_rate': 8.694222222222223e-06, 'epoch': 0.6533333333333333}
Step 14800: {'loss': 1.6496, 'grad_norm': 8.533313751220703, 'learning_rate': 8.685333333333335e-06, 'epoch': 0.6577777777777778}
Step 14900: {'loss': 1.6449, 'grad_norm': 9.507645606994629, 'learning_rate': 8.676444444444444e-06, 'epoch': 0.6622222222222223}
Step 15000: {'loss': 1.6443, 'grad_norm': 6.849364757537842, 'learning_rate': 8.667555555555556e-06, 'epoch': 0.6666666666666666}
Step 15100: {'loss': 1.6485, 'grad_norm': 7.613852024078369, 'learning_rate': 8.658666666666667e-06, 'epoch': 0.6711111111111111}
Step 15200: {'loss': 1.6324, 'grad_norm': 7.4144062995910645, 'learning_rate': 8.649777777777779e-06, 'epoch': 0.6755555555555556}
Step 15300: {'loss': 1.6408, 'grad_norm': 10.9335298538208, 'learning_rate': 8.64088888888889e-06, 'epoch': 0.68}
Step 15400: {'loss': 1.6778, 'grad_norm': 8.390998840332031, 'learning_rate': 8.632e-06, 'epoch': 0.6844444444444444}
Step 15500: {'loss': 1.6611, 'grad_norm': 9.599309921264648, 'learning_rate': 8.623111111111113e-06, 'epoch': 0.6888888888888889}
Step 15600: {'loss': 1.6659, 'grad_norm': 9.75041389465332, 'learning_rate': 8.614222222222222e-06, 'epoch': 0.6933333333333334}
Step 15700: {'loss': 1.6082, 'grad_norm': 8.453857421875, 'learning_rate': 8.605333333333334e-06, 'epoch': 0.6977777777777778}
Step 15800: {'loss': 1.6471, 'grad_norm': 7.816817283630371, 'learning_rate': 8.596444444444445e-06, 'epoch': 0.7022222222222222}
Step 15900: {'loss': 1.6287, 'grad_norm': 10.411517143249512, 'learning_rate': 8.587555555555557e-06, 'epoch': 0.7066666666666667}
Step 16000: {'loss': 1.6555, 'grad_norm': 9.99903678894043, 'learning_rate': 8.578666666666668e-06, 'epoch': 0.7111111111111111}
Step 16100: {'loss': 1.6513, 'grad_norm': 7.137473106384277, 'learning_rate': 8.569777777777778e-06, 'epoch': 0.7155555555555555}
Step 16200: {'loss': 1.6662, 'grad_norm': 9.68210506439209, 'learning_rate': 8.560888888888889e-06, 'epoch': 0.72}
Step 16300: {'loss': 1.6359, 'grad_norm': 9.617753028869629, 'learning_rate': 8.552e-06, 'epoch': 0.7244444444444444}
Step 16400: {'loss': 1.632, 'grad_norm': 6.820323467254639, 'learning_rate': 8.543111111111112e-06, 'epoch': 0.7288888888888889}
Step 16500: {'loss': 1.6578, 'grad_norm': 10.73631477355957, 'learning_rate': 8.534222222222223e-06, 'epoch': 0.7333333333333333}
Step 16600: {'loss': 1.6367, 'grad_norm': 7.06068754196167, 'learning_rate': 8.525333333333335e-06, 'epoch': 0.7377777777777778}
Step 16700: {'loss': 1.6374, 'grad_norm': 7.139745235443115, 'learning_rate': 8.516444444444444e-06, 'epoch': 0.7422222222222222}
Step 16800: {'loss': 1.6219, 'grad_norm': 10.950468063354492, 'learning_rate': 8.507555555555557e-06, 'epoch': 0.7466666666666667}
Step 16900: {'loss': 1.5976, 'grad_norm': 7.788507461547852, 'learning_rate': 8.498666666666667e-06, 'epoch': 0.7511111111111111}
Step 17000: {'loss': 1.6539, 'grad_norm': 8.566888809204102, 'learning_rate': 8.489777777777778e-06, 'epoch': 0.7555555555555555}
Step 17100: {'loss': 1.6026, 'grad_norm': 6.657455921173096, 'learning_rate': 8.48088888888889e-06, 'epoch': 0.76}
Step 17200: {'loss': 1.6371, 'grad_norm': 6.726129531860352, 'learning_rate': 8.47208888888889e-06, 'epoch': 0.7644444444444445}
Step 17300: {'loss': 1.6345, 'grad_norm': 9.021007537841797, 'learning_rate': 8.4632e-06, 'epoch': 0.7688888888888888}
Step 17400: {'loss': 1.6305, 'grad_norm': 8.984150886535645, 'learning_rate': 8.454311111111112e-06, 'epoch': 0.7733333333333333}
Step 17500: {'loss': 1.6333, 'grad_norm': 7.172377586364746, 'learning_rate': 8.445422222222223e-06, 'epoch': 0.7777777777777778}
Step 17600: {'loss': 1.6449, 'grad_norm': 7.673545837402344, 'learning_rate': 8.436533333333335e-06, 'epoch': 0.7822222222222223}
Step 17700: {'loss': 1.6271, 'grad_norm': 6.202629566192627, 'learning_rate': 8.427644444444444e-06, 'epoch': 0.7866666666666666}
Step 17800: {'loss': 1.6368, 'grad_norm': 7.6658477783203125, 'learning_rate': 8.418755555555557e-06, 'epoch': 0.7911111111111111}
Step 17900: {'loss': 1.6164, 'grad_norm': 6.899865627288818, 'learning_rate': 8.409866666666667e-06, 'epoch': 0.7955555555555556}
Step 18000: {'loss': 1.6345, 'grad_norm': 8.96827220916748, 'learning_rate': 8.400977777777779e-06, 'epoch': 0.8}
Step 18100: {'loss': 1.6232, 'grad_norm': 6.600774765014648, 'learning_rate': 8.39208888888889e-06, 'epoch': 0.8044444444444444}
Step 18200: {'loss': 1.6197, 'grad_norm': 6.6473917961120605, 'learning_rate': 8.3832e-06, 'epoch': 0.8088888888888889}
Step 18300: {'loss': 1.5936, 'grad_norm': 7.549502849578857, 'learning_rate': 8.374311111111113e-06, 'epoch': 0.8133333333333334}
Step 18400: {'loss': 1.6078, 'grad_norm': 8.4423828125, 'learning_rate': 8.365422222222222e-06, 'epoch': 0.8177777777777778}
Step 18500: {'loss': 1.6628, 'grad_norm': 6.629007816314697, 'learning_rate': 8.356533333333334e-06, 'epoch': 0.8222222222222222}
Step 18600: {'loss': 1.6375, 'grad_norm': 10.268714904785156, 'learning_rate': 8.347644444444445e-06, 'epoch': 0.8266666666666667}
Step 18700: {'loss': 1.6094, 'grad_norm': 8.607678413391113, 'learning_rate': 8.338755555555557e-06, 'epoch': 0.8311111111111111}
Step 18800: {'loss': 1.6411, 'grad_norm': 7.6094136238098145, 'learning_rate': 8.329866666666668e-06, 'epoch': 0.8355555555555556}
Step 18900: {'loss': 1.6523, 'grad_norm': 9.94395923614502, 'learning_rate': 8.32097777777778e-06, 'epoch': 0.84}
Step 19000: {'loss': 1.6344, 'grad_norm': 7.436761856079102, 'learning_rate': 8.312088888888889e-06, 'epoch': 0.8444444444444444}
Step 19100: {'loss': 1.6335, 'grad_norm': 6.486690998077393, 'learning_rate': 8.3032e-06, 'epoch': 0.8488888888888889}
Step 19200: {'loss': 1.6413, 'grad_norm': 8.458440780639648, 'learning_rate': 8.294311111111112e-06, 'epoch': 0.8533333333333334}
Step 19300: {'loss': 1.595, 'grad_norm': 7.614358901977539, 'learning_rate': 8.285422222222223e-06, 'epoch': 0.8577777777777778}
Step 19400: {'loss': 1.6075, 'grad_norm': 7.695815086364746, 'learning_rate': 8.276622222222223e-06, 'epoch': 0.8622222222222222}
Step 19500: {'loss': 1.6594, 'grad_norm': 5.950987815856934, 'learning_rate': 8.267733333333334e-06, 'epoch': 0.8666666666666667}
Step 19600: {'loss': 1.584, 'grad_norm': 6.752473831176758, 'learning_rate': 8.258844444444445e-06, 'epoch': 0.8711111111111111}
Step 19700: {'loss': 1.6583, 'grad_norm': 8.3783597946167, 'learning_rate': 8.249955555555557e-06, 'epoch': 0.8755555555555555}
Step 19800: {'loss': 1.5932, 'grad_norm': 9.332966804504395, 'learning_rate': 8.241066666666668e-06, 'epoch': 0.88}
Step 19900: {'loss': 1.6253, 'grad_norm': 8.672566413879395, 'learning_rate': 8.23217777777778e-06, 'epoch': 0.8844444444444445}
Step 20000: {'loss': 1.6206, 'grad_norm': 7.815343379974365, 'learning_rate': 8.223288888888889e-06, 'epoch': 0.8888888888888888}
Step 20100: {'loss': 1.6511, 'grad_norm': 8.190935134887695, 'learning_rate': 8.2144e-06, 'epoch': 0.8933333333333333}
Step 20200: {'loss': 1.595, 'grad_norm': 8.077916145324707, 'learning_rate': 8.205511111111112e-06, 'epoch': 0.8977777777777778}
Step 20300: {'loss': 1.6082, 'grad_norm': 8.664311408996582, 'learning_rate': 8.196622222222223e-06, 'epoch': 0.9022222222222223}
Step 20400: {'loss': 1.5958, 'grad_norm': 5.9335832595825195, 'learning_rate': 8.187733333333335e-06, 'epoch': 0.9066666666666666}
Step 20500: {'loss': 1.6367, 'grad_norm': 7.462915897369385, 'learning_rate': 8.178844444444444e-06, 'epoch': 0.9111111111111111}
Step 20600: {'loss': 1.625, 'grad_norm': 7.8472723960876465, 'learning_rate': 8.169955555555556e-06, 'epoch': 0.9155555555555556}
Step 20700: {'loss': 1.598, 'grad_norm': 7.444392681121826, 'learning_rate': 8.161066666666667e-06, 'epoch': 0.92}
Step 20800: {'loss': 1.6024, 'grad_norm': 7.220822811126709, 'learning_rate': 8.152177777777778e-06, 'epoch': 0.9244444444444444}
Step 20900: {'loss': 1.6026, 'grad_norm': 9.68114185333252, 'learning_rate': 8.14328888888889e-06, 'epoch': 0.9288888888888889}
Step 21000: {'loss': 1.6026, 'grad_norm': 6.617390155792236, 'learning_rate': 8.134400000000001e-06, 'epoch': 0.9333333333333333}
Step 21100: {'loss': 1.5902, 'grad_norm': 7.811330795288086, 'learning_rate': 8.125511111111111e-06, 'epoch': 0.9377777777777778}
Step 21200: {'loss': 1.5812, 'grad_norm': 6.562040328979492, 'learning_rate': 8.116622222222222e-06, 'epoch': 0.9422222222222222}
Step 21300: {'loss': 1.6306, 'grad_norm': 8.382641792297363, 'learning_rate': 8.107733333333334e-06, 'epoch': 0.9466666666666667}
Step 21400: {'loss': 1.6184, 'grad_norm': 7.373388290405273, 'learning_rate': 8.098844444444445e-06, 'epoch': 0.9511111111111111}
Step 21500: {'loss': 1.6409, 'grad_norm': 7.544788360595703, 'learning_rate': 8.089955555555556e-06, 'epoch': 0.9555555555555556}
Step 21600: {'loss': 1.6395, 'grad_norm': 7.320038318634033, 'learning_rate': 8.081066666666668e-06, 'epoch': 0.96}
Step 21700: {'loss': 1.592, 'grad_norm': 8.033020973205566, 'learning_rate': 8.07217777777778e-06, 'epoch': 0.9644444444444444}
Step 21800: {'loss': 1.5992, 'grad_norm': 6.521832466125488, 'learning_rate': 8.063288888888889e-06, 'epoch': 0.9688888888888889}
Step 21900: {'loss': 1.6386, 'grad_norm': 7.836426258087158, 'learning_rate': 8.0544e-06, 'epoch': 0.9733333333333334}
Step 22000: {'loss': 1.6123, 'grad_norm': 9.387101173400879, 'learning_rate': 8.045511111111112e-06, 'epoch': 0.9777777777777777}
Step 22100: {'loss': 1.5849, 'grad_norm': 8.025131225585938, 'learning_rate': 8.036622222222223e-06, 'epoch': 0.9822222222222222}
Step 22200: {'loss': 1.6017, 'grad_norm': 7.736831188201904, 'learning_rate': 8.027733333333334e-06, 'epoch': 0.9866666666666667}
Step 22300: {'loss': 1.6117, 'grad_norm': 7.7090840339660645, 'learning_rate': 8.018844444444444e-06, 'epoch': 0.9911111111111112}
Step 22400: {'loss': 1.6061, 'grad_norm': 8.107121467590332, 'learning_rate': 8.009955555555556e-06, 'epoch': 0.9955555555555555}
Step 22500: {'loss': 1.6409, 'grad_norm': 6.659148216247559, 'learning_rate': 8.001066666666667e-06, 'epoch': 1.0}
Step 22600: {'loss': 1.5982, 'grad_norm': 7.932631492614746, 'learning_rate': 7.992177777777778e-06, 'epoch': 1.0044444444444445}
Step 22700: {'loss': 1.5532, 'grad_norm': 7.146259784698486, 'learning_rate': 7.98328888888889e-06, 'epoch': 1.008888888888889}
Step 22800: {'loss': 1.5884, 'grad_norm': 5.4986491203308105, 'learning_rate': 7.974400000000001e-06, 'epoch': 1.0133333333333334}
Step 22900: {'loss': 1.5923, 'grad_norm': 5.687443733215332, 'learning_rate': 7.96551111111111e-06, 'epoch': 1.0177777777777777}
Step 23000: {'loss': 1.5722, 'grad_norm': 6.1999101638793945, 'learning_rate': 7.956622222222224e-06, 'epoch': 1.0222222222222221}
Step 23100: {'loss': 1.6086, 'grad_norm': 6.785337924957275, 'learning_rate': 7.947733333333334e-06, 'epoch': 1.0266666666666666}
Step 23200: {'loss': 1.6115, 'grad_norm': 6.401342391967773, 'learning_rate': 7.938844444444445e-06, 'epoch': 1.031111111111111}
Step 23300: {'loss': 1.5817, 'grad_norm': 6.894389629364014, 'learning_rate': 7.929955555555556e-06, 'epoch': 1.0355555555555556}
Step 23400: {'loss': 1.5786, 'grad_norm': 8.28018569946289, 'learning_rate': 7.921155555555556e-06, 'epoch': 1.04}
Step 23500: {'loss': 1.595, 'grad_norm': 6.259400844573975, 'learning_rate': 7.912266666666667e-06, 'epoch': 1.0444444444444445}
Step 23600: {'loss': 1.5762, 'grad_norm': 8.374893188476562, 'learning_rate': 7.903377777777778e-06, 'epoch': 1.048888888888889}
Step 23700: {'loss': 1.5882, 'grad_norm': 8.769479751586914, 'learning_rate': 7.89448888888889e-06, 'epoch': 1.0533333333333332}
Step 23800: {'loss': 1.6177, 'grad_norm': 7.0068888664245605, 'learning_rate': 7.885600000000001e-06, 'epoch': 1.0577777777777777}
Step 23900: {'loss': 1.6018, 'grad_norm': 6.643163681030273, 'learning_rate': 7.876711111111111e-06, 'epoch': 1.0622222222222222}
Step 24000: {'loss': 1.6197, 'grad_norm': 8.110276222229004, 'learning_rate': 7.867822222222224e-06, 'epoch': 1.0666666666666667}
Step 24100: {'loss': 1.575, 'grad_norm': 6.689886093139648, 'learning_rate': 7.858933333333334e-06, 'epoch': 1.0711111111111111}
Step 24200: {'loss': 1.5652, 'grad_norm': 7.614892959594727, 'learning_rate': 7.850133333333335e-06, 'epoch': 1.0755555555555556}
Step 24300: {'loss': 1.5771, 'grad_norm': 6.388031959533691, 'learning_rate': 7.841244444444444e-06, 'epoch': 1.08}
Step 24400: {'loss': 1.5989, 'grad_norm': 9.326695442199707, 'learning_rate': 7.832355555555556e-06, 'epoch': 1.0844444444444445}
Step 24500: {'loss': 1.5906, 'grad_norm': 6.254749298095703, 'learning_rate': 7.823466666666667e-06, 'epoch': 1.0888888888888888}
Step 24600: {'loss': 1.5477, 'grad_norm': 7.425785064697266, 'learning_rate': 7.814577777777779e-06, 'epoch': 1.0933333333333333}
Step 24700: {'loss': 1.6216, 'grad_norm': 6.93689489364624, 'learning_rate': 7.80568888888889e-06, 'epoch': 1.0977777777777777}
Step 24800: {'loss': 1.597, 'grad_norm': 7.174461364746094, 'learning_rate': 7.796800000000001e-06, 'epoch': 1.1022222222222222}
Step 24900: {'loss': 1.6026, 'grad_norm': 6.8939313888549805, 'learning_rate': 7.787911111111111e-06, 'epoch': 1.1066666666666667}
Step 25000: {'loss': 1.564, 'grad_norm': 6.861616611480713, 'learning_rate': 7.779022222222224e-06, 'epoch': 1.1111111111111112}
Step 25100: {'loss': 1.5875, 'grad_norm': 7.641388893127441, 'learning_rate': 7.770133333333334e-06, 'epoch': 1.1155555555555556}
Step 25200: {'loss': 1.5833, 'grad_norm': 7.865501880645752, 'learning_rate': 7.761244444444445e-06, 'epoch': 1.12}
Step 25300: {'loss': 1.5957, 'grad_norm': 7.109558582305908, 'learning_rate': 7.752355555555557e-06, 'epoch': 1.1244444444444444}
Step 25400: {'loss': 1.5951, 'grad_norm': 7.107778549194336, 'learning_rate': 7.743466666666666e-06, 'epoch': 1.1288888888888888}
Step 25500: {'loss': 1.5898, 'grad_norm': 6.820830821990967, 'learning_rate': 7.73457777777778e-06, 'epoch': 1.1333333333333333}
Step 25600: {'loss': 1.5905, 'grad_norm': 6.92511510848999, 'learning_rate': 7.725688888888889e-06, 'epoch': 1.1377777777777778}
Step 25700: {'loss': 1.624, 'grad_norm': 8.571768760681152, 'learning_rate': 7.7168e-06, 'epoch': 1.1422222222222222}
Step 25800: {'loss': 1.6336, 'grad_norm': 6.309203147888184, 'learning_rate': 7.707911111111112e-06, 'epoch': 1.1466666666666667}
Step 25900: {'loss': 1.605, 'grad_norm': 7.88905668258667, 'learning_rate': 7.699022222222223e-06, 'epoch': 1.1511111111111112}
Step 26000: {'loss': 1.5872, 'grad_norm': 8.97355842590332, 'learning_rate': 7.690133333333335e-06, 'epoch': 1.1555555555555554}
Step 26100: {'loss': 1.5659, 'grad_norm': 7.422603130340576, 'learning_rate': 7.681244444444446e-06, 'epoch': 1.16}
Step 26200: {'loss': 1.6106, 'grad_norm': 6.681830406188965, 'learning_rate': 7.672355555555556e-06, 'epoch': 1.1644444444444444}
Step 26300: {'loss': 1.5434, 'grad_norm': 7.6613287925720215, 'learning_rate': 7.663555555555557e-06, 'epoch': 1.1688888888888889}
Step 26400: {'loss': 1.5475, 'grad_norm': 7.962862014770508, 'learning_rate': 7.654666666666666e-06, 'epoch': 1.1733333333333333}
Step 26500: {'loss': 1.5734, 'grad_norm': 6.370100975036621, 'learning_rate': 7.64577777777778e-06, 'epoch': 1.1777777777777778}
Step 26600: {'loss': 1.5713, 'grad_norm': 8.556066513061523, 'learning_rate': 7.636888888888889e-06, 'epoch': 1.1822222222222223}
Step 26700: {'loss': 1.5863, 'grad_norm': 5.866230010986328, 'learning_rate': 7.628000000000001e-06, 'epoch': 1.1866666666666668}
Step 26800: {'loss': 1.5814, 'grad_norm': 7.438526630401611, 'learning_rate': 7.619111111111112e-06, 'epoch': 1.1911111111111112}
Step 26900: {'loss': 1.578, 'grad_norm': 7.702383041381836, 'learning_rate': 7.610222222222223e-06, 'epoch': 1.1955555555555555}
Step 27000: {'loss': 1.5713, 'grad_norm': 9.642817497253418, 'learning_rate': 7.601333333333334e-06, 'epoch': 1.2}
Step 27100: {'loss': 1.5731, 'grad_norm': 5.6157307624816895, 'learning_rate': 7.592444444444446e-06, 'epoch': 1.2044444444444444}
Step 27200: {'loss': 1.6198, 'grad_norm': 7.1586012840271, 'learning_rate': 7.5835555555555566e-06, 'epoch': 1.208888888888889}
Step 27300: {'loss': 1.5852, 'grad_norm': 6.425991535186768, 'learning_rate': 7.574666666666667e-06, 'epoch': 1.2133333333333334}
Step 27400: {'loss': 1.6167, 'grad_norm': 6.6092848777771, 'learning_rate': 7.5657777777777785e-06, 'epoch': 1.2177777777777778}
Step 27500: {'loss': 1.5545, 'grad_norm': 6.2680511474609375, 'learning_rate': 7.556888888888889e-06, 'epoch': 1.2222222222222223}
Step 27600: {'loss': 1.5651, 'grad_norm': 8.78457260131836, 'learning_rate': 7.548000000000001e-06, 'epoch': 1.2266666666666666}
Step 27700: {'loss': 1.5818, 'grad_norm': 6.5936479568481445, 'learning_rate': 7.539111111111112e-06, 'epoch': 1.231111111111111}
Step 27800: {'loss': 1.6, 'grad_norm': 7.897549629211426, 'learning_rate': 7.530222222222223e-06, 'epoch': 1.2355555555555555}
Step 27900: {'loss': 1.5867, 'grad_norm': 8.465332984924316, 'learning_rate': 7.521333333333334e-06, 'epoch': 1.24}
Step 28000: {'loss': 1.5562, 'grad_norm': 5.552760601043701, 'learning_rate': 7.512444444444446e-06, 'epoch': 1.2444444444444445}
Step 28100: {'loss': 1.5688, 'grad_norm': 6.7730937004089355, 'learning_rate': 7.5035555555555565e-06, 'epoch': 1.248888888888889}
Step 28200: {'loss': 1.5848, 'grad_norm': 7.173166751861572, 'learning_rate': 7.494666666666667e-06, 'epoch': 1.2533333333333334}
Step 28300: {'loss': 1.55, 'grad_norm': 7.592113494873047, 'learning_rate': 7.485777777777778e-06, 'epoch': 1.2577777777777777}
Step 28400: {'loss': 1.5817, 'grad_norm': 6.1944355964660645, 'learning_rate': 7.476888888888889e-06, 'epoch': 1.2622222222222224}
Step 28500: {'loss': 1.5578, 'grad_norm': 7.807333469390869, 'learning_rate': 7.468000000000001e-06, 'epoch': 1.2666666666666666}
Step 28600: {'loss': 1.5804, 'grad_norm': 6.674444675445557, 'learning_rate': 7.459111111111112e-06, 'epoch': 1.271111111111111}
Step 28700: {'loss': 1.6111, 'grad_norm': 5.766730785369873, 'learning_rate': 7.450222222222223e-06, 'epoch': 1.2755555555555556}
Step 28800: {'loss': 1.552, 'grad_norm': 6.227319240570068, 'learning_rate': 7.441333333333334e-06, 'epoch': 1.28}
Step 28900: {'loss': 1.5889, 'grad_norm': 6.967386722564697, 'learning_rate': 7.432444444444446e-06, 'epoch': 1.2844444444444445}
Step 29000: {'loss': 1.5652, 'grad_norm': 7.051300048828125, 'learning_rate': 7.423555555555556e-06, 'epoch': 1.2888888888888888}
Step 29100: {'loss': 1.5643, 'grad_norm': 6.620711803436279, 'learning_rate': 7.414755555555556e-06, 'epoch': 1.2933333333333334}
Step 29200: {'loss': 1.5509, 'grad_norm': 6.74252986907959, 'learning_rate': 7.405866666666667e-06, 'epoch': 1.2977777777777777}
Step 29300: {'loss': 1.5553, 'grad_norm': 5.365945339202881, 'learning_rate': 7.3969777777777785e-06, 'epoch': 1.3022222222222222}
Step 29400: {'loss': 1.5914, 'grad_norm': 6.520283222198486, 'learning_rate': 7.388088888888889e-06, 'epoch': 1.3066666666666666}
Step 29500: {'loss': 1.6039, 'grad_norm': 6.955718994140625, 'learning_rate': 7.3792000000000004e-06, 'epoch': 1.3111111111111111}
Step 29600: {'loss': 1.5875, 'grad_norm': 8.127215385437012, 'learning_rate': 7.370311111111112e-06, 'epoch': 1.3155555555555556}
Step 29700: {'loss': 1.5699, 'grad_norm': 5.897741794586182, 'learning_rate': 7.361422222222223e-06, 'epoch': 1.32}
Step 29800: {'loss': 1.5842, 'grad_norm': 6.550039768218994, 'learning_rate': 7.352533333333334e-06, 'epoch': 1.3244444444444445}
Step 29900: {'loss': 1.5728, 'grad_norm': 6.0433526039123535, 'learning_rate': 7.343644444444445e-06, 'epoch': 1.3288888888888888}
Step 30000: {'loss': 1.5829, 'grad_norm': 6.584390640258789, 'learning_rate': 7.334755555555556e-06, 'epoch': 1.3333333333333333}
Step 30100: {'loss': 1.6131, 'grad_norm': 5.653693199157715, 'learning_rate': 7.325866666666668e-06, 'epoch': 1.3377777777777777}
Step 30200: {'loss': 1.5999, 'grad_norm': 6.434367656707764, 'learning_rate': 7.3169777777777784e-06, 'epoch': 1.3422222222222222}
Step 30300: {'loss': 1.5595, 'grad_norm': 6.658377170562744, 'learning_rate': 7.308088888888889e-06, 'epoch': 1.3466666666666667}
Step 30400: {'loss': 1.5649, 'grad_norm': 9.357048034667969, 'learning_rate': 7.2992e-06, 'epoch': 1.3511111111111112}
Step 30500: {'loss': 1.5559, 'grad_norm': 5.666801452636719, 'learning_rate': 7.290311111111112e-06, 'epoch': 1.3555555555555556}
Step 30600: {'loss': 1.564, 'grad_norm': 6.70497989654541, 'learning_rate': 7.281422222222223e-06, 'epoch': 1.3599999999999999}
Step 30700: {'loss': 1.5569, 'grad_norm': 6.274736404418945, 'learning_rate': 7.272533333333334e-06, 'epoch': 1.3644444444444446}
Step 30800: {'loss': 1.5487, 'grad_norm': 9.971829414367676, 'learning_rate': 7.263644444444445e-06, 'epoch': 1.3688888888888888}
Step 30900: {'loss': 1.5666, 'grad_norm': 6.322424411773682, 'learning_rate': 7.254755555555556e-06, 'epoch': 1.3733333333333333}
Step 31000: {'loss': 1.5603, 'grad_norm': 6.814370155334473, 'learning_rate': 7.245866666666668e-06, 'epoch': 1.3777777777777778}
Step 31100: {'loss': 1.6229, 'grad_norm': 6.601344585418701, 'learning_rate': 7.236977777777778e-06, 'epoch': 1.3822222222222222}
Step 31200: {'loss': 1.5835, 'grad_norm': 6.827740669250488, 'learning_rate': 7.228088888888889e-06, 'epoch': 1.3866666666666667}
Step 31300: {'loss': 1.578, 'grad_norm': 7.558791160583496, 'learning_rate': 7.2192e-06, 'epoch': 1.3911111111111112}
Step 31400: {'loss': 1.5725, 'grad_norm': 5.217653751373291, 'learning_rate': 7.210311111111112e-06, 'epoch': 1.3955555555555557}
Step 31500: {'loss': 1.54, 'grad_norm': 6.986949920654297, 'learning_rate': 7.201422222222223e-06, 'epoch': 1.4}
Step 31600: {'loss': 1.5485, 'grad_norm': 5.91647481918335, 'learning_rate': 7.1925333333333336e-06, 'epoch': 1.4044444444444444}
Step 31700: {'loss': 1.5616, 'grad_norm': 5.266974449157715, 'learning_rate': 7.183644444444445e-06, 'epoch': 1.4088888888888889}
Step 31800: {'loss': 1.6096, 'grad_norm': 7.154858589172363, 'learning_rate': 7.1747555555555555e-06, 'epoch': 1.4133333333333333}
Step 31900: {'loss': 1.5662, 'grad_norm': 6.170751094818115, 'learning_rate': 7.165866666666668e-06, 'epoch': 1.4177777777777778}
Step 32000: {'loss': 1.5553, 'grad_norm': 7.136101245880127, 'learning_rate': 7.156977777777778e-06, 'epoch': 1.4222222222222223}
Step 32100: {'loss': 1.5281, 'grad_norm': 5.539132595062256, 'learning_rate': 7.14808888888889e-06, 'epoch': 1.4266666666666667}
Step 32200: {'loss': 1.5778, 'grad_norm': 6.943672180175781, 'learning_rate': 7.1392e-06, 'epoch': 1.431111111111111}
Step 32300: {'loss': 1.5545, 'grad_norm': 6.080425262451172, 'learning_rate': 7.1303111111111116e-06, 'epoch': 1.4355555555555555}
Step 32400: {'loss': 1.5425, 'grad_norm': 5.968753814697266, 'learning_rate': 7.121422222222223e-06, 'epoch': 1.44}
Step 32500: {'loss': 1.55, 'grad_norm': 6.38734769821167, 'learning_rate': 7.1125333333333335e-06, 'epoch': 1.4444444444444444}
Step 32600: {'loss': 1.5813, 'grad_norm': 4.33803129196167, 'learning_rate': 7.103644444444445e-06, 'epoch': 1.448888888888889}
Step 32700: {'loss': 1.5106, 'grad_norm': 8.313152313232422, 'learning_rate': 7.094755555555555e-06, 'epoch': 1.4533333333333334}
Step 32800: {'loss': 1.5777, 'grad_norm': 6.967814922332764, 'learning_rate': 7.085955555555556e-06, 'epoch': 1.4577777777777778}
Step 32900: {'loss': 1.5416, 'grad_norm': 6.239413261413574, 'learning_rate': 7.077066666666668e-06, 'epoch': 1.462222222222222}
Step 33000: {'loss': 1.56, 'grad_norm': 6.497992992401123, 'learning_rate': 7.068177777777778e-06, 'epoch': 1.4666666666666668}
Step 33100: {'loss': 1.5323, 'grad_norm': 6.4183149337768555, 'learning_rate': 7.05928888888889e-06, 'epoch': 1.471111111111111}
Step 33200: {'loss': 1.5456, 'grad_norm': 5.3181257247924805, 'learning_rate': 7.0504e-06, 'epoch': 1.4755555555555555}
Step 33300: {'loss': 1.5484, 'grad_norm': 5.739299297332764, 'learning_rate': 7.041511111111111e-06, 'epoch': 1.48}
Step 33400: {'loss': 1.5562, 'grad_norm': 6.727297306060791, 'learning_rate': 7.032622222222223e-06, 'epoch': 1.4844444444444445}
Step 33500: {'loss': 1.5787, 'grad_norm': 6.157293796539307, 'learning_rate': 7.023733333333334e-06, 'epoch': 1.488888888888889}
Step 33600: {'loss': 1.6031, 'grad_norm': 5.88239049911499, 'learning_rate': 7.014844444444445e-06, 'epoch': 1.4933333333333334}
Step 33700: {'loss': 1.5574, 'grad_norm': 6.434669494628906, 'learning_rate': 7.0059555555555555e-06, 'epoch': 1.4977777777777779}
Step 33800: {'loss': 1.5588, 'grad_norm': 5.680057048797607, 'learning_rate': 6.997066666666668e-06, 'epoch': 1.5022222222222221}
Step 33900: {'loss': 1.559, 'grad_norm': 7.05838680267334, 'learning_rate': 6.988177777777778e-06, 'epoch': 1.5066666666666668}
Step 34000: {'loss': 1.539, 'grad_norm': 7.737556457519531, 'learning_rate': 6.97928888888889e-06, 'epoch': 1.511111111111111}
Step 34100: {'loss': 1.5851, 'grad_norm': 8.207597732543945, 'learning_rate': 6.9704e-06, 'epoch': 1.5155555555555555}
Step 34200: {'loss': 1.561, 'grad_norm': 6.0109171867370605, 'learning_rate': 6.9615111111111124e-06, 'epoch': 1.52}
Step 34300: {'loss': 1.585, 'grad_norm': 7.500855922698975, 'learning_rate': 6.952622222222223e-06, 'epoch': 1.5244444444444445}
Step 34400: {'loss': 1.5403, 'grad_norm': 7.488315105438232, 'learning_rate': 6.9437333333333335e-06, 'epoch': 1.528888888888889}
Step 34500: {'loss': 1.5501, 'grad_norm': 5.369443416595459, 'learning_rate': 6.934844444444445e-06, 'epoch': 1.5333333333333332}
Step 34600: {'loss': 1.5545, 'grad_norm': 5.619010925292969, 'learning_rate': 6.9259555555555554e-06, 'epoch': 1.537777777777778}
Step 34700: {'loss': 1.5773, 'grad_norm': 7.500516414642334, 'learning_rate': 6.917066666666668e-06, 'epoch': 1.5422222222222222}
Step 34800: {'loss': 1.5823, 'grad_norm': 7.055943965911865, 'learning_rate': 6.908177777777778e-06, 'epoch': 1.5466666666666666}
Step 34900: {'loss': 1.525, 'grad_norm': 6.805295944213867, 'learning_rate': 6.89928888888889e-06, 'epoch': 1.551111111111111}
Step 35000: {'loss': 1.5738, 'grad_norm': 7.62284517288208, 'learning_rate': 6.8904e-06, 'epoch': 1.5555555555555556}
Step 35100: {'loss': 1.5565, 'grad_norm': 8.081843376159668, 'learning_rate': 6.881511111111112e-06, 'epoch': 1.56}
Step 35200: {'loss': 1.5247, 'grad_norm': 7.289237976074219, 'learning_rate': 6.872622222222223e-06, 'epoch': 1.5644444444444443}
Step 35300: {'loss': 1.5626, 'grad_norm': 6.2406325340271, 'learning_rate': 6.8637333333333334e-06, 'epoch': 1.568888888888889}
Step 35400: {'loss': 1.5523, 'grad_norm': 6.458880424499512, 'learning_rate': 6.854844444444445e-06, 'epoch': 1.5733333333333333}
Step 35500: {'loss': 1.5772, 'grad_norm': 6.466349124908447, 'learning_rate': 6.845955555555555e-06, 'epoch': 1.5777777777777777}
Step 35600: {'loss': 1.5376, 'grad_norm': 6.788664817810059, 'learning_rate': 6.837066666666668e-06, 'epoch': 1.5822222222222222}
Step 35700: {'loss': 1.539, 'grad_norm': 6.737133979797363, 'learning_rate': 6.828177777777778e-06, 'epoch': 1.5866666666666667}
Step 35800: {'loss': 1.5327, 'grad_norm': 5.336559295654297, 'learning_rate': 6.8192888888888895e-06, 'epoch': 1.5911111111111111}
Step 35900: {'loss': 1.5263, 'grad_norm': 5.321359634399414, 'learning_rate': 6.8104e-06, 'epoch': 1.5955555555555554}
Step 36000: {'loss': 1.5506, 'grad_norm': 6.8283610343933105, 'learning_rate': 6.801511111111112e-06, 'epoch': 1.6}
Step 36100: {'loss': 1.5363, 'grad_norm': 7.578571796417236, 'learning_rate': 6.792622222222223e-06, 'epoch': 1.6044444444444443}
Step 36200: {'loss': 1.5503, 'grad_norm': 5.720787048339844, 'learning_rate': 6.783733333333334e-06, 'epoch': 1.608888888888889}
Step 36300: {'loss': 1.5626, 'grad_norm': 6.102880954742432, 'learning_rate': 6.774844444444445e-06, 'epoch': 1.6133333333333333}
Step 36400: {'loss': 1.5579, 'grad_norm': 5.9833903312683105, 'learning_rate': 6.765955555555555e-06, 'epoch': 1.6177777777777778}
Step 36500: {'loss': 1.5603, 'grad_norm': 7.473219394683838, 'learning_rate': 6.7570666666666675e-06, 'epoch': 1.6222222222222222}
Step 36600: {'loss': 1.5786, 'grad_norm': 6.983191013336182, 'learning_rate': 6.748177777777778e-06, 'epoch': 1.6266666666666667}
Step 36700: {'loss': 1.5514, 'grad_norm': 5.613826751708984, 'learning_rate': 6.7392888888888894e-06, 'epoch': 1.6311111111111112}
Step 36800: {'loss': 1.5458, 'grad_norm': 6.485313415527344, 'learning_rate': 6.73048888888889e-06, 'epoch': 1.6355555555555554}
Step 36900: {'loss': 1.5597, 'grad_norm': 5.577081203460693, 'learning_rate': 6.7216e-06, 'epoch': 1.6400000000000001}
Step 37000: {'loss': 1.5527, 'grad_norm': 5.367799282073975, 'learning_rate': 6.712711111111112e-06, 'epoch': 1.6444444444444444}
Step 37100: {'loss': 1.5533, 'grad_norm': 7.0143961906433105, 'learning_rate': 6.703822222222223e-06, 'epoch': 1.6488888888888888}
Step 37200: {'loss': 1.5471, 'grad_norm': 6.12742805480957, 'learning_rate': 6.694933333333334e-06, 'epoch': 1.6533333333333333}
Step 37300: {'loss': 1.5363, 'grad_norm': 5.090803623199463, 'learning_rate': 6.686044444444445e-06, 'epoch': 1.6577777777777778}
Step 37400: {'loss': 1.5372, 'grad_norm': 6.5833210945129395, 'learning_rate': 6.677155555555555e-06, 'epoch': 1.6622222222222223}
Step 37500: {'loss': 1.5782, 'grad_norm': 7.243375778198242, 'learning_rate': 6.668266666666668e-06, 'epoch': 1.6666666666666665}
Step 37600: {'loss': 1.5683, 'grad_norm': 6.436271667480469, 'learning_rate': 6.659377777777778e-06, 'epoch': 1.6711111111111112}
Step 37700: {'loss': 1.5602, 'grad_norm': 7.731805324554443, 'learning_rate': 6.6504888888888895e-06, 'epoch': 1.6755555555555555}
Step 37800: {'loss': 1.567, 'grad_norm': 6.2165069580078125, 'learning_rate': 6.6416e-06, 'epoch': 1.6800000000000002}
Step 37900: {'loss': 1.5725, 'grad_norm': 6.394110202789307, 'learning_rate': 6.632711111111112e-06, 'epoch': 1.6844444444444444}
Step 38000: {'loss': 1.541, 'grad_norm': 7.059322834014893, 'learning_rate': 6.623822222222223e-06, 'epoch': 1.6888888888888889}
Step 38100: {'loss': 1.5466, 'grad_norm': 6.320363521575928, 'learning_rate': 6.614933333333334e-06, 'epoch': 1.6933333333333334}
Step 38200: {'loss': 1.5489, 'grad_norm': 6.114841461181641, 'learning_rate': 6.606044444444445e-06, 'epoch': 1.6977777777777778}
Step 38300: {'loss': 1.5424, 'grad_norm': 4.770445346832275, 'learning_rate': 6.597155555555557e-06, 'epoch': 1.7022222222222223}
Step 38400: {'loss': 1.53, 'grad_norm': 6.0331573486328125, 'learning_rate': 6.5882666666666675e-06, 'epoch': 1.7066666666666666}
Step 38500: {'loss': 1.5647, 'grad_norm': 9.305363655090332, 'learning_rate': 6.579377777777778e-06, 'epoch': 1.7111111111111112}
Step 38600: {'loss': 1.5403, 'grad_norm': 6.168186664581299, 'learning_rate': 6.5704888888888895e-06, 'epoch': 1.7155555555555555}
Step 38700: {'loss': 1.5314, 'grad_norm': 6.841040134429932, 'learning_rate': 6.5616e-06, 'epoch': 1.72}
Step 38800: {'loss': 1.534, 'grad_norm': 7.697948932647705, 'learning_rate': 6.5528e-06, 'epoch': 1.7244444444444444}
Step 38900: {'loss': 1.5652, 'grad_norm': 6.8662109375, 'learning_rate': 6.543911111111112e-06, 'epoch': 1.728888888888889}
Step 39000: {'loss': 1.5057, 'grad_norm': 5.625552177429199, 'learning_rate': 6.535022222222223e-06, 'epoch': 1.7333333333333334}
Step 39100: {'loss': 1.5351, 'grad_norm': 6.5353684425354, 'learning_rate': 6.526222222222222e-06, 'epoch': 1.7377777777777776}
Step 39200: {'loss': 1.5665, 'grad_norm': 6.047280788421631, 'learning_rate': 6.5173333333333345e-06, 'epoch': 1.7422222222222223}
Step 39300: {'loss': 1.5289, 'grad_norm': 6.906128883361816, 'learning_rate': 6.508444444444445e-06, 'epoch': 1.7466666666666666}
Step 39400: {'loss': 1.5863, 'grad_norm': 6.388877868652344, 'learning_rate': 6.499555555555556e-06, 'epoch': 1.751111111111111}
Step 39500: {'loss': 1.5643, 'grad_norm': 5.474090099334717, 'learning_rate': 6.490666666666667e-06, 'epoch': 1.7555555555555555}
Step 39600: {'loss': 1.5356, 'grad_norm': 7.389440536499023, 'learning_rate': 6.481777777777778e-06, 'epoch': 1.76}
Step 39700: {'loss': 1.5477, 'grad_norm': 5.59758186340332, 'learning_rate': 6.47288888888889e-06, 'epoch': 1.7644444444444445}
Step 39800: {'loss': 1.5459, 'grad_norm': 6.840673923492432, 'learning_rate': 6.464e-06, 'epoch': 1.7688888888888887}
Step 39900: {'loss': 1.5159, 'grad_norm': 5.078192710876465, 'learning_rate': 6.455111111111112e-06, 'epoch': 1.7733333333333334}
Step 40000: {'loss': 1.4991, 'grad_norm': 5.31404447555542, 'learning_rate': 6.446222222222222e-06, 'epoch': 1.7777777777777777}
Step 40100: {'loss': 1.5362, 'grad_norm': 5.993261814117432, 'learning_rate': 6.4373333333333344e-06, 'epoch': 1.7822222222222224}
Step 40200: {'loss': 1.5676, 'grad_norm': 7.2932209968566895, 'learning_rate': 6.428444444444445e-06, 'epoch': 1.7866666666666666}
Step 40300: {'loss': 1.531, 'grad_norm': 5.164085865020752, 'learning_rate': 6.4195555555555555e-06, 'epoch': 1.791111111111111}
Step 40400: {'loss': 1.5738, 'grad_norm': 5.127695083618164, 'learning_rate': 6.410666666666667e-06, 'epoch': 1.7955555555555556}
Step 40500: {'loss': 1.5634, 'grad_norm': 5.7883381843566895, 'learning_rate': 6.401777777777778e-06, 'epoch': 1.8}
Step 40600: {'loss': 1.5391, 'grad_norm': 5.36130428314209, 'learning_rate': 6.39288888888889e-06, 'epoch': 1.8044444444444445}
Step 40700: {'loss': 1.5582, 'grad_norm': 5.166922569274902, 'learning_rate': 6.384e-06, 'epoch': 1.8088888888888888}
Step 40800: {'loss': 1.5899, 'grad_norm': 6.645636558532715, 'learning_rate': 6.375111111111112e-06, 'epoch': 1.8133333333333335}
Step 40900: {'loss': 1.5534, 'grad_norm': 9.616236686706543, 'learning_rate': 6.366222222222222e-06, 'epoch': 1.8177777777777777}
Step 41000: {'loss': 1.4988, 'grad_norm': 5.828885555267334, 'learning_rate': 6.357333333333334e-06, 'epoch': 1.8222222222222222}
Step 41100: {'loss': 1.5247, 'grad_norm': 6.240208148956299, 'learning_rate': 6.348444444444445e-06, 'epoch': 1.8266666666666667}
Step 41200: {'loss': 1.5112, 'grad_norm': 6.392978668212891, 'learning_rate': 6.339555555555556e-06, 'epoch': 1.8311111111111111}
Step 41300: {'loss': 1.5321, 'grad_norm': 6.465495586395264, 'learning_rate': 6.330666666666667e-06, 'epoch': 1.8355555555555556}
Step 41400: {'loss': 1.5183, 'grad_norm': 5.3829545974731445, 'learning_rate': 6.321777777777778e-06, 'epoch': 1.8399999999999999}
Step 41500: {'loss': 1.5791, 'grad_norm': 5.177002429962158, 'learning_rate': 6.31288888888889e-06, 'epoch': 1.8444444444444446}
Step 41600: {'loss': 1.5241, 'grad_norm': 7.172863006591797, 'learning_rate': 6.304e-06, 'epoch': 1.8488888888888888}
Step 41700: {'loss': 1.5401, 'grad_norm': 6.301916122436523, 'learning_rate': 6.2951111111111115e-06, 'epoch': 1.8533333333333335}
Step 41800: {'loss': 1.5521, 'grad_norm': 5.780455112457275, 'learning_rate': 6.286222222222222e-06, 'epoch': 1.8577777777777778}
Step 41900: {'loss': 1.5197, 'grad_norm': 6.262840747833252, 'learning_rate': 6.277333333333334e-06, 'epoch': 1.8622222222222222}
Step 42000: {'loss': 1.5505, 'grad_norm': 6.059311866760254, 'learning_rate': 6.268444444444445e-06, 'epoch': 1.8666666666666667}
Step 42100: {'loss': 1.5614, 'grad_norm': 10.029783248901367, 'learning_rate': 6.259555555555556e-06, 'epoch': 1.871111111111111}
Step 42200: {'loss': 1.5308, 'grad_norm': 6.964897632598877, 'learning_rate': 6.250666666666667e-06, 'epoch': 1.8755555555555556}
Step 42300: {'loss': 1.57, 'grad_norm': 6.976278305053711, 'learning_rate': 6.241777777777779e-06, 'epoch': 1.88}
Step 42400: {'loss': 1.5103, 'grad_norm': 6.4942426681518555, 'learning_rate': 6.2328888888888895e-06, 'epoch': 1.8844444444444446}
Step 42500: {'loss': 1.526, 'grad_norm': 4.971755504608154, 'learning_rate': 6.224e-06, 'epoch': 1.8888888888888888}
Step 42600: {'loss': 1.5287, 'grad_norm': 6.475659370422363, 'learning_rate': 6.2151111111111114e-06, 'epoch': 1.8933333333333333}
Step 42700: {'loss': 1.5409, 'grad_norm': 5.581170558929443, 'learning_rate': 6.206222222222222e-06, 'epoch': 1.8977777777777778}
Step 42800: {'loss': 1.5251, 'grad_norm': 6.325690269470215, 'learning_rate': 6.197333333333334e-06, 'epoch': 1.9022222222222223}
Step 42900: {'loss': 1.5531, 'grad_norm': 6.605616092681885, 'learning_rate': 6.188444444444445e-06, 'epoch': 1.9066666666666667}
Step 43000: {'loss': 1.5087, 'grad_norm': 5.894525527954102, 'learning_rate': 6.179555555555556e-06, 'epoch': 1.911111111111111}
Step 43100: {'loss': 1.5605, 'grad_norm': 4.477473735809326, 'learning_rate': 6.170755555555556e-06, 'epoch': 1.9155555555555557}
Step 43200: {'loss': 1.5423, 'grad_norm': 4.9798970222473145, 'learning_rate': 6.161866666666667e-06, 'epoch': 1.92}
Step 43300: {'loss': 1.5377, 'grad_norm': 6.738548278808594, 'learning_rate': 6.152977777777779e-06, 'epoch': 1.9244444444444444}
Step 43400: {'loss': 1.5294, 'grad_norm': 7.108492851257324, 'learning_rate': 6.14408888888889e-06, 'epoch': 1.9288888888888889}
Step 43500: {'loss': 1.5418, 'grad_norm': 5.5898542404174805, 'learning_rate': 6.1352e-06, 'epoch': 1.9333333333333333}
Step 43600: {'loss': 1.5341, 'grad_norm': 6.892210006713867, 'learning_rate': 6.1263111111111115e-06, 'epoch': 1.9377777777777778}
Step 43700: {'loss': 1.5425, 'grad_norm': 7.572237968444824, 'learning_rate': 6.117422222222222e-06, 'epoch': 1.942222222222222}
Step 43800: {'loss': 1.5406, 'grad_norm': 6.740200042724609, 'learning_rate': 6.108533333333334e-06, 'epoch': 1.9466666666666668}
Step 43900: {'loss': 1.5526, 'grad_norm': 6.465364456176758, 'learning_rate': 6.099644444444445e-06, 'epoch': 1.951111111111111}
Step 44000: {'loss': 1.5347, 'grad_norm': 5.755325794219971, 'learning_rate': 6.090755555555556e-06, 'epoch': 1.9555555555555557}
Step 44100: {'loss': 1.5057, 'grad_norm': 6.352814197540283, 'learning_rate': 6.081866666666667e-06, 'epoch': 1.96}
Step 44200: {'loss': 1.52, 'grad_norm': 4.7976837158203125, 'learning_rate': 6.072977777777779e-06, 'epoch': 1.9644444444444444}
Step 44300: {'loss': 1.5341, 'grad_norm': 6.730218410491943, 'learning_rate': 6.0640888888888895e-06, 'epoch': 1.968888888888889}
Step 44400: {'loss': 1.5395, 'grad_norm': 4.92548131942749, 'learning_rate': 6.0552e-06, 'epoch': 1.9733333333333334}
Step 44500: {'loss': 1.5253, 'grad_norm': 6.754398345947266, 'learning_rate': 6.0463111111111114e-06, 'epoch': 1.9777777777777779}
Step 44600: {'loss': 1.5546, 'grad_norm': 6.0117950439453125, 'learning_rate': 6.037422222222222e-06, 'epoch': 1.982222222222222}
Step 44700: {'loss': 1.53, 'grad_norm': 5.459531784057617, 'learning_rate': 6.028533333333334e-06, 'epoch': 1.9866666666666668}
Step 44800: {'loss': 1.5319, 'grad_norm': 5.08430814743042, 'learning_rate': 6.019644444444445e-06, 'epoch': 1.991111111111111}
Step 44900: {'loss': 1.5121, 'grad_norm': 5.2377142906188965, 'learning_rate': 6.010755555555556e-06, 'epoch': 1.9955555555555555}
Step 45000: {'loss': 1.5352, 'grad_norm': 7.757847309112549, 'learning_rate': 6.001866666666667e-06, 'epoch': 2.0}
Step 45100: {'loss': 1.5343, 'grad_norm': 4.761312961578369, 'learning_rate': 5.993066666666667e-06, 'epoch': 2.0044444444444443}
Step 45200: {'loss': 1.5171, 'grad_norm': 5.3049750328063965, 'learning_rate': 5.984177777777779e-06, 'epoch': 2.008888888888889}
Step 45300: {'loss': 1.5492, 'grad_norm': 5.489744186401367, 'learning_rate': 5.97528888888889e-06, 'epoch': 2.013333333333333}
Step 45400: {'loss': 1.5266, 'grad_norm': 5.991397857666016, 'learning_rate': 5.9664e-06, 'epoch': 2.017777777777778}
Step 45500: {'loss': 1.5515, 'grad_norm': 5.723753929138184, 'learning_rate': 5.9575111111111116e-06, 'epoch': 2.022222222222222}
Step 45600: {'loss': 1.5343, 'grad_norm': 6.430820941925049, 'learning_rate': 5.948622222222222e-06, 'epoch': 2.026666666666667}
Step 45700: {'loss': 1.5243, 'grad_norm': 6.468318462371826, 'learning_rate': 5.939733333333334e-06, 'epoch': 2.031111111111111}
Step 45800: {'loss': 1.5342, 'grad_norm': 6.821277618408203, 'learning_rate': 5.930844444444445e-06, 'epoch': 2.0355555555555553}
Step 45900: {'loss': 1.5762, 'grad_norm': 4.814772605895996, 'learning_rate': 5.921955555555556e-06, 'epoch': 2.04}
Step 46000: {'loss': 1.5057, 'grad_norm': 5.6122660636901855, 'learning_rate': 5.913066666666667e-06, 'epoch': 2.0444444444444443}
Step 46100: {'loss': 1.5297, 'grad_norm': 8.780856132507324, 'learning_rate': 5.904177777777779e-06, 'epoch': 2.048888888888889}
Step 46200: {'loss': 1.5138, 'grad_norm': 6.174585342407227, 'learning_rate': 5.8952888888888896e-06, 'epoch': 2.0533333333333332}
Step 46300: {'loss': 1.4987, 'grad_norm': 5.936131954193115, 'learning_rate': 5.886400000000001e-06, 'epoch': 2.057777777777778}
Step 46400: {'loss': 1.5033, 'grad_norm': 7.0469279289245605, 'learning_rate': 5.8775111111111115e-06, 'epoch': 2.062222222222222}
Step 46500: {'loss': 1.5009, 'grad_norm': 8.529433250427246, 'learning_rate': 5.868622222222222e-06, 'epoch': 2.066666666666667}
Step 46600: {'loss': 1.5229, 'grad_norm': 5.265646934509277, 'learning_rate': 5.859733333333334e-06, 'epoch': 2.071111111111111}
Step 46700: {'loss': 1.4916, 'grad_norm': 7.464733123779297, 'learning_rate': 5.850844444444445e-06, 'epoch': 2.0755555555555554}
Step 46800: {'loss': 1.5157, 'grad_norm': 6.116762638092041, 'learning_rate': 5.841955555555556e-06, 'epoch': 2.08}
Step 46900: {'loss': 1.5438, 'grad_norm': 6.834920883178711, 'learning_rate': 5.833066666666667e-06, 'epoch': 2.0844444444444443}
Step 47000: {'loss': 1.5595, 'grad_norm': 6.78181266784668, 'learning_rate': 5.824177777777779e-06, 'epoch': 2.088888888888889}
Step 47100: {'loss': 1.514, 'grad_norm': 5.6565351486206055, 'learning_rate': 5.815377777777778e-06, 'epoch': 2.0933333333333333}
Step 47200: {'loss': 1.5115, 'grad_norm': 5.9250383377075195, 'learning_rate': 5.80648888888889e-06, 'epoch': 2.097777777777778}
Step 47300: {'loss': 1.5066, 'grad_norm': 5.671351432800293, 'learning_rate': 5.797600000000001e-06, 'epoch': 2.102222222222222}
Step 47400: {'loss': 1.5416, 'grad_norm': 5.357291221618652, 'learning_rate': 5.788711111111112e-06, 'epoch': 2.1066666666666665}
Step 47500: {'loss': 1.5185, 'grad_norm': 6.848896026611328, 'learning_rate': 5.779822222222222e-06, 'epoch': 2.111111111111111}
Step 47600: {'loss': 1.4997, 'grad_norm': 6.10288143157959, 'learning_rate': 5.770933333333334e-06, 'epoch': 2.1155555555555554}
Step 47700: {'loss': 1.5188, 'grad_norm': 7.043394088745117, 'learning_rate': 5.762044444444445e-06, 'epoch': 2.12}
Step 47800: {'loss': 1.4818, 'grad_norm': 6.95942497253418, 'learning_rate': 5.753155555555556e-06, 'epoch': 2.1244444444444444}
Step 47900: {'loss': 1.5059, 'grad_norm': 5.96002197265625, 'learning_rate': 5.744266666666667e-06, 'epoch': 2.128888888888889}
Step 48000: {'loss': 1.5021, 'grad_norm': 6.679205417633057, 'learning_rate': 5.735377777777778e-06, 'epoch': 2.1333333333333333}
Step 48100: {'loss': 1.4924, 'grad_norm': 4.740731716156006, 'learning_rate': 5.7264888888888896e-06, 'epoch': 2.137777777777778}
Step 48200: {'loss': 1.4968, 'grad_norm': 6.495748996734619, 'learning_rate': 5.717600000000001e-06, 'epoch': 2.1422222222222222}
Step 48300: {'loss': 1.5304, 'grad_norm': 5.082347393035889, 'learning_rate': 5.7087111111111115e-06, 'epoch': 2.1466666666666665}
Step 48400: {'loss': 1.4815, 'grad_norm': 5.523674488067627, 'learning_rate': 5.699822222222222e-06, 'epoch': 2.151111111111111}
Step 48500: {'loss': 1.5133, 'grad_norm': 5.963528156280518, 'learning_rate': 5.690933333333334e-06, 'epoch': 2.1555555555555554}
Step 48600: {'loss': 1.5316, 'grad_norm': 7.243768215179443, 'learning_rate': 5.682044444444445e-06, 'epoch': 2.16}
Step 48700: {'loss': 1.5137, 'grad_norm': 5.850263595581055, 'learning_rate': 5.673155555555556e-06, 'epoch': 2.1644444444444444}
Step 48800: {'loss': 1.5332, 'grad_norm': 5.14383602142334, 'learning_rate': 5.664266666666667e-06, 'epoch': 2.168888888888889}
Step 48900: {'loss': 1.5417, 'grad_norm': 7.416391849517822, 'learning_rate': 5.655377777777778e-06, 'epoch': 2.1733333333333333}
Step 49000: {'loss': 1.5467, 'grad_norm': 5.645524501800537, 'learning_rate': 5.6464888888888895e-06, 'epoch': 2.1777777777777776}
Step 49100: {'loss': 1.514, 'grad_norm': 4.871496677398682, 'learning_rate': 5.637688888888889e-06, 'epoch': 2.1822222222222223}
Step 49200: {'loss': 1.5427, 'grad_norm': 5.456282138824463, 'learning_rate': 5.628800000000001e-06, 'epoch': 2.1866666666666665}
Step 49300: {'loss': 1.5123, 'grad_norm': 4.673121452331543, 'learning_rate': 5.619911111111112e-06, 'epoch': 2.1911111111111112}
Step 49400: {'loss': 1.4587, 'grad_norm': 5.322500705718994, 'learning_rate': 5.611022222222222e-06, 'epoch': 2.1955555555555555}
Step 49500: {'loss': 1.528, 'grad_norm': 5.479060649871826, 'learning_rate': 5.6021333333333335e-06, 'epoch': 2.2}
Step 49600: {'loss': 1.5057, 'grad_norm': 6.097711563110352, 'learning_rate': 5.593244444444444e-06, 'epoch': 2.2044444444444444}
Step 49700: {'loss': 1.5048, 'grad_norm': 5.178313732147217, 'learning_rate': 5.584355555555556e-06, 'epoch': 2.2088888888888887}
Step 49800: {'loss': 1.5508, 'grad_norm': 8.11757755279541, 'learning_rate': 5.575466666666667e-06, 'epoch': 2.2133333333333334}
Step 49900: {'loss': 1.5295, 'grad_norm': 5.741850852966309, 'learning_rate': 5.566577777777778e-06, 'epoch': 2.2177777777777776}
Step 50000: {'loss': 1.5598, 'grad_norm': 6.379236221313477, 'learning_rate': 5.557688888888889e-06, 'epoch': 2.2222222222222223}
Step 50100: {'loss': 1.4851, 'grad_norm': 6.768738269805908, 'learning_rate': 5.548800000000001e-06, 'epoch': 2.2266666666666666}
Step 50200: {'loss': 1.5191, 'grad_norm': 6.8568115234375, 'learning_rate': 5.5399111111111115e-06, 'epoch': 2.2311111111111113}
Step 50300: {'loss': 1.5595, 'grad_norm': 5.235949516296387, 'learning_rate': 5.531022222222223e-06, 'epoch': 2.2355555555555555}
Step 50400: {'loss': 1.5137, 'grad_norm': 4.760732173919678, 'learning_rate': 5.5221333333333334e-06, 'epoch': 2.24}
Step 50500: {'loss': 1.5049, 'grad_norm': 6.6893229484558105, 'learning_rate': 5.513244444444444e-06, 'epoch': 2.2444444444444445}
Step 50600: {'loss': 1.5121, 'grad_norm': 6.023736953735352, 'learning_rate': 5.504355555555556e-06, 'epoch': 2.2488888888888887}
Step 50700: {'loss': 1.5034, 'grad_norm': 6.089096546173096, 'learning_rate': 5.495466666666667e-06, 'epoch': 2.2533333333333334}
Step 50800: {'loss': 1.5438, 'grad_norm': 6.3143720626831055, 'learning_rate': 5.486577777777778e-06, 'epoch': 2.2577777777777777}
Step 50900: {'loss': 1.4925, 'grad_norm': 6.149752616882324, 'learning_rate': 5.477688888888889e-06, 'epoch': 2.2622222222222224}
Step 51000: {'loss': 1.5409, 'grad_norm': 6.142329216003418, 'learning_rate': 5.468800000000001e-06, 'epoch': 2.2666666666666666}
Step 51100: {'loss': 1.4973, 'grad_norm': 5.844442844390869, 'learning_rate': 5.4599111111111114e-06, 'epoch': 2.2711111111111113}
Step 51200: {'loss': 1.4943, 'grad_norm': 7.2972822189331055, 'learning_rate': 5.451111111111112e-06, 'epoch': 2.2755555555555556}
Step 51300: {'loss': 1.4793, 'grad_norm': 5.636104106903076, 'learning_rate': 5.442222222222223e-06, 'epoch': 2.2800000000000002}
Step 51400: {'loss': 1.5239, 'grad_norm': 5.185155868530273, 'learning_rate': 5.4333333333333335e-06, 'epoch': 2.2844444444444445}
Step 51500: {'loss': 1.5058, 'grad_norm': 7.224742889404297, 'learning_rate': 5.424444444444444e-06, 'epoch': 2.2888888888888888}
Step 51600: {'loss': 1.4948, 'grad_norm': 5.577940464019775, 'learning_rate': 5.415555555555556e-06, 'epoch': 2.2933333333333334}
Step 51700: {'loss': 1.5072, 'grad_norm': 4.76424503326416, 'learning_rate': 5.406666666666667e-06, 'epoch': 2.2977777777777777}
Step 51800: {'loss': 1.4853, 'grad_norm': 6.593615531921387, 'learning_rate': 5.397777777777778e-06, 'epoch': 2.3022222222222224}
Step 51900: {'loss': 1.5063, 'grad_norm': 5.591152667999268, 'learning_rate': 5.388888888888889e-06, 'epoch': 2.3066666666666666}
Step 52000: {'loss': 1.4896, 'grad_norm': 6.849484443664551, 'learning_rate': 5.380000000000001e-06, 'epoch': 2.311111111111111}
Step 52100: {'loss': 1.5006, 'grad_norm': 8.302868843078613, 'learning_rate': 5.3711111111111115e-06, 'epoch': 2.3155555555555556}
Step 52200: {'loss': 1.4979, 'grad_norm': 6.053877830505371, 'learning_rate': 5.362222222222223e-06, 'epoch': 2.32}
Step 52300: {'loss': 1.4983, 'grad_norm': 6.882979869842529, 'learning_rate': 5.3533333333333335e-06, 'epoch': 2.3244444444444445}
Step 52400: {'loss': 1.5494, 'grad_norm': 5.813206195831299, 'learning_rate': 5.344444444444446e-06, 'epoch': 2.328888888888889}
Step 52500: {'loss': 1.5221, 'grad_norm': 5.880963325500488, 'learning_rate': 5.335555555555556e-06, 'epoch': 2.3333333333333335}
Step 52600: {'loss': 1.4977, 'grad_norm': 5.244113445281982, 'learning_rate': 5.326666666666667e-06, 'epoch': 2.3377777777777777}
Step 52700: {'loss': 1.4919, 'grad_norm': 5.471473693847656, 'learning_rate': 5.317777777777778e-06, 'epoch': 2.3422222222222224}
Step 52800: {'loss': 1.5121, 'grad_norm': 5.4229021072387695, 'learning_rate': 5.308888888888889e-06, 'epoch': 2.3466666666666667}
Step 52900: {'loss': 1.5621, 'grad_norm': 6.466554164886475, 'learning_rate': 5.300000000000001e-06, 'epoch': 2.351111111111111}
Step 53000: {'loss': 1.4929, 'grad_norm': 5.774801254272461, 'learning_rate': 5.2911111111111115e-06, 'epoch': 2.3555555555555556}
Step 53100: {'loss': 1.514, 'grad_norm': 5.282663822174072, 'learning_rate': 5.282222222222223e-06, 'epoch': 2.36}
Step 53200: {'loss': 1.5295, 'grad_norm': 7.818274021148682, 'learning_rate': 5.273422222222223e-06, 'epoch': 2.3644444444444446}
Step 53300: {'loss': 1.49, 'grad_norm': 5.880062580108643, 'learning_rate': 5.2645333333333336e-06, 'epoch': 2.368888888888889}
Step 53400: {'loss': 1.4812, 'grad_norm': 6.674001216888428, 'learning_rate': 5.255644444444444e-06, 'epoch': 2.3733333333333335}
Step 53500: {'loss': 1.5146, 'grad_norm': 8.164111137390137, 'learning_rate': 5.246755555555556e-06, 'epoch': 2.3777777777777778}
Step 53600: {'loss': 1.5362, 'grad_norm': 7.0122270584106445, 'learning_rate': 5.237866666666667e-06, 'epoch': 2.3822222222222225}
Step 53700: {'loss': 1.5023, 'grad_norm': 5.460092544555664, 'learning_rate': 5.228977777777778e-06, 'epoch': 2.3866666666666667}
Step 53800: {'loss': 1.4976, 'grad_norm': 5.112514019012451, 'learning_rate': 5.220088888888889e-06, 'epoch': 2.391111111111111}
Step 53900: {'loss': 1.4961, 'grad_norm': 5.488147258758545, 'learning_rate': 5.211200000000001e-06, 'epoch': 2.3955555555555557}
Step 54000: {'loss': 1.4827, 'grad_norm': 5.504265785217285, 'learning_rate': 5.2023111111111116e-06, 'epoch': 2.4}
Step 54100: {'loss': 1.4632, 'grad_norm': 6.506826400756836, 'learning_rate': 5.193422222222223e-06, 'epoch': 2.4044444444444446}
Step 54200: {'loss': 1.514, 'grad_norm': 5.887313365936279, 'learning_rate': 5.1845333333333335e-06, 'epoch': 2.408888888888889}
Step 54300: {'loss': 1.5146, 'grad_norm': 5.640110492706299, 'learning_rate': 5.175644444444446e-06, 'epoch': 2.413333333333333}
Step 54400: {'loss': 1.5027, 'grad_norm': 6.710358619689941, 'learning_rate': 5.166755555555556e-06, 'epoch': 2.417777777777778}
Step 54500: {'loss': 1.4981, 'grad_norm': 6.882486820220947, 'learning_rate': 5.157866666666667e-06, 'epoch': 2.422222222222222}
Step 54600: {'loss': 1.5212, 'grad_norm': 6.703976154327393, 'learning_rate': 5.148977777777778e-06, 'epoch': 2.4266666666666667}
Step 54700: {'loss': 1.5314, 'grad_norm': 5.121593952178955, 'learning_rate': 5.140088888888889e-06, 'epoch': 2.431111111111111}
Step 54800: {'loss': 1.5265, 'grad_norm': 5.737277507781982, 'learning_rate': 5.131200000000001e-06, 'epoch': 2.4355555555555557}
Step 54900: {'loss': 1.5073, 'grad_norm': 5.255755424499512, 'learning_rate': 5.1223111111111115e-06, 'epoch': 2.44}
Step 55000: {'loss': 1.4575, 'grad_norm': 5.641067981719971, 'learning_rate': 5.113422222222223e-06, 'epoch': 2.4444444444444446}
Step 55100: {'loss': 1.5082, 'grad_norm': 6.184950351715088, 'learning_rate': 5.104533333333333e-06, 'epoch': 2.448888888888889}
Step 55200: {'loss': 1.5263, 'grad_norm': 5.064931392669678, 'learning_rate': 5.095733333333334e-06, 'epoch': 2.453333333333333}
Step 55300: {'loss': 1.4907, 'grad_norm': 6.856351375579834, 'learning_rate': 5.086844444444445e-06, 'epoch': 2.457777777777778}
Step 55400: {'loss': 1.4792, 'grad_norm': 6.560573101043701, 'learning_rate': 5.077955555555556e-06, 'epoch': 2.462222222222222}
Step 55500: {'loss': 1.5239, 'grad_norm': 5.817989826202393, 'learning_rate': 5.069066666666667e-06, 'epoch': 2.466666666666667}
Step 55600: {'loss': 1.4841, 'grad_norm': 6.017120361328125, 'learning_rate': 5.060177777777778e-06, 'epoch': 2.471111111111111}
Step 55700: {'loss': 1.4953, 'grad_norm': 5.952930450439453, 'learning_rate': 5.051288888888889e-06, 'epoch': 2.4755555555555557}
Step 55800: {'loss': 1.5126, 'grad_norm': 7.287938117980957, 'learning_rate': 5.0424e-06, 'epoch': 2.48}
Step 55900: {'loss': 1.5384, 'grad_norm': 6.166382312774658, 'learning_rate': 5.033511111111112e-06, 'epoch': 2.4844444444444447}
Step 56000: {'loss': 1.5216, 'grad_norm': 5.449310302734375, 'learning_rate': 5.024622222222223e-06, 'epoch': 2.488888888888889}
Step 56100: {'loss': 1.4836, 'grad_norm': 6.579117298126221, 'learning_rate': 5.0157333333333335e-06, 'epoch': 2.493333333333333}
Step 56200: {'loss': 1.5391, 'grad_norm': 6.376302242279053, 'learning_rate': 5.006844444444445e-06, 'epoch': 2.497777777777778}
Step 56300: {'loss': 1.523, 'grad_norm': 5.201292514801025, 'learning_rate': 4.997955555555556e-06, 'epoch': 2.502222222222222}
Step 56400: {'loss': 1.4858, 'grad_norm': 4.974180698394775, 'learning_rate': 4.989066666666667e-06, 'epoch': 2.506666666666667}
Step 56500: {'loss': 1.4892, 'grad_norm': 4.642875671386719, 'learning_rate': 4.980177777777778e-06, 'epoch': 2.511111111111111}
Step 56600: {'loss': 1.4777, 'grad_norm': 5.531876564025879, 'learning_rate': 4.97128888888889e-06, 'epoch': 2.5155555555555553}
Step 56700: {'loss': 1.4957, 'grad_norm': 6.389926433563232, 'learning_rate': 4.962400000000001e-06, 'epoch': 2.52}
Step 56800: {'loss': 1.5091, 'grad_norm': 5.7910075187683105, 'learning_rate': 4.9535111111111115e-06, 'epoch': 2.5244444444444447}
Step 56900: {'loss': 1.4848, 'grad_norm': 5.244346618652344, 'learning_rate': 4.944622222222223e-06, 'epoch': 2.528888888888889}
Step 57000: {'loss': 1.5177, 'grad_norm': 5.812253952026367, 'learning_rate': 4.9357333333333334e-06, 'epoch': 2.533333333333333}
Step 57100: {'loss': 1.4998, 'grad_norm': 6.983330249786377, 'learning_rate': 4.926844444444445e-06, 'epoch': 2.537777777777778}
Step 57200: {'loss': 1.4737, 'grad_norm': 6.082135200500488, 'learning_rate': 4.918044444444445e-06, 'epoch': 2.542222222222222}
Step 57300: {'loss': 1.5118, 'grad_norm': 6.433398246765137, 'learning_rate': 4.9091555555555555e-06, 'epoch': 2.546666666666667}
Step 57400: {'loss': 1.4834, 'grad_norm': 5.940297603607178, 'learning_rate': 4.900266666666667e-06, 'epoch': 2.551111111111111}
Step 57500: {'loss': 1.4881, 'grad_norm': 5.582930088043213, 'learning_rate': 4.891377777777778e-06, 'epoch': 2.5555555555555554}
Step 57600: {'loss': 1.4951, 'grad_norm': 5.967439651489258, 'learning_rate': 4.88248888888889e-06, 'epoch': 2.56}
Step 57700: {'loss': 1.5217, 'grad_norm': 5.973031044006348, 'learning_rate': 4.8736e-06, 'epoch': 2.5644444444444443}
Step 57800: {'loss': 1.4981, 'grad_norm': 6.034971714019775, 'learning_rate': 4.864711111111112e-06, 'epoch': 2.568888888888889}
Step 57900: {'loss': 1.4722, 'grad_norm': 6.0276384353637695, 'learning_rate': 4.855822222222223e-06, 'epoch': 2.5733333333333333}
Step 58000: {'loss': 1.496, 'grad_norm': 5.920516014099121, 'learning_rate': 4.8469333333333335e-06, 'epoch': 2.5777777777777775}
Step 58100: {'loss': 1.4882, 'grad_norm': 6.082977771759033, 'learning_rate': 4.838044444444445e-06, 'epoch': 2.582222222222222}
Step 58200: {'loss': 1.49, 'grad_norm': 5.561607360839844, 'learning_rate': 4.8291555555555555e-06, 'epoch': 2.586666666666667}
Step 58300: {'loss': 1.508, 'grad_norm': 6.030642032623291, 'learning_rate': 4.820266666666667e-06, 'epoch': 2.591111111111111}
Step 58400: {'loss': 1.5001, 'grad_norm': 5.320519924163818, 'learning_rate': 4.811377777777778e-06, 'epoch': 2.5955555555555554}
Step 58500: {'loss': 1.5065, 'grad_norm': 7.070597171783447, 'learning_rate': 4.80248888888889e-06, 'epoch': 2.6}
Step 58600: {'loss': 1.5292, 'grad_norm': 5.735476016998291, 'learning_rate': 4.7936e-06, 'epoch': 2.6044444444444443}
Step 58700: {'loss': 1.5006, 'grad_norm': 6.948894023895264, 'learning_rate': 4.7847111111111115e-06, 'epoch': 2.608888888888889}
Step 58800: {'loss': 1.5029, 'grad_norm': 6.708220481872559, 'learning_rate': 4.775822222222223e-06, 'epoch': 2.6133333333333333}
Step 58900: {'loss': 1.5069, 'grad_norm': 4.850708484649658, 'learning_rate': 4.766933333333334e-06, 'epoch': 2.6177777777777775}
Step 59000: {'loss': 1.5058, 'grad_norm': 5.3550872802734375, 'learning_rate': 4.758044444444445e-06, 'epoch': 2.6222222222222222}
Step 59100: {'loss': 1.4963, 'grad_norm': 5.763555526733398, 'learning_rate': 4.749155555555555e-06, 'epoch': 2.626666666666667}
Step 59200: {'loss': 1.5031, 'grad_norm': 6.027457237243652, 'learning_rate': 4.740266666666667e-06, 'epoch': 2.631111111111111}
Step 59300: {'loss': 1.5186, 'grad_norm': 8.011354446411133, 'learning_rate': 4.731466666666667e-06, 'epoch': 2.6355555555555554}
Step 59400: {'loss': 1.5018, 'grad_norm': 4.744731903076172, 'learning_rate': 4.722577777777778e-06, 'epoch': 2.64}
Step 59500: {'loss': 1.5114, 'grad_norm': 6.273828983306885, 'learning_rate': 4.71368888888889e-06, 'epoch': 2.6444444444444444}
Step 59600: {'loss': 1.5146, 'grad_norm': 5.922699928283691, 'learning_rate': 4.7048e-06, 'epoch': 2.648888888888889}
Step 59700: {'loss': 1.4889, 'grad_norm': 6.403932094573975, 'learning_rate': 4.695911111111112e-06, 'epoch': 2.6533333333333333}
Step 59800: {'loss': 1.4811, 'grad_norm': 5.765309810638428, 'learning_rate': 4.687022222222223e-06, 'epoch': 2.6577777777777776}
Step 59900: {'loss': 1.4921, 'grad_norm': 6.039694786071777, 'learning_rate': 4.6781333333333336e-06, 'epoch': 2.6622222222222223}
Step 60000: {'loss': 1.4926, 'grad_norm': 6.3470683097839355, 'learning_rate': 4.669244444444445e-06, 'epoch': 2.6666666666666665}
Step 60100: {'loss': 1.4759, 'grad_norm': 6.5463433265686035, 'learning_rate': 4.6603555555555555e-06, 'epoch': 2.671111111111111}
Step 60200: {'loss': 1.5457, 'grad_norm': 6.308573246002197, 'learning_rate': 4.651466666666667e-06, 'epoch': 2.6755555555555555}
Step 60300: {'loss': 1.5375, 'grad_norm': 5.194228649139404, 'learning_rate': 4.642577777777778e-06, 'epoch': 2.68}
Step 60400: {'loss': 1.5117, 'grad_norm': 5.918785572052002, 'learning_rate': 4.63368888888889e-06, 'epoch': 2.6844444444444444}
Step 60500: {'loss': 1.5202, 'grad_norm': 5.234470844268799, 'learning_rate': 4.6248e-06, 'epoch': 2.688888888888889}
Step 60600: {'loss': 1.5209, 'grad_norm': 6.5933709144592285, 'learning_rate': 4.6159111111111116e-06, 'epoch': 2.6933333333333334}
Step 60700: {'loss': 1.4927, 'grad_norm': 5.659660339355469, 'learning_rate': 4.607022222222223e-06, 'epoch': 2.6977777777777776}
Step 60800: {'loss': 1.4916, 'grad_norm': 6.615653991699219, 'learning_rate': 4.5981333333333335e-06, 'epoch': 2.7022222222222223}
Step 60900: {'loss': 1.5183, 'grad_norm': 4.850601673126221, 'learning_rate': 4.589244444444445e-06, 'epoch': 2.7066666666666666}
Step 61000: {'loss': 1.4776, 'grad_norm': 7.29971981048584, 'learning_rate': 4.580355555555555e-06, 'epoch': 2.7111111111111112}
Step 61100: {'loss': 1.505, 'grad_norm': 6.2976531982421875, 'learning_rate': 4.571466666666667e-06, 'epoch': 2.7155555555555555}
Step 61200: {'loss': 1.4753, 'grad_norm': 6.488854885101318, 'learning_rate': 4.562577777777778e-06, 'epoch': 2.7199999999999998}
Step 61300: {'loss': 1.5324, 'grad_norm': 5.0250630378723145, 'learning_rate': 4.553777777777778e-06, 'epoch': 2.7244444444444444}
Step 61400: {'loss': 1.5257, 'grad_norm': 6.219089031219482, 'learning_rate': 4.544888888888889e-06, 'epoch': 2.728888888888889}
Step 61500: {'loss': 1.4801, 'grad_norm': 5.596930503845215, 'learning_rate': 4.536e-06, 'epoch': 2.7333333333333334}
Step 61600: {'loss': 1.502, 'grad_norm': 7.108383655548096, 'learning_rate': 4.527111111111112e-06, 'epoch': 2.7377777777777776}
Step 61700: {'loss': 1.4781, 'grad_norm': 6.42667818069458, 'learning_rate': 4.518222222222223e-06, 'epoch': 2.7422222222222223}
Step 61800: {'loss': 1.498, 'grad_norm': 5.03157901763916, 'learning_rate': 4.509333333333334e-06, 'epoch': 2.7466666666666666}
Step 61900: {'loss': 1.4919, 'grad_norm': 5.326007843017578, 'learning_rate': 4.500444444444445e-06, 'epoch': 2.7511111111111113}
Step 62000: {'loss': 1.4846, 'grad_norm': 5.567666053771973, 'learning_rate': 4.4915555555555555e-06, 'epoch': 2.7555555555555555}
Step 62100: {'loss': 1.5177, 'grad_norm': 5.832616806030273, 'learning_rate': 4.482666666666667e-06, 'epoch': 2.76}
Step 62200: {'loss': 1.4839, 'grad_norm': 5.224260330200195, 'learning_rate': 4.473777777777778e-06, 'epoch': 2.7644444444444445}
Step 62300: {'loss': 1.5153, 'grad_norm': 6.000185012817383, 'learning_rate': 4.464888888888889e-06, 'epoch': 2.7688888888888887}
Step 62400: {'loss': 1.467, 'grad_norm': 5.372166156768799, 'learning_rate': 4.456e-06, 'epoch': 2.7733333333333334}
Step 62500: {'loss': 1.5424, 'grad_norm': 6.32235860824585, 'learning_rate': 4.447111111111112e-06, 'epoch': 2.7777777777777777}
Step 62600: {'loss': 1.4558, 'grad_norm': 5.185024738311768, 'learning_rate': 4.438222222222223e-06, 'epoch': 2.7822222222222224}
Step 62700: {'loss': 1.4945, 'grad_norm': 5.624604225158691, 'learning_rate': 4.4293333333333335e-06, 'epoch': 2.7866666666666666}
Step 62800: {'loss': 1.494, 'grad_norm': 5.403887748718262, 'learning_rate': 4.420444444444445e-06, 'epoch': 2.7911111111111113}
Step 62900: {'loss': 1.4864, 'grad_norm': 7.1487321853637695, 'learning_rate': 4.411555555555556e-06, 'epoch': 2.7955555555555556}
Step 63000: {'loss': 1.5225, 'grad_norm': 5.500217437744141, 'learning_rate': 4.402666666666667e-06, 'epoch': 2.8}
Step 63100: {'loss': 1.4667, 'grad_norm': 6.1774001121521, 'learning_rate': 4.393777777777778e-06, 'epoch': 2.8044444444444445}
Step 63200: {'loss': 1.4978, 'grad_norm': 6.519896507263184, 'learning_rate': 4.384888888888889e-06, 'epoch': 2.8088888888888888}
Step 63300: {'loss': 1.4856, 'grad_norm': 5.96351432800293, 'learning_rate': 4.376e-06, 'epoch': 2.8133333333333335}
Step 63400: {'loss': 1.4966, 'grad_norm': 6.2807536125183105, 'learning_rate': 4.3672e-06, 'epoch': 2.8177777777777777}
Step 63500: {'loss': 1.5207, 'grad_norm': 6.320102214813232, 'learning_rate': 4.358311111111112e-06, 'epoch': 2.822222222222222}
Step 63600: {'loss': 1.4689, 'grad_norm': 5.797898292541504, 'learning_rate': 4.349422222222223e-06, 'epoch': 2.8266666666666667}
Step 63700: {'loss': 1.5319, 'grad_norm': 6.031149864196777, 'learning_rate': 4.340533333333334e-06, 'epoch': 2.8311111111111114}
Step 63800: {'loss': 1.5381, 'grad_norm': 5.318360805511475, 'learning_rate': 4.331644444444445e-06, 'epoch': 2.8355555555555556}
Step 63900: {'loss': 1.5183, 'grad_norm': 6.756066799163818, 'learning_rate': 4.322755555555556e-06, 'epoch': 2.84}
Step 64000: {'loss': 1.4885, 'grad_norm': 5.406097888946533, 'learning_rate': 4.313866666666667e-06, 'epoch': 2.8444444444444446}
Step 64100: {'loss': 1.5066, 'grad_norm': 4.3415961265563965, 'learning_rate': 4.304977777777778e-06, 'epoch': 2.848888888888889}
Step 64200: {'loss': 1.523, 'grad_norm': 6.32993221282959, 'learning_rate': 4.296088888888889e-06, 'epoch': 2.8533333333333335}
Step 64300: {'loss': 1.4821, 'grad_norm': 5.347588062286377, 'learning_rate': 4.2872e-06, 'epoch': 2.8577777777777778}
Step 64400: {'loss': 1.489, 'grad_norm': 6.655091762542725, 'learning_rate': 4.278311111111112e-06, 'epoch': 2.862222222222222}
Step 64500: {'loss': 1.4991, 'grad_norm': 7.709441661834717, 'learning_rate': 4.269422222222223e-06, 'epoch': 2.8666666666666667}
Step 64600: {'loss': 1.4956, 'grad_norm': 5.5391845703125, 'learning_rate': 4.2605333333333335e-06, 'epoch': 2.871111111111111}
Step 64700: {'loss': 1.4835, 'grad_norm': 6.048527240753174, 'learning_rate': 4.251644444444445e-06, 'epoch': 2.8755555555555556}
Step 64800: {'loss': 1.4911, 'grad_norm': 6.818532466888428, 'learning_rate': 4.242755555555556e-06, 'epoch': 2.88}
Step 64900: {'loss': 1.4711, 'grad_norm': 5.508950710296631, 'learning_rate': 4.233866666666667e-06, 'epoch': 2.8844444444444446}
Step 65000: {'loss': 1.496, 'grad_norm': 6.665717601776123, 'learning_rate': 4.224977777777778e-06, 'epoch': 2.888888888888889}
Step 65100: {'loss': 1.4969, 'grad_norm': 6.383749961853027, 'learning_rate': 4.216088888888889e-06, 'epoch': 2.8933333333333335}
Step 65200: {'loss': 1.4885, 'grad_norm': 5.981866836547852, 'learning_rate': 4.2072e-06, 'epoch': 2.897777777777778}
Step 65300: {'loss': 1.4853, 'grad_norm': 5.73339319229126, 'learning_rate': 4.1983111111111115e-06, 'epoch': 2.902222222222222}
Step 65400: {'loss': 1.5345, 'grad_norm': 5.6282639503479, 'learning_rate': 4.189511111111112e-06, 'epoch': 2.9066666666666667}
Step 65500: {'loss': 1.4722, 'grad_norm': 5.71094274520874, 'learning_rate': 4.180622222222222e-06, 'epoch': 2.911111111111111}
Step 65600: {'loss': 1.5047, 'grad_norm': 5.275905609130859, 'learning_rate': 4.171733333333334e-06, 'epoch': 2.9155555555555557}
Step 65700: {'loss': 1.5198, 'grad_norm': 6.308121681213379, 'learning_rate': 4.162844444444445e-06, 'epoch': 2.92}
Step 65800: {'loss': 1.488, 'grad_norm': 5.949952125549316, 'learning_rate': 4.153955555555556e-06, 'epoch': 2.924444444444444}
Step 65900: {'loss': 1.5316, 'grad_norm': 5.6444315910339355, 'learning_rate': 4.145066666666667e-06, 'epoch': 2.928888888888889}
Step 66000: {'loss': 1.4958, 'grad_norm': 5.609707355499268, 'learning_rate': 4.1361777777777775e-06, 'epoch': 2.9333333333333336}
Step 66100: {'loss': 1.4934, 'grad_norm': 5.500119686126709, 'learning_rate': 4.127288888888889e-06, 'epoch': 2.937777777777778}
Step 66200: {'loss': 1.4898, 'grad_norm': 4.816367149353027, 'learning_rate': 4.1184e-06, 'epoch': 2.942222222222222}
Step 66300: {'loss': 1.4873, 'grad_norm': 6.662552356719971, 'learning_rate': 4.109511111111112e-06, 'epoch': 2.9466666666666668}
Step 66400: {'loss': 1.4525, 'grad_norm': 5.58004093170166, 'learning_rate': 4.100622222222222e-06, 'epoch': 2.951111111111111}
Step 66500: {'loss': 1.5193, 'grad_norm': 6.6220903396606445, 'learning_rate': 4.0917333333333336e-06, 'epoch': 2.9555555555555557}
Step 66600: {'loss': 1.5135, 'grad_norm': 5.3732733726501465, 'learning_rate': 4.082844444444445e-06, 'epoch': 2.96}
Step 66700: {'loss': 1.5078, 'grad_norm': 5.401058673858643, 'learning_rate': 4.073955555555556e-06, 'epoch': 2.964444444444444}
Step 66800: {'loss': 1.4989, 'grad_norm': 5.136219501495361, 'learning_rate': 4.065066666666667e-06, 'epoch': 2.968888888888889}
Step 66900: {'loss': 1.4904, 'grad_norm': 5.204802513122559, 'learning_rate': 4.056177777777778e-06, 'epoch': 2.9733333333333336}
Step 67000: {'loss': 1.483, 'grad_norm': 5.622727870941162, 'learning_rate': 4.04728888888889e-06, 'epoch': 2.977777777777778}
Step 67100: {'loss': 1.4856, 'grad_norm': 6.939631938934326, 'learning_rate': 4.0384e-06, 'epoch': 2.982222222222222}
Step 67200: {'loss': 1.4928, 'grad_norm': 4.753209590911865, 'learning_rate': 4.0295111111111115e-06, 'epoch': 2.986666666666667}
Step 67300: {'loss': 1.481, 'grad_norm': 5.267347812652588, 'learning_rate': 4.020622222222222e-06, 'epoch': 2.991111111111111}
Step 67400: {'loss': 1.5322, 'grad_norm': 6.722236156463623, 'learning_rate': 4.0117333333333335e-06, 'epoch': 2.9955555555555557}
Step 67500: {'loss': 1.4819, 'grad_norm': 5.010489463806152, 'learning_rate': 4.002933333333334e-06, 'epoch': 3.0}
Step 67600: {'loss': 1.4784, 'grad_norm': 6.456971168518066, 'learning_rate': 3.994044444444445e-06, 'epoch': 3.0044444444444443}
Step 67700: {'loss': 1.4689, 'grad_norm': 6.23629093170166, 'learning_rate': 3.985155555555556e-06, 'epoch': 3.008888888888889}
Step 67800: {'loss': 1.4776, 'grad_norm': 5.751826286315918, 'learning_rate': 3.976266666666667e-06, 'epoch': 3.013333333333333}
Step 67900: {'loss': 1.4847, 'grad_norm': 5.6091461181640625, 'learning_rate': 3.967377777777778e-06, 'epoch': 3.017777777777778}
Step 68000: {'loss': 1.464, 'grad_norm': 5.450532913208008, 'learning_rate': 3.958488888888889e-06, 'epoch': 3.022222222222222}
Step 68100: {'loss': 1.4576, 'grad_norm': 6.161269187927246, 'learning_rate': 3.9496e-06, 'epoch': 3.026666666666667}
Step 68200: {'loss': 1.4632, 'grad_norm': 6.9100117683410645, 'learning_rate': 3.940711111111112e-06, 'epoch': 3.031111111111111}
Step 68300: {'loss': 1.5121, 'grad_norm': 6.692798137664795, 'learning_rate': 3.931822222222222e-06, 'epoch': 3.0355555555555553}
Step 68400: {'loss': 1.4671, 'grad_norm': 6.434842109680176, 'learning_rate': 3.9229333333333336e-06, 'epoch': 3.04}
Step 68500: {'loss': 1.4806, 'grad_norm': 6.490229606628418, 'learning_rate': 3.914044444444445e-06, 'epoch': 3.0444444444444443}
Step 68600: {'loss': 1.4609, 'grad_norm': 5.973875045776367, 'learning_rate': 3.905155555555556e-06, 'epoch': 3.048888888888889}
Step 68700: {'loss': 1.4987, 'grad_norm': 6.311092853546143, 'learning_rate': 3.896266666666667e-06, 'epoch': 3.0533333333333332}
Step 68800: {'loss': 1.5074, 'grad_norm': 5.088950157165527, 'learning_rate': 3.887377777777778e-06, 'epoch': 3.057777777777778}
Step 68900: {'loss': 1.4813, 'grad_norm': 6.233273506164551, 'learning_rate': 3.87848888888889e-06, 'epoch': 3.062222222222222}
Step 69000: {'loss': 1.453, 'grad_norm': 6.843947410583496, 'learning_rate': 3.8696e-06, 'epoch': 3.066666666666667}
Step 69100: {'loss': 1.4901, 'grad_norm': 5.806759357452393, 'learning_rate': 3.8607111111111116e-06, 'epoch': 3.071111111111111}
Step 69200: {'loss': 1.487, 'grad_norm': 5.605520248413086, 'learning_rate': 3.851822222222222e-06, 'epoch': 3.0755555555555554}
Step 69300: {'loss': 1.4838, 'grad_norm': 7.700567722320557, 'learning_rate': 3.8429333333333335e-06, 'epoch': 3.08}
Step 69400: {'loss': 1.497, 'grad_norm': 5.428143501281738, 'learning_rate': 3.834044444444445e-06, 'epoch': 3.0844444444444443}
Step 69500: {'loss': 1.5084, 'grad_norm': 5.726452350616455, 'learning_rate': 3.825244444444445e-06, 'epoch': 3.088888888888889}
Step 69600: {'loss': 1.4987, 'grad_norm': 5.508836269378662, 'learning_rate': 3.816355555555556e-06, 'epoch': 3.0933333333333333}
Step 69700: {'loss': 1.488, 'grad_norm': 6.680547714233398, 'learning_rate': 3.807466666666667e-06, 'epoch': 3.097777777777778}
Step 69800: {'loss': 1.4773, 'grad_norm': 7.054487228393555, 'learning_rate': 3.7985777777777784e-06, 'epoch': 3.102222222222222}
Step 69900: {'loss': 1.4791, 'grad_norm': 6.310404300689697, 'learning_rate': 3.7896888888888893e-06, 'epoch': 3.1066666666666665}
Step 70000: {'loss': 1.4409, 'grad_norm': 5.769993305206299, 'learning_rate': 3.7808000000000007e-06, 'epoch': 3.111111111111111}
Step 70100: {'loss': 1.4713, 'grad_norm': 5.1509904861450195, 'learning_rate': 3.7719111111111113e-06, 'epoch': 3.1155555555555554}
Step 70200: {'loss': 1.4896, 'grad_norm': 4.624653339385986, 'learning_rate': 3.7630222222222222e-06, 'epoch': 3.12}
Step 70300: {'loss': 1.5015, 'grad_norm': 5.23190975189209, 'learning_rate': 3.7541333333333336e-06, 'epoch': 3.1244444444444444}
Step 70400: {'loss': 1.4991, 'grad_norm': 5.831934928894043, 'learning_rate': 3.7452444444444446e-06, 'epoch': 3.128888888888889}
Step 70500: {'loss': 1.4971, 'grad_norm': 5.881008148193359, 'learning_rate': 3.736355555555556e-06, 'epoch': 3.1333333333333333}
Step 70600: {'loss': 1.4833, 'grad_norm': 6.504662036895752, 'learning_rate': 3.727466666666667e-06, 'epoch': 3.137777777777778}
Step 70700: {'loss': 1.4529, 'grad_norm': 6.193457126617432, 'learning_rate': 3.7185777777777783e-06, 'epoch': 3.1422222222222222}
Step 70800: {'loss': 1.4792, 'grad_norm': 6.827535629272461, 'learning_rate': 3.7096888888888892e-06, 'epoch': 3.1466666666666665}
Step 70900: {'loss': 1.478, 'grad_norm': 5.163330554962158, 'learning_rate': 3.7008000000000006e-06, 'epoch': 3.151111111111111}
Step 71000: {'loss': 1.4842, 'grad_norm': 5.801815032958984, 'learning_rate': 3.6919111111111116e-06, 'epoch': 3.1555555555555554}
Step 71100: {'loss': 1.4553, 'grad_norm': 5.748512268066406, 'learning_rate': 3.683022222222222e-06, 'epoch': 3.16}
Step 71200: {'loss': 1.5002, 'grad_norm': 4.946335792541504, 'learning_rate': 3.6741333333333335e-06, 'epoch': 3.1644444444444444}
Step 71300: {'loss': 1.4846, 'grad_norm': 7.37860107421875, 'learning_rate': 3.6652444444444445e-06, 'epoch': 3.168888888888889}
Step 71400: {'loss': 1.4705, 'grad_norm': 4.950206279754639, 'learning_rate': 3.656355555555556e-06, 'epoch': 3.1733333333333333}
Step 71500: {'loss': 1.4672, 'grad_norm': 6.25102424621582, 'learning_rate': 3.647555555555556e-06, 'epoch': 3.1777777777777776}
Step 71600: {'loss': 1.485, 'grad_norm': 5.933204174041748, 'learning_rate': 3.638666666666667e-06, 'epoch': 3.1822222222222223}
Step 71700: {'loss': 1.4999, 'grad_norm': 6.293167591094971, 'learning_rate': 3.629777777777778e-06, 'epoch': 3.1866666666666665}
Step 71800: {'loss': 1.4871, 'grad_norm': 5.500380992889404, 'learning_rate': 3.6208888888888894e-06, 'epoch': 3.1911111111111112}
Step 71900: {'loss': 1.4771, 'grad_norm': 5.314475059509277, 'learning_rate': 3.6120000000000003e-06, 'epoch': 3.1955555555555555}
Step 72000: {'loss': 1.5135, 'grad_norm': 5.918814182281494, 'learning_rate': 3.6031111111111117e-06, 'epoch': 3.2}
Step 72100: {'loss': 1.4731, 'grad_norm': 4.627537727355957, 'learning_rate': 3.5942222222222222e-06, 'epoch': 3.2044444444444444}
Step 72200: {'loss': 1.4986, 'grad_norm': 7.277796745300293, 'learning_rate': 3.5853333333333336e-06, 'epoch': 3.2088888888888887}
Step 72300: {'loss': 1.4279, 'grad_norm': 5.7858757972717285, 'learning_rate': 3.5764444444444446e-06, 'epoch': 3.2133333333333334}
Step 72400: {'loss': 1.4854, 'grad_norm': 5.33024787902832, 'learning_rate': 3.567555555555556e-06, 'epoch': 3.2177777777777776}
Step 72500: {'loss': 1.4702, 'grad_norm': 4.617566108703613, 'learning_rate': 3.558666666666667e-06, 'epoch': 3.2222222222222223}
Step 72600: {'loss': 1.4604, 'grad_norm': 5.288885593414307, 'learning_rate': 3.549777777777778e-06, 'epoch': 3.2266666666666666}
Step 72700: {'loss': 1.4821, 'grad_norm': 5.84838342666626, 'learning_rate': 3.5408888888888893e-06, 'epoch': 3.2311111111111113}
Step 72800: {'loss': 1.4821, 'grad_norm': 5.399189472198486, 'learning_rate': 3.5320000000000002e-06, 'epoch': 3.2355555555555555}
Step 72900: {'loss': 1.5048, 'grad_norm': 8.183982849121094, 'learning_rate': 3.5231111111111116e-06, 'epoch': 3.24}
Step 73000: {'loss': 1.513, 'grad_norm': 6.0346479415893555, 'learning_rate': 3.5142222222222226e-06, 'epoch': 3.2444444444444445}
Step 73100: {'loss': 1.4908, 'grad_norm': 4.88754415512085, 'learning_rate': 3.5053333333333335e-06, 'epoch': 3.2488888888888887}
Step 73200: {'loss': 1.47, 'grad_norm': 6.454187870025635, 'learning_rate': 3.4964444444444445e-06, 'epoch': 3.2533333333333334}
Step 73300: {'loss': 1.4637, 'grad_norm': 6.0101213455200195, 'learning_rate': 3.487555555555556e-06, 'epoch': 3.2577777777777777}
Step 73400: {'loss': 1.4994, 'grad_norm': 6.544532299041748, 'learning_rate': 3.478666666666667e-06, 'epoch': 3.2622222222222224}
Step 73500: {'loss': 1.5089, 'grad_norm': 5.672341823577881, 'learning_rate': 3.469866666666667e-06, 'epoch': 3.2666666666666666}
Step 73600: {'loss': 1.5009, 'grad_norm': 6.260368824005127, 'learning_rate': 3.460977777777778e-06, 'epoch': 3.2711111111111113}
Step 73700: {'loss': 1.4786, 'grad_norm': 6.667388439178467, 'learning_rate': 3.4520888888888894e-06, 'epoch': 3.2755555555555556}
Step 73800: {'loss': 1.4937, 'grad_norm': 6.588907718658447, 'learning_rate': 3.4432000000000003e-06, 'epoch': 3.2800000000000002}
Step 73900: {'loss': 1.4719, 'grad_norm': 6.877096176147461, 'learning_rate': 3.4343111111111117e-06, 'epoch': 3.2844444444444445}
Step 74000: {'loss': 1.4961, 'grad_norm': 4.9716291427612305, 'learning_rate': 3.4254222222222227e-06, 'epoch': 3.2888888888888888}
Step 74100: {'loss': 1.474, 'grad_norm': 5.7824788093566895, 'learning_rate': 3.4165333333333332e-06, 'epoch': 3.2933333333333334}
Step 74200: {'loss': 1.5111, 'grad_norm': 4.991307735443115, 'learning_rate': 3.4076444444444446e-06, 'epoch': 3.2977777777777777}
Step 74300: {'loss': 1.5124, 'grad_norm': 6.096679210662842, 'learning_rate': 3.3987555555555556e-06, 'epoch': 3.3022222222222224}
Step 74400: {'loss': 1.5067, 'grad_norm': 6.412217140197754, 'learning_rate': 3.389866666666667e-06, 'epoch': 3.3066666666666666}
Step 74500: {'loss': 1.5005, 'grad_norm': 6.167520523071289, 'learning_rate': 3.380977777777778e-06, 'epoch': 3.311111111111111}
Step 74600: {'loss': 1.4873, 'grad_norm': 5.6220502853393555, 'learning_rate': 3.3720888888888893e-06, 'epoch': 3.3155555555555556}
Step 74700: {'loss': 1.488, 'grad_norm': 5.642742156982422, 'learning_rate': 3.3632000000000003e-06, 'epoch': 3.32}
Step 74800: {'loss': 1.4813, 'grad_norm': 5.235828399658203, 'learning_rate': 3.3543111111111116e-06, 'epoch': 3.3244444444444445}
Step 74900: {'loss': 1.4905, 'grad_norm': 7.0505781173706055, 'learning_rate': 3.3454222222222226e-06, 'epoch': 3.328888888888889}
Step 75000: {'loss': 1.519, 'grad_norm': 6.213498115539551, 'learning_rate': 3.336533333333334e-06, 'epoch': 3.3333333333333335}
Step 75100: {'loss': 1.4758, 'grad_norm': 5.498691558837891, 'learning_rate': 3.3276444444444445e-06, 'epoch': 3.3377777777777777}
Step 75200: {'loss': 1.4899, 'grad_norm': 6.751542091369629, 'learning_rate': 3.3187555555555555e-06, 'epoch': 3.3422222222222224}
Step 75300: {'loss': 1.5159, 'grad_norm': 5.199931621551514, 'learning_rate': 3.309866666666667e-06, 'epoch': 3.3466666666666667}
Step 75400: {'loss': 1.4904, 'grad_norm': 5.88987398147583, 'learning_rate': 3.300977777777778e-06, 'epoch': 3.351111111111111}
Step 75500: {'loss': 1.4336, 'grad_norm': 5.655282974243164, 'learning_rate': 3.2920888888888892e-06, 'epoch': 3.3555555555555556}
Step 75600: {'loss': 1.4387, 'grad_norm': 6.3228864669799805, 'learning_rate': 3.2832888888888894e-06, 'epoch': 3.36}
Step 75700: {'loss': 1.4497, 'grad_norm': 4.874483585357666, 'learning_rate': 3.2744000000000004e-06, 'epoch': 3.3644444444444446}
Step 75800: {'loss': 1.4968, 'grad_norm': 6.095190048217773, 'learning_rate': 3.2655111111111113e-06, 'epoch': 3.368888888888889}
Step 75900: {'loss': 1.5339, 'grad_norm': 5.674749374389648, 'learning_rate': 3.2566222222222227e-06, 'epoch': 3.3733333333333335}
Step 76000: {'loss': 1.4812, 'grad_norm': 5.185273170471191, 'learning_rate': 3.2477333333333337e-06, 'epoch': 3.3777777777777778}
Step 76100: {'loss': 1.4735, 'grad_norm': 5.0878071784973145, 'learning_rate': 3.2388444444444446e-06, 'epoch': 3.3822222222222225}
Step 76200: {'loss': 1.4981, 'grad_norm': 6.037543773651123, 'learning_rate': 3.2299555555555556e-06, 'epoch': 3.3866666666666667}
Step 76300: {'loss': 1.5082, 'grad_norm': 4.482745170593262, 'learning_rate': 3.221066666666667e-06, 'epoch': 3.391111111111111}
Step 76400: {'loss': 1.4517, 'grad_norm': 6.84913969039917, 'learning_rate': 3.212177777777778e-06, 'epoch': 3.3955555555555557}
Step 76500: {'loss': 1.4712, 'grad_norm': 5.955472946166992, 'learning_rate': 3.2032888888888893e-06, 'epoch': 3.4}
Step 76600: {'loss': 1.4542, 'grad_norm': 5.478865623474121, 'learning_rate': 3.1944000000000003e-06, 'epoch': 3.4044444444444446}
Step 76700: {'loss': 1.4927, 'grad_norm': 5.895533561706543, 'learning_rate': 3.1855111111111112e-06, 'epoch': 3.408888888888889}
Step 76800: {'loss': 1.4371, 'grad_norm': 5.396526336669922, 'learning_rate': 3.1766222222222226e-06, 'epoch': 3.413333333333333}
Step 76900: {'loss': 1.4577, 'grad_norm': 5.265944957733154, 'learning_rate': 3.1677333333333336e-06, 'epoch': 3.417777777777778}
Step 77000: {'loss': 1.4775, 'grad_norm': 5.9266133308410645, 'learning_rate': 3.158844444444445e-06, 'epoch': 3.422222222222222}
Step 77100: {'loss': 1.4859, 'grad_norm': 6.088873386383057, 'learning_rate': 3.1499555555555555e-06, 'epoch': 3.4266666666666667}
Step 77200: {'loss': 1.4586, 'grad_norm': 7.1954803466796875, 'learning_rate': 3.141066666666667e-06, 'epoch': 3.431111111111111}
Step 77300: {'loss': 1.4472, 'grad_norm': 6.238870620727539, 'learning_rate': 3.132177777777778e-06, 'epoch': 3.4355555555555557}
Step 77400: {'loss': 1.464, 'grad_norm': 5.8458709716796875, 'learning_rate': 3.1232888888888892e-06, 'epoch': 3.44}
Step 77500: {'loss': 1.4436, 'grad_norm': 6.232944965362549, 'learning_rate': 3.1144e-06, 'epoch': 3.4444444444444446}
Step 77600: {'loss': 1.4906, 'grad_norm': 5.768046855926514, 'learning_rate': 3.105511111111111e-06, 'epoch': 3.448888888888889}
Step 77700: {'loss': 1.4549, 'grad_norm': 5.622902870178223, 'learning_rate': 3.0967111111111113e-06, 'epoch': 3.453333333333333}
Step 77800: {'loss': 1.4753, 'grad_norm': 6.063200950622559, 'learning_rate': 3.0878222222222227e-06, 'epoch': 3.457777777777778}
Step 77900: {'loss': 1.4575, 'grad_norm': 7.093649387359619, 'learning_rate': 3.0789333333333337e-06, 'epoch': 3.462222222222222}
Step 78000: {'loss': 1.4626, 'grad_norm': 6.172305583953857, 'learning_rate': 3.070044444444445e-06, 'epoch': 3.466666666666667}
Step 78100: {'loss': 1.5199, 'grad_norm': 7.065784454345703, 'learning_rate': 3.0611555555555556e-06, 'epoch': 3.471111111111111}
Step 78200: {'loss': 1.4896, 'grad_norm': 5.617783069610596, 'learning_rate': 3.0522666666666666e-06, 'epoch': 3.4755555555555557}
Step 78300: {'loss': 1.494, 'grad_norm': 6.714874744415283, 'learning_rate': 3.043377777777778e-06, 'epoch': 3.48}
Step 78400: {'loss': 1.482, 'grad_norm': 5.472385883331299, 'learning_rate': 3.034488888888889e-06, 'epoch': 3.4844444444444447}
Step 78500: {'loss': 1.4577, 'grad_norm': 5.516600608825684, 'learning_rate': 3.0256000000000003e-06, 'epoch': 3.488888888888889}
Step 78600: {'loss': 1.4613, 'grad_norm': 5.53046989440918, 'learning_rate': 3.0167111111111113e-06, 'epoch': 3.493333333333333}
Step 78700: {'loss': 1.4414, 'grad_norm': 6.263238906860352, 'learning_rate': 3.0078222222222227e-06, 'epoch': 3.497777777777778}
Step 78800: {'loss': 1.4689, 'grad_norm': 6.013365268707275, 'learning_rate': 2.9989333333333336e-06, 'epoch': 3.502222222222222}
Step 78900: {'loss': 1.4577, 'grad_norm': 6.321290493011475, 'learning_rate': 2.990044444444445e-06, 'epoch': 3.506666666666667}
Step 79000: {'loss': 1.474, 'grad_norm': 8.205909729003906, 'learning_rate': 2.981155555555556e-06, 'epoch': 3.511111111111111}
Step 79100: {'loss': 1.4982, 'grad_norm': 6.314091682434082, 'learning_rate': 2.9722666666666665e-06, 'epoch': 3.5155555555555553}
Step 79200: {'loss': 1.4953, 'grad_norm': 5.178350448608398, 'learning_rate': 2.963377777777778e-06, 'epoch': 3.52}
Step 79300: {'loss': 1.4674, 'grad_norm': 6.452359676361084, 'learning_rate': 2.954488888888889e-06, 'epoch': 3.5244444444444447}
Step 79400: {'loss': 1.4774, 'grad_norm': 5.745318412780762, 'learning_rate': 2.9456000000000002e-06, 'epoch': 3.528888888888889}
Step 79500: {'loss': 1.4729, 'grad_norm': 5.92656946182251, 'learning_rate': 2.936711111111111e-06, 'epoch': 3.533333333333333}
Step 79600: {'loss': 1.4555, 'grad_norm': 6.3130950927734375, 'learning_rate': 2.9278222222222226e-06, 'epoch': 3.537777777777778}
Step 79700: {'loss': 1.4867, 'grad_norm': 5.6452436447143555, 'learning_rate': 2.9190222222222223e-06, 'epoch': 3.542222222222222}
Step 79800: {'loss': 1.4729, 'grad_norm': 6.028998374938965, 'learning_rate': 2.9101333333333337e-06, 'epoch': 3.546666666666667}
Step 79900: {'loss': 1.4655, 'grad_norm': 6.4552459716796875, 'learning_rate': 2.9012444444444447e-06, 'epoch': 3.551111111111111}
Step 80000: {'loss': 1.484, 'grad_norm': 6.396975040435791, 'learning_rate': 2.892355555555556e-06, 'epoch': 3.5555555555555554}
Step 80100: {'loss': 1.4759, 'grad_norm': 5.559775352478027, 'learning_rate': 2.8834666666666666e-06, 'epoch': 3.56}
Step 80200: {'loss': 1.4939, 'grad_norm': 5.721710205078125, 'learning_rate': 2.874577777777778e-06, 'epoch': 3.5644444444444443}
Step 80300: {'loss': 1.4553, 'grad_norm': 6.613556385040283, 'learning_rate': 2.865688888888889e-06, 'epoch': 3.568888888888889}
Step 80400: {'loss': 1.4581, 'grad_norm': 5.795932769775391, 'learning_rate': 2.8568000000000003e-06, 'epoch': 3.5733333333333333}
Step 80500: {'loss': 1.4493, 'grad_norm': 6.397973537445068, 'learning_rate': 2.8479111111111113e-06, 'epoch': 3.5777777777777775}
Step 80600: {'loss': 1.449, 'grad_norm': 5.10132360458374, 'learning_rate': 2.8390222222222223e-06, 'epoch': 3.582222222222222}
Step 80700: {'loss': 1.4394, 'grad_norm': 6.435302734375, 'learning_rate': 2.8301333333333336e-06, 'epoch': 3.586666666666667}
Step 80800: {'loss': 1.4658, 'grad_norm': 5.645084381103516, 'learning_rate': 2.8212444444444446e-06, 'epoch': 3.591111111111111}
Step 80900: {'loss': 1.4998, 'grad_norm': 6.676713943481445, 'learning_rate': 2.812355555555556e-06, 'epoch': 3.5955555555555554}
Step 81000: {'loss': 1.4882, 'grad_norm': 6.176899433135986, 'learning_rate': 2.803466666666667e-06, 'epoch': 3.6}
Step 81100: {'loss': 1.4388, 'grad_norm': 5.3763556480407715, 'learning_rate': 2.7945777777777783e-06, 'epoch': 3.6044444444444443}
Step 81200: {'loss': 1.513, 'grad_norm': 5.51738166809082, 'learning_rate': 2.785688888888889e-06, 'epoch': 3.608888888888889}
Step 81300: {'loss': 1.4545, 'grad_norm': 6.114760398864746, 'learning_rate': 2.7768000000000002e-06, 'epoch': 3.6133333333333333}
Step 81400: {'loss': 1.4804, 'grad_norm': 6.055105686187744, 'learning_rate': 2.767911111111111e-06, 'epoch': 3.6177777777777775}
Step 81500: {'loss': 1.5023, 'grad_norm': 5.831375598907471, 'learning_rate': 2.759022222222222e-06, 'epoch': 3.6222222222222222}
Step 81600: {'loss': 1.481, 'grad_norm': 6.47007417678833, 'learning_rate': 2.7501333333333336e-06, 'epoch': 3.626666666666667}
Step 81700: {'loss': 1.4875, 'grad_norm': 6.083168029785156, 'learning_rate': 2.7412444444444445e-06, 'epoch': 3.631111111111111}
Step 81800: {'loss': 1.4483, 'grad_norm': 7.284368991851807, 'learning_rate': 2.7324444444444447e-06, 'epoch': 3.6355555555555554}
Step 81900: {'loss': 1.4934, 'grad_norm': 5.553062915802002, 'learning_rate': 2.723555555555556e-06, 'epoch': 3.64}
Step 82000: {'loss': 1.4556, 'grad_norm': 5.737816333770752, 'learning_rate': 2.714666666666667e-06, 'epoch': 3.6444444444444444}
Step 82100: {'loss': 1.4798, 'grad_norm': 6.463519096374512, 'learning_rate': 2.7057777777777776e-06, 'epoch': 3.648888888888889}
Step 82200: {'loss': 1.5071, 'grad_norm': 6.61394739151001, 'learning_rate': 2.696888888888889e-06, 'epoch': 3.6533333333333333}
Step 82300: {'loss': 1.4795, 'grad_norm': 5.780580043792725, 'learning_rate': 2.688e-06, 'epoch': 3.6577777777777776}
Step 82400: {'loss': 1.493, 'grad_norm': 6.994561195373535, 'learning_rate': 2.6791111111111113e-06, 'epoch': 3.6622222222222223}
Step 82500: {'loss': 1.503, 'grad_norm': 5.392574787139893, 'learning_rate': 2.6702222222222223e-06, 'epoch': 3.6666666666666665}
Step 82600: {'loss': 1.4787, 'grad_norm': 6.888299942016602, 'learning_rate': 2.6613333333333337e-06, 'epoch': 3.671111111111111}
Step 82700: {'loss': 1.4506, 'grad_norm': 5.4014892578125, 'learning_rate': 2.6524444444444446e-06, 'epoch': 3.6755555555555555}
Step 82800: {'loss': 1.4927, 'grad_norm': 6.886924743652344, 'learning_rate': 2.643555555555556e-06, 'epoch': 3.68}
Step 82900: {'loss': 1.4866, 'grad_norm': 5.084714412689209, 'learning_rate': 2.634666666666667e-06, 'epoch': 3.6844444444444444}
Step 83000: {'loss': 1.4695, 'grad_norm': 6.514251708984375, 'learning_rate': 2.6257777777777783e-06, 'epoch': 3.688888888888889}
Step 83100: {'loss': 1.4788, 'grad_norm': 6.548316478729248, 'learning_rate': 2.6168888888888893e-06, 'epoch': 3.6933333333333334}
Step 83200: {'loss': 1.4959, 'grad_norm': 5.997863292694092, 'learning_rate': 2.608e-06, 'epoch': 3.6977777777777776}
Step 83300: {'loss': 1.5126, 'grad_norm': 6.067978858947754, 'learning_rate': 2.5991111111111112e-06, 'epoch': 3.7022222222222223}
Step 83400: {'loss': 1.4723, 'grad_norm': 5.753020763397217, 'learning_rate': 2.590222222222222e-06, 'epoch': 3.7066666666666666}
Step 83500: {'loss': 1.4457, 'grad_norm': 5.67930793762207, 'learning_rate': 2.5813333333333336e-06, 'epoch': 3.7111111111111112}
Step 83600: {'loss': 1.4706, 'grad_norm': 5.754179000854492, 'learning_rate': 2.5724444444444445e-06, 'epoch': 3.7155555555555555}
Step 83700: {'loss': 1.475, 'grad_norm': 6.850240230560303, 'learning_rate': 2.563555555555556e-06, 'epoch': 3.7199999999999998}
Step 83800: {'loss': 1.4801, 'grad_norm': 6.298110485076904, 'learning_rate': 2.5547555555555557e-06, 'epoch': 3.7244444444444444}
Step 83900: {'loss': 1.4725, 'grad_norm': 5.886129856109619, 'learning_rate': 2.545866666666667e-06, 'epoch': 3.728888888888889}
Step 84000: {'loss': 1.4546, 'grad_norm': 5.315913200378418, 'learning_rate': 2.536977777777778e-06, 'epoch': 3.7333333333333334}
Step 84100: {'loss': 1.445, 'grad_norm': 5.611718654632568, 'learning_rate': 2.5280888888888894e-06, 'epoch': 3.7377777777777776}
Step 84200: {'loss': 1.4497, 'grad_norm': 6.184893608093262, 'learning_rate': 2.5192e-06, 'epoch': 3.7422222222222223}
Step 84300: {'loss': 1.4941, 'grad_norm': 6.290193557739258, 'learning_rate': 2.5103111111111113e-06, 'epoch': 3.7466666666666666}
Step 84400: {'loss': 1.4921, 'grad_norm': 6.400639057159424, 'learning_rate': 2.5014222222222223e-06, 'epoch': 3.7511111111111113}
Step 84500: {'loss': 1.4958, 'grad_norm': 7.843289375305176, 'learning_rate': 2.4925333333333337e-06, 'epoch': 3.7555555555555555}
Step 84600: {'loss': 1.4887, 'grad_norm': 6.301848411560059, 'learning_rate': 2.4836444444444446e-06, 'epoch': 3.76}
Step 84700: {'loss': 1.475, 'grad_norm': 6.512951850891113, 'learning_rate': 2.4747555555555556e-06, 'epoch': 3.7644444444444445}
Step 84800: {'loss': 1.5002, 'grad_norm': 5.72511100769043, 'learning_rate': 2.465866666666667e-06, 'epoch': 3.7688888888888887}
Step 84900: {'loss': 1.4876, 'grad_norm': 4.795119285583496, 'learning_rate': 2.456977777777778e-06, 'epoch': 3.7733333333333334}
Step 85000: {'loss': 1.4629, 'grad_norm': 5.793945789337158, 'learning_rate': 2.448088888888889e-06, 'epoch': 3.7777777777777777}
Step 85100: {'loss': 1.449, 'grad_norm': 5.594667434692383, 'learning_rate': 2.4392000000000003e-06, 'epoch': 3.7822222222222224}
Step 85200: {'loss': 1.4375, 'grad_norm': 5.289131164550781, 'learning_rate': 2.4303111111111113e-06, 'epoch': 3.7866666666666666}
Step 85300: {'loss': 1.4636, 'grad_norm': 6.408015727996826, 'learning_rate': 2.4214222222222226e-06, 'epoch': 3.7911111111111113}
Step 85400: {'loss': 1.4426, 'grad_norm': 6.116429328918457, 'learning_rate': 2.4125333333333336e-06, 'epoch': 3.7955555555555556}
Step 85500: {'loss': 1.4902, 'grad_norm': 5.315277576446533, 'learning_rate': 2.4036444444444446e-06, 'epoch': 3.8}
Step 85600: {'loss': 1.4725, 'grad_norm': 8.665156364440918, 'learning_rate': 2.3947555555555555e-06, 'epoch': 3.8044444444444445}
Step 85700: {'loss': 1.4566, 'grad_norm': 5.568695068359375, 'learning_rate': 2.385866666666667e-06, 'epoch': 3.8088888888888888}
Step 85800: {'loss': 1.5048, 'grad_norm': 5.946971416473389, 'learning_rate': 2.376977777777778e-06, 'epoch': 3.8133333333333335}
Step 85900: {'loss': 1.4609, 'grad_norm': 5.770876407623291, 'learning_rate': 2.368177777777778e-06, 'epoch': 3.8177777777777777}
Step 86000: {'loss': 1.4477, 'grad_norm': 5.67747688293457, 'learning_rate': 2.359288888888889e-06, 'epoch': 3.822222222222222}
Step 86100: {'loss': 1.4871, 'grad_norm': 5.977498531341553, 'learning_rate': 2.3504e-06, 'epoch': 3.8266666666666667}
Step 86200: {'loss': 1.4235, 'grad_norm': 6.229110240936279, 'learning_rate': 2.3415111111111114e-06, 'epoch': 3.8311111111111114}
Step 86300: {'loss': 1.4943, 'grad_norm': 5.816751956939697, 'learning_rate': 2.3326222222222223e-06, 'epoch': 3.8355555555555556}
Step 86400: {'loss': 1.5073, 'grad_norm': 5.453084468841553, 'learning_rate': 2.3237333333333333e-06, 'epoch': 3.84}
Step 86500: {'loss': 1.4915, 'grad_norm': 5.748610019683838, 'learning_rate': 2.3148444444444447e-06, 'epoch': 3.8444444444444446}
Step 86600: {'loss': 1.458, 'grad_norm': 6.5627241134643555, 'learning_rate': 2.3059555555555556e-06, 'epoch': 3.848888888888889}
Step 86700: {'loss': 1.4489, 'grad_norm': 6.02609395980835, 'learning_rate': 2.297066666666667e-06, 'epoch': 3.8533333333333335}
Step 86800: {'loss': 1.4966, 'grad_norm': 5.873152256011963, 'learning_rate': 2.288177777777778e-06, 'epoch': 3.8577777777777778}
Step 86900: {'loss': 1.4618, 'grad_norm': 5.2707200050354, 'learning_rate': 2.279288888888889e-06, 'epoch': 3.862222222222222}
Step 87000: {'loss': 1.4569, 'grad_norm': 5.243300437927246, 'learning_rate': 2.2704e-06, 'epoch': 3.8666666666666667}
Step 87100: {'loss': 1.4992, 'grad_norm': 6.555243492126465, 'learning_rate': 2.2615111111111113e-06, 'epoch': 3.871111111111111}
Step 87200: {'loss': 1.4562, 'grad_norm': 5.864575386047363, 'learning_rate': 2.2526222222222222e-06, 'epoch': 3.8755555555555556}
Step 87300: {'loss': 1.4864, 'grad_norm': 6.064406394958496, 'learning_rate': 2.2437333333333336e-06, 'epoch': 3.88}
Step 87400: {'loss': 1.4683, 'grad_norm': 6.4289326667785645, 'learning_rate': 2.2348444444444446e-06, 'epoch': 3.8844444444444446}
Step 87500: {'loss': 1.4622, 'grad_norm': 6.209162712097168, 'learning_rate': 2.2259555555555555e-06, 'epoch': 3.888888888888889}
Step 87600: {'loss': 1.4986, 'grad_norm': 7.085419654846191, 'learning_rate': 2.217066666666667e-06, 'epoch': 3.8933333333333335}
Step 87700: {'loss': 1.4773, 'grad_norm': 5.4114603996276855, 'learning_rate': 2.208177777777778e-06, 'epoch': 3.897777777777778}
Step 87800: {'loss': 1.4638, 'grad_norm': 6.0949788093566895, 'learning_rate': 2.1992888888888893e-06, 'epoch': 3.902222222222222}
Step 87900: {'loss': 1.4627, 'grad_norm': 6.367434501647949, 'learning_rate': 2.190488888888889e-06, 'epoch': 3.9066666666666667}
Step 88000: {'loss': 1.4783, 'grad_norm': 6.251357078552246, 'learning_rate': 2.1816e-06, 'epoch': 3.911111111111111}
Step 88100: {'loss': 1.448, 'grad_norm': 6.2452616691589355, 'learning_rate': 2.1727111111111114e-06, 'epoch': 3.9155555555555557}
Step 88200: {'loss': 1.4553, 'grad_norm': 7.117393970489502, 'learning_rate': 2.1638222222222223e-06, 'epoch': 3.92}
Step 88300: {'loss': 1.4386, 'grad_norm': 5.848743915557861, 'learning_rate': 2.1549333333333337e-06, 'epoch': 3.924444444444444}
Step 88400: {'loss': 1.5027, 'grad_norm': 5.032512664794922, 'learning_rate': 2.1460444444444447e-06, 'epoch': 3.928888888888889}
Step 88500: {'loss': 1.4922, 'grad_norm': 5.097212791442871, 'learning_rate': 2.1371555555555557e-06, 'epoch': 3.9333333333333336}
Step 88600: {'loss': 1.458, 'grad_norm': 5.168776035308838, 'learning_rate': 2.1282666666666666e-06, 'epoch': 3.937777777777778}
Step 88700: {'loss': 1.4451, 'grad_norm': 7.001770496368408, 'learning_rate': 2.119377777777778e-06, 'epoch': 3.942222222222222}
Step 88800: {'loss': 1.4622, 'grad_norm': 6.935302734375, 'learning_rate': 2.110488888888889e-06, 'epoch': 3.9466666666666668}
Step 88900: {'loss': 1.4653, 'grad_norm': 5.767982482910156, 'learning_rate': 2.1016000000000003e-06, 'epoch': 3.951111111111111}
Step 89000: {'loss': 1.4809, 'grad_norm': 6.1770453453063965, 'learning_rate': 2.0927111111111113e-06, 'epoch': 3.9555555555555557}
Step 89100: {'loss': 1.4501, 'grad_norm': 5.755301475524902, 'learning_rate': 2.0838222222222223e-06, 'epoch': 3.96}
Step 89200: {'loss': 1.4586, 'grad_norm': 5.646642208099365, 'learning_rate': 2.0749333333333336e-06, 'epoch': 3.964444444444444}
Step 89300: {'loss': 1.4917, 'grad_norm': 6.5859222412109375, 'learning_rate': 2.0660444444444446e-06, 'epoch': 3.968888888888889}
Step 89400: {'loss': 1.531, 'grad_norm': 5.752042770385742, 'learning_rate': 2.057155555555556e-06, 'epoch': 3.9733333333333336}
Step 89500: {'loss': 1.4679, 'grad_norm': 5.757594585418701, 'learning_rate': 2.0482666666666665e-06, 'epoch': 3.977777777777778}
Step 89600: {'loss': 1.4878, 'grad_norm': 7.0830864906311035, 'learning_rate': 2.039377777777778e-06, 'epoch': 3.982222222222222}
Step 89700: {'loss': 1.5051, 'grad_norm': 5.22099494934082, 'learning_rate': 2.030488888888889e-06, 'epoch': 3.986666666666667}
Step 89800: {'loss': 1.4563, 'grad_norm': 4.855579376220703, 'learning_rate': 2.0216000000000003e-06, 'epoch': 3.991111111111111}
Step 89900: {'loss': 1.4914, 'grad_norm': 5.169380187988281, 'learning_rate': 2.0128e-06, 'epoch': 3.9955555555555557}
Step 90000: {'loss': 1.4571, 'grad_norm': 5.464121341705322, 'learning_rate': 2.003911111111111e-06, 'epoch': 4.0}
Step 90100: {'loss': 1.475, 'grad_norm': 5.717636585235596, 'learning_rate': 1.9950222222222224e-06, 'epoch': 4.004444444444444}
Step 90200: {'loss': 1.4771, 'grad_norm': 6.452152729034424, 'learning_rate': 1.9861333333333333e-06, 'epoch': 4.0088888888888885}
Step 90300: {'loss': 1.4775, 'grad_norm': 6.054673671722412, 'learning_rate': 1.9772444444444447e-06, 'epoch': 4.013333333333334}
Step 90400: {'loss': 1.4429, 'grad_norm': 6.880422115325928, 'learning_rate': 1.9683555555555557e-06, 'epoch': 4.017777777777778}
Step 90500: {'loss': 1.4251, 'grad_norm': 6.652312755584717, 'learning_rate': 1.9594666666666666e-06, 'epoch': 4.022222222222222}
Step 90600: {'loss': 1.4714, 'grad_norm': 4.691494941711426, 'learning_rate': 1.950577777777778e-06, 'epoch': 4.026666666666666}
Step 90700: {'loss': 1.4818, 'grad_norm': 6.9395527839660645, 'learning_rate': 1.941688888888889e-06, 'epoch': 4.0311111111111115}
Step 90800: {'loss': 1.4506, 'grad_norm': 6.544039249420166, 'learning_rate': 1.9328000000000004e-06, 'epoch': 4.035555555555556}
Step 90900: {'loss': 1.4484, 'grad_norm': 5.471756458282471, 'learning_rate': 1.9239111111111113e-06, 'epoch': 4.04}
Step 91000: {'loss': 1.4338, 'grad_norm': 7.148268222808838, 'learning_rate': 1.9150222222222223e-06, 'epoch': 4.044444444444444}
Step 91100: {'loss': 1.4738, 'grad_norm': 5.525921821594238, 'learning_rate': 1.9061333333333335e-06, 'epoch': 4.0488888888888885}
Step 91200: {'loss': 1.4746, 'grad_norm': 5.404995918273926, 'learning_rate': 1.8972444444444446e-06, 'epoch': 4.053333333333334}
Step 91300: {'loss': 1.4833, 'grad_norm': 6.0300469398498535, 'learning_rate': 1.8883555555555558e-06, 'epoch': 4.057777777777778}
Step 91400: {'loss': 1.4473, 'grad_norm': 5.615963935852051, 'learning_rate': 1.879466666666667e-06, 'epoch': 4.062222222222222}
Step 91500: {'loss': 1.4297, 'grad_norm': 5.653198719024658, 'learning_rate': 1.8705777777777777e-06, 'epoch': 4.066666666666666}
Step 91600: {'loss': 1.4834, 'grad_norm': 5.966580867767334, 'learning_rate': 1.861688888888889e-06, 'epoch': 4.071111111111111}
Step 91700: {'loss': 1.4748, 'grad_norm': 6.429533004760742, 'learning_rate': 1.8528e-06, 'epoch': 4.075555555555556}
Step 91800: {'loss': 1.4524, 'grad_norm': 5.717661380767822, 'learning_rate': 1.8439111111111112e-06, 'epoch': 4.08}
Step 91900: {'loss': 1.4398, 'grad_norm': 6.148539066314697, 'learning_rate': 1.8351111111111114e-06, 'epoch': 4.084444444444444}
Step 92000: {'loss': 1.4411, 'grad_norm': 6.0812087059021, 'learning_rate': 1.8262222222222222e-06, 'epoch': 4.088888888888889}
Step 92100: {'loss': 1.4979, 'grad_norm': 5.202348709106445, 'learning_rate': 1.8173333333333334e-06, 'epoch': 4.093333333333334}
Step 92200: {'loss': 1.4595, 'grad_norm': 5.843108177185059, 'learning_rate': 1.8084444444444445e-06, 'epoch': 4.097777777777778}
Step 92300: {'loss': 1.4591, 'grad_norm': 6.138033866882324, 'learning_rate': 1.7995555555555557e-06, 'epoch': 4.102222222222222}
Step 92400: {'loss': 1.4759, 'grad_norm': 6.948319911956787, 'learning_rate': 1.7906666666666669e-06, 'epoch': 4.1066666666666665}
Step 92500: {'loss': 1.499, 'grad_norm': 5.580836772918701, 'learning_rate': 1.7817777777777778e-06, 'epoch': 4.111111111111111}
Step 92600: {'loss': 1.4998, 'grad_norm': 5.709314346313477, 'learning_rate': 1.772888888888889e-06, 'epoch': 4.115555555555556}
Step 92700: {'loss': 1.4919, 'grad_norm': 5.595238208770752, 'learning_rate': 1.7640000000000002e-06, 'epoch': 4.12}
Step 92800: {'loss': 1.484, 'grad_norm': 5.944914817810059, 'learning_rate': 1.7551111111111114e-06, 'epoch': 4.124444444444444}
Step 92900: {'loss': 1.472, 'grad_norm': 4.783907890319824, 'learning_rate': 1.7462222222222225e-06, 'epoch': 4.128888888888889}
Step 93000: {'loss': 1.4611, 'grad_norm': 6.931858062744141, 'learning_rate': 1.7373333333333333e-06, 'epoch': 4.133333333333334}
Step 93100: {'loss': 1.4456, 'grad_norm': 5.4562602043151855, 'learning_rate': 1.7284444444444444e-06, 'epoch': 4.137777777777778}
Step 93200: {'loss': 1.4749, 'grad_norm': 6.857855796813965, 'learning_rate': 1.7195555555555556e-06, 'epoch': 4.142222222222222}
Step 93300: {'loss': 1.4431, 'grad_norm': 5.794741630554199, 'learning_rate': 1.7106666666666668e-06, 'epoch': 4.1466666666666665}
Step 93400: {'loss': 1.4337, 'grad_norm': 5.899787425994873, 'learning_rate': 1.701777777777778e-06, 'epoch': 4.151111111111111}
Step 93500: {'loss': 1.4631, 'grad_norm': 6.777344703674316, 'learning_rate': 1.692888888888889e-06, 'epoch': 4.155555555555556}
Step 93600: {'loss': 1.4727, 'grad_norm': 5.255881309509277, 'learning_rate': 1.684e-06, 'epoch': 4.16}
Step 93700: {'loss': 1.4623, 'grad_norm': 6.042337894439697, 'learning_rate': 1.6751111111111113e-06, 'epoch': 4.164444444444444}
Step 93800: {'loss': 1.4312, 'grad_norm': 5.695792198181152, 'learning_rate': 1.6662222222222224e-06, 'epoch': 4.168888888888889}
Step 93900: {'loss': 1.4357, 'grad_norm': 5.632971286773682, 'learning_rate': 1.6573333333333336e-06, 'epoch': 4.173333333333334}
Step 94000: {'loss': 1.4681, 'grad_norm': 6.096491813659668, 'learning_rate': 1.6485333333333334e-06, 'epoch': 4.177777777777778}
Step 94100: {'loss': 1.4901, 'grad_norm': 6.124307632446289, 'learning_rate': 1.6396444444444446e-06, 'epoch': 4.182222222222222}
Step 94200: {'loss': 1.4659, 'grad_norm': 5.642000198364258, 'learning_rate': 1.6307555555555557e-06, 'epoch': 4.1866666666666665}
Step 94300: {'loss': 1.4485, 'grad_norm': 5.49066686630249, 'learning_rate': 1.621866666666667e-06, 'epoch': 4.191111111111111}
Step 94400: {'loss': 1.462, 'grad_norm': 5.986698627471924, 'learning_rate': 1.612977777777778e-06, 'epoch': 4.195555555555556}
Step 94500: {'loss': 1.4516, 'grad_norm': 5.647345066070557, 'learning_rate': 1.6040888888888888e-06, 'epoch': 4.2}
Step 94600: {'loss': 1.4975, 'grad_norm': 6.966411590576172, 'learning_rate': 1.5952e-06, 'epoch': 4.204444444444444}
Step 94700: {'loss': 1.446, 'grad_norm': 6.3130340576171875, 'learning_rate': 1.5863111111111112e-06, 'epoch': 4.208888888888889}
Step 94800: {'loss': 1.4784, 'grad_norm': 6.474659442901611, 'learning_rate': 1.5774222222222223e-06, 'epoch': 4.213333333333333}
Step 94900: {'loss': 1.4629, 'grad_norm': 4.534820556640625, 'learning_rate': 1.5685333333333335e-06, 'epoch': 4.217777777777778}
Step 95000: {'loss': 1.4774, 'grad_norm': 5.672637462615967, 'learning_rate': 1.5596444444444445e-06, 'epoch': 4.222222222222222}
Step 95100: {'loss': 1.4757, 'grad_norm': 7.0561323165893555, 'learning_rate': 1.5507555555555556e-06, 'epoch': 4.226666666666667}
Step 95200: {'loss': 1.4662, 'grad_norm': 5.929336071014404, 'learning_rate': 1.5418666666666668e-06, 'epoch': 4.231111111111111}
Step 95300: {'loss': 1.4338, 'grad_norm': 5.334821701049805, 'learning_rate': 1.532977777777778e-06, 'epoch': 4.235555555555556}
Step 95400: {'loss': 1.4262, 'grad_norm': 5.582449436187744, 'learning_rate': 1.5240888888888892e-06, 'epoch': 4.24}
Step 95500: {'loss': 1.4559, 'grad_norm': 6.467552661895752, 'learning_rate': 1.5152e-06, 'epoch': 4.2444444444444445}
Step 95600: {'loss': 1.4511, 'grad_norm': 6.5976715087890625, 'learning_rate': 1.506311111111111e-06, 'epoch': 4.248888888888889}
Step 95700: {'loss': 1.4718, 'grad_norm': 5.364105701446533, 'learning_rate': 1.4974222222222223e-06, 'epoch': 4.253333333333333}
Step 95800: {'loss': 1.4919, 'grad_norm': 6.4009108543396, 'learning_rate': 1.4885333333333334e-06, 'epoch': 4.257777777777778}
Step 95900: {'loss': 1.4705, 'grad_norm': 5.448566913604736, 'learning_rate': 1.4796444444444446e-06, 'epoch': 4.262222222222222}
Step 96000: {'loss': 1.4433, 'grad_norm': 5.5686936378479, 'learning_rate': 1.4708444444444444e-06, 'epoch': 4.266666666666667}
Step 96100: {'loss': 1.4672, 'grad_norm': 5.567440509796143, 'learning_rate': 1.4619555555555555e-06, 'epoch': 4.271111111111111}
Step 96200: {'loss': 1.4427, 'grad_norm': 4.823049068450928, 'learning_rate': 1.4530666666666667e-06, 'epoch': 4.275555555555556}
Step 96300: {'loss': 1.4942, 'grad_norm': 6.498258590698242, 'learning_rate': 1.4441777777777779e-06, 'epoch': 4.28}
Step 96400: {'loss': 1.4476, 'grad_norm': 5.807685852050781, 'learning_rate': 1.435288888888889e-06, 'epoch': 4.2844444444444445}
Step 96500: {'loss': 1.4688, 'grad_norm': 6.143428802490234, 'learning_rate': 1.4264e-06, 'epoch': 4.288888888888889}
Step 96600: {'loss': 1.4512, 'grad_norm': 6.4391679763793945, 'learning_rate': 1.4175111111111112e-06, 'epoch': 4.293333333333333}
Step 96700: {'loss': 1.4704, 'grad_norm': 6.691664695739746, 'learning_rate': 1.4086222222222224e-06, 'epoch': 4.297777777777778}
Step 96800: {'loss': 1.45, 'grad_norm': 5.731221675872803, 'learning_rate': 1.3997333333333335e-06, 'epoch': 4.302222222222222}
Step 96900: {'loss': 1.4383, 'grad_norm': 6.569011688232422, 'learning_rate': 1.3908444444444447e-06, 'epoch': 4.306666666666667}
Step 97000: {'loss': 1.4699, 'grad_norm': 5.808239936828613, 'learning_rate': 1.3819555555555555e-06, 'epoch': 4.311111111111111}
Step 97100: {'loss': 1.5139, 'grad_norm': 5.99342155456543, 'learning_rate': 1.3730666666666666e-06, 'epoch': 4.315555555555555}
Step 97200: {'loss': 1.486, 'grad_norm': 6.331967830657959, 'learning_rate': 1.3641777777777778e-06, 'epoch': 4.32}
Step 97300: {'loss': 1.4677, 'grad_norm': 5.728707790374756, 'learning_rate': 1.355288888888889e-06, 'epoch': 4.3244444444444445}
Step 97400: {'loss': 1.4675, 'grad_norm': 5.79250955581665, 'learning_rate': 1.3464000000000001e-06, 'epoch': 4.328888888888889}
Step 97500: {'loss': 1.4409, 'grad_norm': 5.870486259460449, 'learning_rate': 1.337511111111111e-06, 'epoch': 4.333333333333333}
Step 97600: {'loss': 1.4271, 'grad_norm': 6.207481861114502, 'learning_rate': 1.3286222222222223e-06, 'epoch': 4.337777777777778}
Step 97700: {'loss': 1.4313, 'grad_norm': 4.744669437408447, 'learning_rate': 1.3197333333333335e-06, 'epoch': 4.342222222222222}
Step 97800: {'loss': 1.4919, 'grad_norm': 8.293895721435547, 'learning_rate': 1.3108444444444446e-06, 'epoch': 4.346666666666667}
Step 97900: {'loss': 1.487, 'grad_norm': 6.367178916931152, 'learning_rate': 1.3019555555555558e-06, 'epoch': 4.351111111111111}
Step 98000: {'loss': 1.4556, 'grad_norm': 5.550983905792236, 'learning_rate': 1.2930666666666665e-06, 'epoch': 4.355555555555555}
Step 98100: {'loss': 1.4564, 'grad_norm': 4.73635721206665, 'learning_rate': 1.2842666666666667e-06, 'epoch': 4.36}
Step 98200: {'loss': 1.4653, 'grad_norm': 6.090312480926514, 'learning_rate': 1.275377777777778e-06, 'epoch': 4.364444444444445}
Step 98300: {'loss': 1.4457, 'grad_norm': 4.916983127593994, 'learning_rate': 1.266488888888889e-06, 'epoch': 4.368888888888889}
Step 98400: {'loss': 1.4645, 'grad_norm': 4.965715408325195, 'learning_rate': 1.2576000000000002e-06, 'epoch': 4.373333333333333}
Step 98500: {'loss': 1.4819, 'grad_norm': 5.9778008460998535, 'learning_rate': 1.2487111111111112e-06, 'epoch': 4.377777777777778}
Step 98600: {'loss': 1.4495, 'grad_norm': 5.598294258117676, 'learning_rate': 1.2398222222222222e-06, 'epoch': 4.3822222222222225}
Step 98700: {'loss': 1.4576, 'grad_norm': 4.552911281585693, 'learning_rate': 1.2309333333333333e-06, 'epoch': 4.386666666666667}
Step 98800: {'loss': 1.4662, 'grad_norm': 5.565591812133789, 'learning_rate': 1.2220444444444445e-06, 'epoch': 4.391111111111111}
Step 98900: {'loss': 1.4588, 'grad_norm': 5.745782375335693, 'learning_rate': 1.2131555555555557e-06, 'epoch': 4.395555555555555}
Step 99000: {'loss': 1.4596, 'grad_norm': 6.320018768310547, 'learning_rate': 1.2042666666666669e-06, 'epoch': 4.4}
Step 99100: {'loss': 1.4774, 'grad_norm': 6.994563102722168, 'learning_rate': 1.195377777777778e-06, 'epoch': 4.404444444444445}
Step 99200: {'loss': 1.4615, 'grad_norm': 5.355563640594482, 'learning_rate': 1.186488888888889e-06, 'epoch': 4.408888888888889}
Step 99300: {'loss': 1.4436, 'grad_norm': 6.470183849334717, 'learning_rate': 1.1776000000000002e-06, 'epoch': 4.413333333333333}
Step 99400: {'loss': 1.4657, 'grad_norm': 5.527093887329102, 'learning_rate': 1.1687111111111111e-06, 'epoch': 4.417777777777777}
Step 99500: {'loss': 1.4507, 'grad_norm': 5.246734619140625, 'learning_rate': 1.1598222222222223e-06, 'epoch': 4.4222222222222225}
Step 99600: {'loss': 1.4396, 'grad_norm': 5.317460536956787, 'learning_rate': 1.1509333333333335e-06, 'epoch': 4.426666666666667}
Step 99700: {'loss': 1.4598, 'grad_norm': 6.125283241271973, 'learning_rate': 1.1420444444444444e-06, 'epoch': 4.431111111111111}
Step 99800: {'loss': 1.4529, 'grad_norm': 5.926527976989746, 'learning_rate': 1.1331555555555556e-06, 'epoch': 4.435555555555555}
Step 99900: {'loss': 1.4516, 'grad_norm': 5.542158603668213, 'learning_rate': 1.1242666666666668e-06, 'epoch': 4.44}
Step 100000: {'loss': 1.4838, 'grad_norm': 6.173229694366455, 'learning_rate': 1.115377777777778e-06, 'epoch': 4.444444444444445}
Step 100100: {'loss': 1.4304, 'grad_norm': 5.433622360229492, 'learning_rate': 1.1065777777777777e-06, 'epoch': 4.448888888888889}
Step 100200: {'loss': 1.4557, 'grad_norm': 5.67618465423584, 'learning_rate': 1.0976888888888889e-06, 'epoch': 4.453333333333333}
Step 100300: {'loss': 1.4545, 'grad_norm': 5.997843265533447, 'learning_rate': 1.0888e-06, 'epoch': 4.457777777777777}
Step 100400: {'loss': 1.4768, 'grad_norm': 7.00557279586792, 'learning_rate': 1.0799111111111112e-06, 'epoch': 4.4622222222222225}
Step 100500: {'loss': 1.4533, 'grad_norm': 5.914608955383301, 'learning_rate': 1.0710222222222224e-06, 'epoch': 4.466666666666667}
Step 100600: {'loss': 1.4694, 'grad_norm': 5.21259069442749, 'learning_rate': 1.0621333333333336e-06, 'epoch': 4.471111111111111}
Step 100700: {'loss': 1.4666, 'grad_norm': 6.129192352294922, 'learning_rate': 1.0532444444444445e-06, 'epoch': 4.475555555555555}
Step 100800: {'loss': 1.44, 'grad_norm': 6.293127059936523, 'learning_rate': 1.0443555555555557e-06, 'epoch': 4.48}
Step 100900: {'loss': 1.4637, 'grad_norm': 6.08936882019043, 'learning_rate': 1.0354666666666667e-06, 'epoch': 4.484444444444445}
Step 101000: {'loss': 1.4635, 'grad_norm': 5.724727153778076, 'learning_rate': 1.0265777777777778e-06, 'epoch': 4.488888888888889}
Step 101100: {'loss': 1.5004, 'grad_norm': 5.614858627319336, 'learning_rate': 1.017688888888889e-06, 'epoch': 4.493333333333333}
Step 101200: {'loss': 1.4722, 'grad_norm': 6.1114301681518555, 'learning_rate': 1.0088e-06, 'epoch': 4.497777777777777}
Step 101300: {'loss': 1.4943, 'grad_norm': 5.086793899536133, 'learning_rate': 9.999111111111112e-07, 'epoch': 4.502222222222223}
Step 101400: {'loss': 1.4903, 'grad_norm': 5.553809642791748, 'learning_rate': 9.910222222222223e-07, 'epoch': 4.506666666666667}
Step 101500: {'loss': 1.4893, 'grad_norm': 5.438779354095459, 'learning_rate': 9.821333333333335e-07, 'epoch': 4.511111111111111}
Step 101600: {'loss': 1.4356, 'grad_norm': 7.456002712249756, 'learning_rate': 9.732444444444447e-07, 'epoch': 4.515555555555555}
Step 101700: {'loss': 1.4746, 'grad_norm': 5.3717546463012695, 'learning_rate': 9.643555555555556e-07, 'epoch': 4.52}
Step 101800: {'loss': 1.457, 'grad_norm': 6.589951038360596, 'learning_rate': 9.554666666666668e-07, 'epoch': 4.524444444444445}
Step 101900: {'loss': 1.4586, 'grad_norm': 5.940182209014893, 'learning_rate': 9.465777777777778e-07, 'epoch': 4.528888888888889}
Step 102000: {'loss': 1.4985, 'grad_norm': 6.582196235656738, 'learning_rate': 9.376888888888889e-07, 'epoch': 4.533333333333333}
Step 102100: {'loss': 1.4827, 'grad_norm': 5.004903316497803, 'learning_rate': 9.288000000000001e-07, 'epoch': 4.5377777777777775}
Step 102200: {'loss': 1.4684, 'grad_norm': 4.974508762359619, 'learning_rate': 9.200000000000001e-07, 'epoch': 4.542222222222223}
Step 102300: {'loss': 1.4646, 'grad_norm': 5.590771675109863, 'learning_rate': 9.111111111111113e-07, 'epoch': 4.546666666666667}
Step 102400: {'loss': 1.4842, 'grad_norm': 6.194372177124023, 'learning_rate': 9.022222222222222e-07, 'epoch': 4.551111111111111}
Step 102500: {'loss': 1.517, 'grad_norm': 5.559220314025879, 'learning_rate': 8.933333333333334e-07, 'epoch': 4.555555555555555}
Step 102600: {'loss': 1.4643, 'grad_norm': 6.595807075500488, 'learning_rate': 8.844444444444446e-07, 'epoch': 4.5600000000000005}
Step 102700: {'loss': 1.4616, 'grad_norm': 4.539565563201904, 'learning_rate': 8.755555555555556e-07, 'epoch': 4.564444444444445}
Step 102800: {'loss': 1.4562, 'grad_norm': 5.117699146270752, 'learning_rate': 8.666666666666668e-07, 'epoch': 4.568888888888889}
Step 102900: {'loss': 1.4698, 'grad_norm': 6.817371368408203, 'learning_rate': 8.577777777777778e-07, 'epoch': 4.573333333333333}
Step 103000: {'loss': 1.4503, 'grad_norm': 5.118162155151367, 'learning_rate': 8.488888888888889e-07, 'epoch': 4.5777777777777775}
Step 103100: {'loss': 1.4471, 'grad_norm': 5.916085720062256, 'learning_rate': 8.400000000000001e-07, 'epoch': 4.582222222222223}
Step 103200: {'loss': 1.4483, 'grad_norm': 5.5114617347717285, 'learning_rate': 8.311111111111112e-07, 'epoch': 4.586666666666667}
Step 103300: {'loss': 1.4952, 'grad_norm': 5.606804847717285, 'learning_rate': 8.222222222222223e-07, 'epoch': 4.591111111111111}
Step 103400: {'loss': 1.4781, 'grad_norm': 6.051234245300293, 'learning_rate': 8.133333333333333e-07, 'epoch': 4.595555555555555}
Step 103500: {'loss': 1.4686, 'grad_norm': 5.780882358551025, 'learning_rate': 8.044444444444445e-07, 'epoch': 4.6}
Step 103600: {'loss': 1.4601, 'grad_norm': 5.271137237548828, 'learning_rate': 7.955555555555557e-07, 'epoch': 4.604444444444445}
Step 103700: {'loss': 1.4481, 'grad_norm': 5.567619323730469, 'learning_rate': 7.866666666666667e-07, 'epoch': 4.608888888888889}
Step 103800: {'loss': 1.4823, 'grad_norm': 6.3453593254089355, 'learning_rate': 7.777777777777779e-07, 'epoch': 4.613333333333333}
Step 103900: {'loss': 1.5016, 'grad_norm': 7.1086578369140625, 'learning_rate': 7.688888888888891e-07, 'epoch': 4.6177777777777775}
Step 104000: {'loss': 1.4508, 'grad_norm': 5.673357009887695, 'learning_rate': 7.6e-07, 'epoch': 4.622222222222222}
Step 104100: {'loss': 1.4438, 'grad_norm': 5.108613967895508, 'learning_rate': 7.511111111111112e-07, 'epoch': 4.626666666666667}
Step 104200: {'loss': 1.4719, 'grad_norm': 5.772858142852783, 'learning_rate': 7.422222222222223e-07, 'epoch': 4.631111111111111}
Step 104300: {'loss': 1.4403, 'grad_norm': 5.130529403686523, 'learning_rate': 7.334222222222223e-07, 'epoch': 4.635555555555555}
Step 104400: {'loss': 1.4688, 'grad_norm': 6.230302333831787, 'learning_rate': 7.245333333333333e-07, 'epoch': 4.64}
Step 104500: {'loss': 1.4749, 'grad_norm': 6.764962673187256, 'learning_rate': 7.156444444444445e-07, 'epoch': 4.644444444444445}
Step 104600: {'loss': 1.4537, 'grad_norm': 5.979761123657227, 'learning_rate': 7.067555555555557e-07, 'epoch': 4.648888888888889}
Step 104700: {'loss': 1.4421, 'grad_norm': 5.448606014251709, 'learning_rate': 6.978666666666667e-07, 'epoch': 4.653333333333333}
Step 104800: {'loss': 1.4169, 'grad_norm': 5.669739246368408, 'learning_rate': 6.889777777777779e-07, 'epoch': 4.657777777777778}
Step 104900: {'loss': 1.4689, 'grad_norm': 6.838133811950684, 'learning_rate': 6.800888888888889e-07, 'epoch': 4.662222222222223}
Step 105000: {'loss': 1.4356, 'grad_norm': 6.236388206481934, 'learning_rate': 6.712e-07, 'epoch': 4.666666666666667}
Step 105100: {'loss': 1.4631, 'grad_norm': 6.403168201446533, 'learning_rate': 6.623111111111112e-07, 'epoch': 4.671111111111111}
Step 105200: {'loss': 1.4594, 'grad_norm': 6.411676406860352, 'learning_rate': 6.534222222222223e-07, 'epoch': 4.6755555555555555}
Step 105300: {'loss': 1.4775, 'grad_norm': 5.863963603973389, 'learning_rate': 6.445333333333334e-07, 'epoch': 4.68}
Step 105400: {'loss': 1.4049, 'grad_norm': 5.757351875305176, 'learning_rate': 6.356444444444446e-07, 'epoch': 4.684444444444445}
Step 105500: {'loss': 1.4706, 'grad_norm': 5.668099880218506, 'learning_rate': 6.267555555555556e-07, 'epoch': 4.688888888888889}
Step 105600: {'loss': 1.467, 'grad_norm': 6.80448055267334, 'learning_rate': 6.178666666666666e-07, 'epoch': 4.693333333333333}
Step 105700: {'loss': 1.4847, 'grad_norm': 5.574192523956299, 'learning_rate': 6.089777777777778e-07, 'epoch': 4.697777777777778}
Step 105800: {'loss': 1.4723, 'grad_norm': 5.63320779800415, 'learning_rate': 6.00088888888889e-07, 'epoch': 4.702222222222222}
Step 105900: {'loss': 1.4804, 'grad_norm': 6.713346481323242, 'learning_rate': 5.912e-07, 'epoch': 4.706666666666667}
Step 106000: {'loss': 1.4268, 'grad_norm': 6.701361656188965, 'learning_rate': 5.823111111111111e-07, 'epoch': 4.711111111111111}
Step 106100: {'loss': 1.474, 'grad_norm': 6.0786943435668945, 'learning_rate': 5.734222222222223e-07, 'epoch': 4.7155555555555555}
Step 106200: {'loss': 1.504, 'grad_norm': 5.86102819442749, 'learning_rate': 5.645333333333334e-07, 'epoch': 4.72}
Step 106300: {'loss': 1.4645, 'grad_norm': 6.012783527374268, 'learning_rate': 5.557333333333334e-07, 'epoch': 4.724444444444444}
Step 106400: {'loss': 1.4729, 'grad_norm': 6.6905741691589355, 'learning_rate': 5.468444444444445e-07, 'epoch': 4.728888888888889}
Step 106500: {'loss': 1.4682, 'grad_norm': 6.413095474243164, 'learning_rate': 5.379555555555556e-07, 'epoch': 4.733333333333333}
Step 106600: {'loss': 1.4523, 'grad_norm': 5.00433349609375, 'learning_rate': 5.290666666666666e-07, 'epoch': 4.737777777777778}
Step 106700: {'loss': 1.4804, 'grad_norm': 6.90960168838501, 'learning_rate': 5.201777777777778e-07, 'epoch': 4.742222222222222}
Step 106800: {'loss': 1.475, 'grad_norm': 5.061540603637695, 'learning_rate': 5.11288888888889e-07, 'epoch': 4.746666666666667}
Step 106900: {'loss': 1.4791, 'grad_norm': 7.02651309967041, 'learning_rate': 5.024e-07, 'epoch': 4.751111111111111}
Step 107000: {'loss': 1.4337, 'grad_norm': 6.744498252868652, 'learning_rate': 4.935111111111111e-07, 'epoch': 4.7555555555555555}
Step 107100: {'loss': 1.4524, 'grad_norm': 8.213024139404297, 'learning_rate': 4.846222222222222e-07, 'epoch': 4.76}
Step 107200: {'loss': 1.449, 'grad_norm': 5.174575328826904, 'learning_rate': 4.757333333333334e-07, 'epoch': 4.764444444444445}
Step 107300: {'loss': 1.4765, 'grad_norm': 6.089974880218506, 'learning_rate': 4.668444444444445e-07, 'epoch': 4.768888888888889}
Step 107400: {'loss': 1.4702, 'grad_norm': 8.099469184875488, 'learning_rate': 4.579555555555556e-07, 'epoch': 4.773333333333333}
Step 107500: {'loss': 1.4636, 'grad_norm': 6.453033447265625, 'learning_rate': 4.4906666666666666e-07, 'epoch': 4.777777777777778}
Step 107600: {'loss': 1.4333, 'grad_norm': 6.242104530334473, 'learning_rate': 4.4017777777777784e-07, 'epoch': 4.782222222222222}
Step 107700: {'loss': 1.4514, 'grad_norm': 6.011796951293945, 'learning_rate': 4.3128888888888896e-07, 'epoch': 4.786666666666667}
Step 107800: {'loss': 1.4703, 'grad_norm': 5.579388618469238, 'learning_rate': 4.224e-07, 'epoch': 4.791111111111111}
Step 107900: {'loss': 1.4544, 'grad_norm': 5.958630561828613, 'learning_rate': 4.1351111111111114e-07, 'epoch': 4.795555555555556}
Step 108000: {'loss': 1.4497, 'grad_norm': 6.977032661437988, 'learning_rate': 4.046222222222222e-07, 'epoch': 4.8}
Step 108100: {'loss': 1.4882, 'grad_norm': 7.174180507659912, 'learning_rate': 3.957333333333334e-07, 'epoch': 4.804444444444444}
Step 108200: {'loss': 1.4488, 'grad_norm': 6.275386333465576, 'learning_rate': 3.868444444444445e-07, 'epoch': 4.808888888888889}
Step 108300: {'loss': 1.4651, 'grad_norm': 8.35708999633789, 'learning_rate': 3.780444444444445e-07, 'epoch': 4.8133333333333335}
Step 108400: {'loss': 1.4443, 'grad_norm': 6.147104263305664, 'learning_rate': 3.691555555555556e-07, 'epoch': 4.817777777777778}
Step 108500: {'loss': 1.4627, 'grad_norm': 5.720384120941162, 'learning_rate': 3.6026666666666666e-07, 'epoch': 4.822222222222222}
Step 108600: {'loss': 1.454, 'grad_norm': 6.762266635894775, 'learning_rate': 3.513777777777778e-07, 'epoch': 4.826666666666666}
Step 108700: {'loss': 1.5007, 'grad_norm': 5.2505669593811035, 'learning_rate': 3.4248888888888896e-07, 'epoch': 4.831111111111111}
Step 108800: {'loss': 1.4619, 'grad_norm': 6.249788761138916, 'learning_rate': 3.336e-07, 'epoch': 4.835555555555556}
Step 108900: {'loss': 1.4586, 'grad_norm': 6.55705451965332, 'learning_rate': 3.2471111111111114e-07, 'epoch': 4.84}
Step 109000: {'loss': 1.4512, 'grad_norm': 5.370385646820068, 'learning_rate': 3.158222222222222e-07, 'epoch': 4.844444444444444}
Step 109100: {'loss': 1.4521, 'grad_norm': 5.679711818695068, 'learning_rate': 3.0693333333333333e-07, 'epoch': 4.848888888888889}
Step 109200: {'loss': 1.4326, 'grad_norm': 6.2606611251831055, 'learning_rate': 2.980444444444445e-07, 'epoch': 4.8533333333333335}
Step 109300: {'loss': 1.4363, 'grad_norm': 5.739768028259277, 'learning_rate': 2.8915555555555557e-07, 'epoch': 4.857777777777778}
Step 109400: {'loss': 1.4266, 'grad_norm': 6.105921268463135, 'learning_rate': 2.802666666666667e-07, 'epoch': 4.862222222222222}
Step 109500: {'loss': 1.4389, 'grad_norm': 7.147911548614502, 'learning_rate': 2.713777777777778e-07, 'epoch': 4.866666666666667}
Step 109600: {'loss': 1.4399, 'grad_norm': 6.3040547370910645, 'learning_rate': 2.624888888888889e-07, 'epoch': 4.871111111111111}
Step 109700: {'loss': 1.4749, 'grad_norm': 6.282522201538086, 'learning_rate': 2.5360000000000005e-07, 'epoch': 4.875555555555556}
Step 109800: {'loss': 1.4486, 'grad_norm': 5.767916202545166, 'learning_rate': 2.447111111111111e-07, 'epoch': 4.88}
Step 109900: {'loss': 1.4219, 'grad_norm': 5.848240852355957, 'learning_rate': 2.3582222222222226e-07, 'epoch': 4.884444444444444}
Step 110000: {'loss': 1.4792, 'grad_norm': 6.074346542358398, 'learning_rate': 2.2693333333333335e-07, 'epoch': 4.888888888888889}
Step 110100: {'loss': 1.4666, 'grad_norm': 5.611324787139893, 'learning_rate': 2.1804444444444447e-07, 'epoch': 4.8933333333333335}
Step 110200: {'loss': 1.4658, 'grad_norm': 5.431663513183594, 'learning_rate': 2.0915555555555557e-07, 'epoch': 4.897777777777778}
Step 110300: {'loss': 1.4439, 'grad_norm': 6.112133502960205, 'learning_rate': 2.003555555555556e-07, 'epoch': 4.902222222222222}
Step 110400: {'loss': 1.477, 'grad_norm': 6.4361443519592285, 'learning_rate': 1.914666666666667e-07, 'epoch': 4.906666666666666}
Step 110500: {'loss': 1.4887, 'grad_norm': 6.13197135925293, 'learning_rate': 1.825777777777778e-07, 'epoch': 4.911111111111111}
Step 110600: {'loss': 1.4438, 'grad_norm': 5.580016613006592, 'learning_rate': 1.736888888888889e-07, 'epoch': 4.915555555555556}
Step 110700: {'loss': 1.4772, 'grad_norm': 7.236052513122559, 'learning_rate': 1.6480000000000002e-07, 'epoch': 4.92}
Step 110800: {'loss': 1.4895, 'grad_norm': 5.264605522155762, 'learning_rate': 1.559111111111111e-07, 'epoch': 4.924444444444444}
Step 110900: {'loss': 1.4528, 'grad_norm': 5.911496162414551, 'learning_rate': 1.4702222222222223e-07, 'epoch': 4.928888888888888}
Step 111000: {'loss': 1.4228, 'grad_norm': 5.179041385650635, 'learning_rate': 1.3813333333333335e-07, 'epoch': 4.933333333333334}
Step 111100: {'loss': 1.4445, 'grad_norm': 7.6038384437561035, 'learning_rate': 1.2924444444444447e-07, 'epoch': 4.937777777777778}
Step 111200: {'loss': 1.4488, 'grad_norm': 6.782458305358887, 'learning_rate': 1.2035555555555557e-07, 'epoch': 4.942222222222222}
Step 111300: {'loss': 1.4537, 'grad_norm': 7.120882987976074, 'learning_rate': 1.1146666666666667e-07, 'epoch': 4.946666666666666}
Step 111400: {'loss': 1.4725, 'grad_norm': 5.992689609527588, 'learning_rate': 1.0257777777777778e-07, 'epoch': 4.9511111111111115}
Step 111500: {'loss': 1.4354, 'grad_norm': 6.698713779449463, 'learning_rate': 9.368888888888889e-08, 'epoch': 4.955555555555556}
Step 111600: {'loss': 1.4497, 'grad_norm': 6.13614559173584, 'learning_rate': 8.48e-08, 'epoch': 4.96}
Step 111700: {'loss': 1.4338, 'grad_norm': 5.799744606018066, 'learning_rate': 7.591111111111111e-08, 'epoch': 4.964444444444444}
Step 111800: {'loss': 1.4809, 'grad_norm': 5.525567054748535, 'learning_rate': 6.702222222222223e-08, 'epoch': 4.968888888888889}
Step 111900: {'loss': 1.4226, 'grad_norm': 5.766046524047852, 'learning_rate': 5.813333333333334e-08, 'epoch': 4.973333333333334}
Step 112000: {'loss': 1.4936, 'grad_norm': 5.9698615074157715, 'learning_rate': 4.924444444444445e-08, 'epoch': 4.977777777777778}
Step 112100: {'loss': 1.4399, 'grad_norm': 6.404995918273926, 'learning_rate': 4.035555555555556e-08, 'epoch': 4.982222222222222}
Step 112200: {'loss': 1.4686, 'grad_norm': 5.676665306091309, 'learning_rate': 3.146666666666667e-08, 'epoch': 4.986666666666666}
Step 112300: {'loss': 1.4616, 'grad_norm': 5.009476661682129, 'learning_rate': 2.266666666666667e-08, 'epoch': 4.9911111111111115}
Step 112400: {'loss': 1.4688, 'grad_norm': 6.2681756019592285, 'learning_rate': 1.3777777777777778e-08, 'epoch': 4.995555555555556}
Step 112500: {'loss': 1.444, 'grad_norm': 7.130458354949951, 'learning_rate': 4.888888888888889e-09, 'epoch': 5.0}
Step 112500: {'train_runtime': 27163.5035, 'train_samples_per_second': 16.566, 'train_steps_per_second': 4.142, 'total_flos': 9.1845906845184e+16, 'train_loss': 1.5469643907335069, 'epoch': 5.0}
